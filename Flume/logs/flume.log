12 Dec 2020 14:55:53,576 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
12 Dec 2020 14:55:53,582 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
12 Dec 2020 14:55:53,590 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,591 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:55:53,591 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,591 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:55:53,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:55:53,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:55:53,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 14:55:53,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: HDFS , ToKafka Agent: TwitterAgent
12 Dec 2020 14:55:53,593 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,593 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,593 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,594 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,594 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,594 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:55:53,594 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,594 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 14:55:53,595 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,595 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 14:55:53,595 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,595 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,595 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,595 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,596 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:55:53,596 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:55:53,596 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
12 Dec 2020 14:55:53,609 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
12 Dec 2020 14:55:53,610 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
12 Dec 2020 14:55:53,611 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
12 Dec 2020 14:55:53,619 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
12 Dec 2020 14:55:53,624 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
12 Dec 2020 14:55:53,626 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
12 Dec 2020 14:55:53,686 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
12 Dec 2020 14:55:53,695 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
12 Dec 2020 14:55:53,704 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
12 Dec 2020 14:55:53,704 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
12 Dec 2020 14:55:53,704 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:388)  - batchSize is deprecated. Please use the parameter flumeBatchSize
12 Dec 2020 14:55:53,705 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
12 Dec 2020 14:55:53,710 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, HDFS, ToKafka]
12 Dec 2020 14:55:53,712 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@139b291f counterGroup:{ name:null counters:{} } }, ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ba152b counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 14:55:53,712 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
12 Dec 2020 14:55:53,843 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
12 Dec 2020 14:55:53,843 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
12 Dec 2020 14:55:53,843 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
12 Dec 2020 14:55:53,844 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
12 Dec 2020 14:55:53,844 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
12 Dec 2020 14:55:53,845 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
12 Dec 2020 14:55:53,846 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
12 Dec 2020 14:55:53,849 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
12 Dec 2020 14:55:53,877 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

12 Dec 2020 14:55:53,973 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
12 Dec 2020 14:55:53,973 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
12 Dec 2020 14:55:53,975 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
12 Dec 2020 14:55:53,975 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
12 Dec 2020 14:55:55,740 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
12 Dec 2020 14:55:55,741 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
12 Dec 2020 14:55:55,822 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
12 Dec 2020 14:55:55,989 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/user/hadoop/twitterraw/FlumeData.1607781355823.tmp
12 Dec 2020 14:55:55,999 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
12 Dec 2020 14:55:56,122 WARN  [hdfs-HDFS-call-runner-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:60)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12 Dec 2020 14:55:57,501 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.security.AccessControlException: Permission denied: user=casperhansen, access=WRITE, inode="/user/hadoop":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2431)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2375)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:469)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:286)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1263)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1242)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1224)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1162)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:549)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:546)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:560)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:487)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=casperhansen, access=WRITE, inode="/user/hadoop":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2431)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2375)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:469)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1562)
	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:234)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:119)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)
	... 24 more
12 Dec 2020 14:55:58,930 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/user/hadoop/twitterraw/FlumeData.1607781355824.tmp
12 Dec 2020 14:55:58,965 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.security.AccessControlException: Permission denied: user=casperhansen, access=WRITE, inode="/user/hadoop":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2431)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2375)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:469)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:286)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1263)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1242)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1224)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1162)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:549)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:546)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:560)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:487)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=casperhansen, access=WRITE, inode="/user/hadoop":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2431)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2375)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:469)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1562)
	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:234)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:119)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)
	... 24 more
12 Dec 2020 14:55:59,756 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@139b291f counterGroup:{ name:null counters:{runner.backoffs.consecutive=2, runner.backoffs=2} } }, ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ba152b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 14:55:59,758 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
12 Dec 2020 14:55:59,758 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
12 Dec 2020 14:55:59,773 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
12 Dec 2020 14:55:59,773 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:714)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:370)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:152)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:117)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:297)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:339)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:188)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:478)
12 Dec 2020 14:55:59,786 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
12 Dec 2020 14:55:59,787 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@139b291f counterGroup:{ name:null counters:{runner.backoffs.consecutive=2, runner.backoffs=2} } }
12 Dec 2020 14:55:59,788 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://localhost:9000/user/hadoop/twitterraw/FlumeData
12 Dec 2020 14:55:59,789 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://localhost:9000/user/hadoop/twitterraw/FlumeData.1607781355824.tmp
12 Dec 2020 14:55:59,789 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:443)  - HDFSWriter is already closed: hdfs://localhost:9000/user/hadoop/twitterraw/FlumeData.1607781355824.tmp
12 Dec 2020 14:55:59,812 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
12 Dec 2020 14:55:59,812 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607781353846
12 Dec 2020 14:55:59,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607781359812
12 Dec 2020 14:55:59,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
12 Dec 2020 14:55:59,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
12 Dec 2020 14:55:59,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 0
12 Dec 2020 14:55:59,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
12 Dec 2020 14:55:59,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 0
12 Dec 2020 14:55:59,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 0
12 Dec 2020 14:55:59,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 4
12 Dec 2020 14:55:59,814 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 0
12 Dec 2020 14:55:59,814 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 0
12 Dec 2020 14:55:59,814 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 2
12 Dec 2020 14:55:59,814 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
12 Dec 2020 14:55:59,814 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ba152b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
12 Dec 2020 14:55:59,815 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1047)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:832)
12 Dec 2020 14:55:59,815 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:832)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1047)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
12 Dec 2020 14:56:04,820 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
12 Dec 2020 14:56:04,832 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
12 Dec 2020 14:56:04,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607781353975
12 Dec 2020 14:56:04,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607781364832
12 Dec 2020 14:56:04,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 5707
12 Dec 2020 14:56:04,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
12 Dec 2020 14:56:04,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
12 Dec 2020 14:56:04,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
12 Dec 2020 14:56:04,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
12 Dec 2020 14:56:04,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
12 Dec 2020 14:56:04,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
12 Dec 2020 14:56:04,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
12 Dec 2020 14:56:04,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
12 Dec 2020 14:56:04,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 41
12 Dec 2020 14:56:04,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 40
12 Dec 2020 14:56:04,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
12 Dec 2020 14:56:04,834 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=5707, sink.event.drain.attempt=41, sink.batch.complete=0, sink.event.drain.sucess=40, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
12 Dec 2020 14:56:04,835 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
12 Dec 2020 14:56:04,835 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ba152b counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
12 Dec 2020 14:56:04,835 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
12 Dec 2020 14:56:04,835 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
12 Dec 2020 14:56:04,835 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607781353843
12 Dec 2020 14:56:04,835 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607781364835
12 Dec 2020 14:56:04,835 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
12 Dec 2020 14:56:04,835 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 1
12 Dec 2020 14:56:04,835 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 41
12 Dec 2020 14:56:04,836 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 41
12 Dec 2020 14:56:04,836 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 44
12 Dec 2020 14:56:04,836 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 40
12 Dec 2020 14:56:04,836 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 14
12 Dec 2020 14:56:04,837 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
12 Dec 2020 14:56:29,257 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
12 Dec 2020 14:56:29,264 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
12 Dec 2020 14:56:29,274 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,275 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:56:29,275 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,275 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:56:29,275 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,275 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,276 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:56:29,276 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,276 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:56:29,276 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 14:56:29,276 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: HDFS , ToKafka Agent: TwitterAgent
12 Dec 2020 14:56:29,276 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,277 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,277 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,278 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,278 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,278 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 14:56:29,278 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 14:56:29,279 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 14:56:29,280 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
12 Dec 2020 14:56:29,294 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
12 Dec 2020 14:56:29,295 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
12 Dec 2020 14:56:29,295 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
12 Dec 2020 14:56:29,303 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
12 Dec 2020 14:56:29,315 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
12 Dec 2020 14:56:29,324 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
12 Dec 2020 14:56:29,455 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
12 Dec 2020 14:56:29,477 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
12 Dec 2020 14:56:29,486 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
12 Dec 2020 14:56:29,487 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
12 Dec 2020 14:56:29,487 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:388)  - batchSize is deprecated. Please use the parameter flumeBatchSize
12 Dec 2020 14:56:29,487 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
12 Dec 2020 14:56:29,498 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, HDFS, ToKafka]
12 Dec 2020 14:56:29,501 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@139b291f counterGroup:{ name:null counters:{} } }, ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ba152b counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 14:56:29,502 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
12 Dec 2020 14:56:29,656 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
12 Dec 2020 14:56:29,657 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
12 Dec 2020 14:56:29,657 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
12 Dec 2020 14:56:29,658 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
12 Dec 2020 14:56:29,659 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
12 Dec 2020 14:56:29,660 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
12 Dec 2020 14:56:29,660 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
12 Dec 2020 14:56:29,670 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
12 Dec 2020 14:56:29,782 INFO  [lifecycleSupervisor-1-3] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

12 Dec 2020 14:56:29,911 INFO  [lifecycleSupervisor-1-3] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
12 Dec 2020 14:56:29,911 INFO  [lifecycleSupervisor-1-3] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
12 Dec 2020 14:56:29,912 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
12 Dec 2020 14:56:29,913 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
12 Dec 2020 14:56:31,486 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
12 Dec 2020 14:56:31,487 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
12 Dec 2020 14:56:31,615 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
12 Dec 2020 14:56:31,758 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
12 Dec 2020 14:56:31,844 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/user/hadoop/casperhansen/twitter/FlumeData.1607781391616.tmp
12 Dec 2020 14:56:32,084 WARN  [hdfs-HDFS-call-runner-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:60)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12 Dec 2020 14:56:34,048 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.security.AccessControlException: Permission denied: user=casperhansen, access=WRITE, inode="/user/hadoop":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2431)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2375)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:469)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:286)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1263)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1242)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1224)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1162)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:549)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:546)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:560)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:487)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=casperhansen, access=WRITE, inode="/user/hadoop":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2431)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2375)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:469)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1562)
	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:234)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:119)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)
	... 24 more
12 Dec 2020 14:56:35,366 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/user/hadoop/casperhansen/twitter/FlumeData.1607781391617.tmp
12 Dec 2020 14:56:35,408 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.security.AccessControlException: Permission denied: user=casperhansen, access=WRITE, inode="/user/hadoop":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2431)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2375)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:469)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:64)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:286)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1263)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1242)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1224)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1162)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:549)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:546)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:560)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:487)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1164)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1144)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1033)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1021)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
	at java.base/java.lang.Thread.run(Thread.java:832)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=casperhansen, access=WRITE, inode="/user/hadoop":hadoop:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1855)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1839)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1798)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:323)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2431)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2375)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:791)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:469)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1562)
	at org.apache.hadoop.ipc.Client.call(Client.java:1508)
	at org.apache.hadoop.ipc.Client.call(Client.java:1405)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:234)
	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:119)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)
	... 24 more
12 Dec 2020 14:56:37,128 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@139b291f counterGroup:{ name:null counters:{runner.backoffs.consecutive=2, runner.backoffs=2} } }, ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ba152b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 14:56:37,128 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
12 Dec 2020 14:56:37,129 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
12 Dec 2020 14:56:37,323 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
12 Dec 2020 14:56:37,323 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:714)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:370)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:152)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:117)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:297)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:339)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:188)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:478)
12 Dec 2020 14:56:37,330 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
12 Dec 2020 14:56:37,330 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@139b291f counterGroup:{ name:null counters:{runner.backoffs.consecutive=2, runner.backoffs=2} } }
12 Dec 2020 14:56:37,330 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://localhost:9000/user/hadoop/casperhansen/twitter/FlumeData
12 Dec 2020 14:56:37,331 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://localhost:9000/user/hadoop/casperhansen/twitter/FlumeData.1607781391617.tmp
12 Dec 2020 14:56:37,331 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:443)  - HDFSWriter is already closed: hdfs://localhost:9000/user/hadoop/casperhansen/twitter/FlumeData.1607781391617.tmp
12 Dec 2020 14:56:37,355 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
12 Dec 2020 14:56:37,355 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607781389660
12 Dec 2020 14:56:37,355 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607781397355
12 Dec 2020 14:56:37,355 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
12 Dec 2020 14:56:37,355 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 0
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 0
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 0
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 4
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 0
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 0
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 2
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
12 Dec 2020 14:56:37,356 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ba152b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
12 Dec 2020 14:56:37,357 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1047)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:832)
12 Dec 2020 14:56:37,357 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:832)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1047)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
12 Dec 2020 14:56:42,373 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
12 Dec 2020 14:56:42,408 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
12 Dec 2020 14:56:42,413 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607781389913
12 Dec 2020 14:56:42,413 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607781402408
12 Dec 2020 14:56:42,413 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 6503
12 Dec 2020 14:56:42,413 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 50
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 40
12 Dec 2020 14:56:42,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
12 Dec 2020 14:56:42,415 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=6503, sink.event.drain.attempt=50, sink.batch.complete=0, sink.event.drain.sucess=40, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
12 Dec 2020 14:56:42,415 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
12 Dec 2020 14:56:42,415 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
12 Dec 2020 14:56:42,415 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
12 Dec 2020 14:56:42,415 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607781389657
12 Dec 2020 14:56:42,415 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607781402415
12 Dec 2020 14:56:42,415 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
12 Dec 2020 14:56:42,415 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 10
12 Dec 2020 14:56:42,416 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 50
12 Dec 2020 14:56:42,416 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 50
12 Dec 2020 14:56:42,416 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 53
12 Dec 2020 14:56:42,416 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 40
12 Dec 2020 14:56:42,416 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 14
12 Dec 2020 14:56:42,415 INFO  [lifecycleSupervisor-1-5] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ba152b counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
12 Dec 2020 14:56:42,423 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
12 Dec 2020 22:53:14,860 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
12 Dec 2020 22:53:14,880 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
12 Dec 2020 22:53:14,892 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,894 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 22:53:14,894 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,895 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 22:53:14,895 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,897 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 22:53:14,897 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,898 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 22:53:14,898 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 22:53:14,898 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka # , HDFS Agent: TwitterAgent
12 Dec 2020 22:53:14,899 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,901 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,901 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,906 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,906 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,906 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 22:53:14,907 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,907 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 22:53:14,907 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,908 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 22:53:14,908 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,908 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,908 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,908 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,908 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 22:53:14,911 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 22:53:14,911 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
12 Dec 2020 22:53:14,931 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink#
12 Dec 2020 22:53:14,939 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
12 Dec 2020 22:53:14,941 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
12 Dec 2020 22:53:14,941 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
12 Dec 2020 22:53:14,950 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
12 Dec 2020 22:53:14,956 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
12 Dec 2020 22:53:14,960 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
12 Dec 2020 22:53:15,038 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
12 Dec 2020 22:53:15,047 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
12 Dec 2020 22:53:15,047 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
12 Dec 2020 22:53:15,048 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:388)  - batchSize is deprecated. Please use the parameter flumeBatchSize
12 Dec 2020 22:53:15,048 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
12 Dec 2020 22:53:15,055 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
12 Dec 2020 22:53:15,064 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, ToKafka, HDFS]
12 Dec 2020 22:53:15,066 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@41d3d5f9 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7d6cc4ec counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 22:53:15,071 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
12 Dec 2020 22:53:15,258 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
12 Dec 2020 22:53:15,262 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
12 Dec 2020 22:53:15,262 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
12 Dec 2020 22:53:15,263 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
12 Dec 2020 22:53:15,264 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
12 Dec 2020 22:53:15,278 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
12 Dec 2020 22:53:15,275 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
12 Dec 2020 22:53:15,281 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
12 Dec 2020 22:53:15,321 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

12 Dec 2020 22:53:15,443 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
12 Dec 2020 22:53:15,443 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
12 Dec 2020 22:53:15,445 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
12 Dec 2020 22:53:15,446 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
12 Dec 2020 22:53:17,094 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
12 Dec 2020 22:53:17,094 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
12 Dec 2020 22:53:17,259 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
12 Dec 2020 22:53:17,632 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607809997256.tmp
12 Dec 2020 22:53:17,749 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
12 Dec 2020 22:53:22,547 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
12 Dec 2020 22:56:56,979 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@41d3d5f9 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7d6cc4ec counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 22:56:56,979 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
12 Dec 2020 22:56:56,979 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
12 Dec 2020 22:56:57,020 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
12 Dec 2020 22:56:57,021 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
12 Dec 2020 22:56:57,024 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
12 Dec 2020 22:56:57,025 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@41d3d5f9 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
12 Dec 2020 22:56:57,026 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 22:56:57,026 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
12 Dec 2020 22:57:02,028 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
12 Dec 2020 22:57:02,046 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
12 Dec 2020 22:57:02,046 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607809995446
12 Dec 2020 22:57:02,046 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607810222046
12 Dec 2020 22:57:02,046 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 218588
12 Dec 2020 22:57:02,046 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
12 Dec 2020 22:57:02,046 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 1477
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 1460
12 Dec 2020 22:57:02,047 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
12 Dec 2020 22:57:02,048 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=218588, sink.event.drain.attempt=1477, sink.batch.complete=0, sink.event.drain.sucess=1460, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
12 Dec 2020 22:57:02,051 INFO  [lifecycleSupervisor-1-5] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@41d3d5f9 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
12 Dec 2020 22:57:02,051 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
12 Dec 2020 22:57:02,051 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7d6cc4ec counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
12 Dec 2020 22:57:02,052 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
12 Dec 2020 22:57:02,052 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607809997256.tmp
12 Dec 2020 22:57:02,100 INFO  [hdfs-HDFS-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607809997256.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607809997256
12 Dec 2020 22:57:02,128 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
12 Dec 2020 22:57:02,128 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607809995281
12 Dec 2020 22:57:02,128 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607810222128
12 Dec 2020 22:57:02,128 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 1
12 Dec 2020 22:57:02,128 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 1441
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 1441
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
12 Dec 2020 22:57:02,129 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607809995262
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607810222130
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 0
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 2901
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 2901
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 2921
12 Dec 2020 22:57:02,130 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 2901
12 Dec 2020 22:57:02,131 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
12 Dec 2020 22:57:02,134 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
12 Dec 2020 23:02:05,282 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
12 Dec 2020 23:02:05,296 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
12 Dec 2020 23:02:05,317 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:02:05,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,324 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:02:05,325 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,325 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,325 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:02:05,325 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,326 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:02:05,326 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 23:02:05,326 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka # , HDFS Agent: TwitterAgent
12 Dec 2020 23:02:05,327 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,330 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,331 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,339 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,339 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,340 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:02:05,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 23:02:05,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,352 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 23:02:05,353 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,353 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,353 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,353 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,355 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:02:05,355 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:02:05,356 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
12 Dec 2020 23:02:05,422 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink#
12 Dec 2020 23:02:05,437 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
12 Dec 2020 23:02:05,447 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
12 Dec 2020 23:02:05,448 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
12 Dec 2020 23:02:05,491 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
12 Dec 2020 23:02:05,494 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
12 Dec 2020 23:02:05,508 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
12 Dec 2020 23:02:05,714 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
12 Dec 2020 23:02:05,730 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
12 Dec 2020 23:02:05,731 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
12 Dec 2020 23:02:05,731 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:388)  - batchSize is deprecated. Please use the parameter flumeBatchSize
12 Dec 2020 23:02:05,743 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
12 Dec 2020 23:02:05,758 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
12 Dec 2020 23:02:05,790 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, ToKafka, HDFS]
12 Dec 2020 23:02:05,795 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7804c4d4 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@43da42b4 counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 23:02:05,795 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
12 Dec 2020 23:02:06,264 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
12 Dec 2020 23:02:06,264 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
12 Dec 2020 23:02:06,265 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
12 Dec 2020 23:02:06,267 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
12 Dec 2020 23:02:06,271 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
12 Dec 2020 23:02:06,293 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
12 Dec 2020 23:02:06,293 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
12 Dec 2020 23:02:06,301 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
12 Dec 2020 23:02:06,383 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

12 Dec 2020 23:02:06,770 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
12 Dec 2020 23:02:06,770 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
12 Dec 2020 23:02:06,780 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
12 Dec 2020 23:02:06,780 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
12 Dec 2020 23:02:08,363 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
12 Dec 2020 23:02:08,364 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
12 Dec 2020 23:02:08,602 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
12 Dec 2020 23:02:09,813 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528602.tmp
12 Dec 2020 23:02:10,320 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
12 Dec 2020 23:02:10,884 WARN  [hdfs-HDFS-call-runner-0] (org.apache.hadoop.util.NativeCodeLoader.<clinit>:60)  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
12 Dec 2020 23:02:11,294 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:12,463 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528603.tmp
12 Dec 2020 23:02:12,470 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:14,543 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528604.tmp
12 Dec 2020 23:02:14,546 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:17,693 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528605.tmp
12 Dec 2020 23:02:17,702 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:21,849 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528606.tmp
12 Dec 2020 23:02:21,857 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:26,962 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528607.tmp
12 Dec 2020 23:02:26,982 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:32,205 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528608.tmp
12 Dec 2020 23:02:32,223 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:37,495 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528609.tmp
12 Dec 2020 23:02:37,498 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:42,814 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528610.tmp
12 Dec 2020 23:02:42,815 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:48,035 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528611.tmp
12 Dec 2020 23:02:48,040 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:53,189 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528612.tmp
12 Dec 2020 23:02:53,195 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:02:58,260 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528613.tmp
12 Dec 2020 23:02:58,267 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:03,392 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528614.tmp
12 Dec 2020 23:03:03,395 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:08,584 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528615.tmp
12 Dec 2020 23:03:08,588 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:13,853 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528616.tmp
12 Dec 2020 23:03:13,860 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:18,926 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528617.tmp
12 Dec 2020 23:03:18,928 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:24,045 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528618.tmp
12 Dec 2020 23:03:24,046 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:29,090 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528619.tmp
12 Dec 2020 23:03:29,094 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:34,210 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528620.tmp
12 Dec 2020 23:03:34,213 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:39,325 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528621.tmp
12 Dec 2020 23:03:39,334 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:44,438 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528622.tmp
12 Dec 2020 23:03:44,441 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:49,574 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528623.tmp
12 Dec 2020 23:03:49,579 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:54,701 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528624.tmp
12 Dec 2020 23:03:54,704 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:03:59,855 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528625.tmp
12 Dec 2020 23:03:59,858 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:04:04,882 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528626.tmp
12 Dec 2020 23:04:04,885 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:04:09,926 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528627.tmp
12 Dec 2020 23:04:09,929 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:04:15,110 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528628.tmp
12 Dec 2020 23:04:15,111 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:04:15,920 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7804c4d4 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@43da42b4 counterGroup:{ name:null counters:{runner.backoffs.consecutive=27, runner.backoffs=27} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 23:04:15,920 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
12 Dec 2020 23:04:15,921 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
12 Dec 2020 23:04:15,922 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
12 Dec 2020 23:04:15,922 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
12 Dec 2020 23:04:15,925 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
12 Dec 2020 23:04:15,925 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7804c4d4 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
12 Dec 2020 23:04:15,925 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:04:15,926 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
12 Dec 2020 23:04:20,117 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607810528629.tmp
12 Dec 2020 23:04:20,120 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "hdfs"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:255)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:04:20,926 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
12 Dec 2020 23:04:20,948 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607810526780
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607810660948
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 128597
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
12 Dec 2020 23:04:20,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
12 Dec 2020 23:04:20,950 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 1786
12 Dec 2020 23:04:20,950 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 1780
12 Dec 2020 23:04:20,950 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
12 Dec 2020 23:04:20,950 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=128597, sink.event.drain.attempt=1786, sink.batch.complete=0, sink.event.drain.sucess=1780, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
12 Dec 2020 23:04:20,950 INFO  [lifecycleSupervisor-1-9] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7804c4d4 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
12 Dec 2020 23:04:20,951 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
12 Dec 2020 23:04:20,951 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@43da42b4 counterGroup:{ name:null counters:{runner.backoffs.consecutive=28, runner.backoffs=28} } }
12 Dec 2020 23:04:20,951 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
12 Dec 2020 23:04:20,952 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607810528629.tmp
12 Dec 2020 23:04:20,954 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:443)  - HDFSWriter is already closed: hdfs://node-master:9000/twitterraw/FlumeData.1607810528629.tmp
12 Dec 2020 23:04:20,957 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607810526293
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607810660957
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 0
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 0
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 0
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 56
12 Dec 2020 23:04:20,958 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 0
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 0
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 28
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607810526264
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607810660959
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
12 Dec 2020 23:04:20,959 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 6
12 Dec 2020 23:04:20,960 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 1786
12 Dec 2020 23:04:20,960 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 1786
12 Dec 2020 23:04:20,960 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 1815
12 Dec 2020 23:04:20,960 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 1780
12 Dec 2020 23:04:20,960 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
12 Dec 2020 23:04:20,975 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
12 Dec 2020 23:20:21,798 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
12 Dec 2020 23:20:21,812 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
12 Dec 2020 23:20:21,834 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:20:21,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:20:21,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,838 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,838 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:20:21,838 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,843 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:20:21,843 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 23:20:21,843 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka # , HDFS Agent: TwitterAgent
12 Dec 2020 23:20:21,843 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,845 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,845 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,851 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,851 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,851 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
12 Dec 2020 23:20:21,859 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,859 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 23:20:21,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
12 Dec 2020 23:20:21,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
12 Dec 2020 23:20:21,861 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
12 Dec 2020 23:20:21,863 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
12 Dec 2020 23:20:21,903 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink#
12 Dec 2020 23:20:21,906 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
12 Dec 2020 23:20:21,910 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
12 Dec 2020 23:20:21,910 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
12 Dec 2020 23:20:21,937 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
12 Dec 2020 23:20:21,947 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
12 Dec 2020 23:20:21,948 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
12 Dec 2020 23:20:22,090 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
12 Dec 2020 23:20:22,113 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
12 Dec 2020 23:20:22,114 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
12 Dec 2020 23:20:22,114 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:388)  - batchSize is deprecated. Please use the parameter flumeBatchSize
12 Dec 2020 23:20:22,115 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
12 Dec 2020 23:20:22,131 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
12 Dec 2020 23:20:22,143 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, ToKafka, HDFS]
12 Dec 2020 23:20:22,145 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6e8bfdc2 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@16facacd counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 23:20:22,155 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
12 Dec 2020 23:20:22,599 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
12 Dec 2020 23:20:22,609 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
12 Dec 2020 23:20:22,609 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
12 Dec 2020 23:20:22,615 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
12 Dec 2020 23:20:22,618 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
12 Dec 2020 23:20:22,644 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
12 Dec 2020 23:20:22,649 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
12 Dec 2020 23:20:22,655 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
12 Dec 2020 23:20:22,744 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

12 Dec 2020 23:20:23,149 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
12 Dec 2020 23:20:23,149 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
12 Dec 2020 23:20:23,151 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
12 Dec 2020 23:20:23,179 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
12 Dec 2020 23:20:24,967 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
12 Dec 2020 23:20:24,968 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
12 Dec 2020 23:20:25,267 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
12 Dec 2020 23:20:26,145 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607811625268.tmp
12 Dec 2020 23:20:26,431 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
12 Dec 2020 23:20:32,693 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
12 Dec 2020 23:22:37,467 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6e8bfdc2 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@16facacd counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
12 Dec 2020 23:22:37,471 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
12 Dec 2020 23:22:37,471 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
12 Dec 2020 23:22:37,591 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
12 Dec 2020 23:22:37,592 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
12 Dec 2020 23:22:37,597 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
12 Dec 2020 23:22:37,597 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6e8bfdc2 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
12 Dec 2020 23:22:37,598 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
12 Dec 2020 23:22:37,598 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
12 Dec 2020 23:22:42,601 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
12 Dec 2020 23:22:42,629 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
12 Dec 2020 23:22:42,629 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607811623179
12 Dec 2020 23:22:42,629 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607811762629
12 Dec 2020 23:22:42,629 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 132244
12 Dec 2020 23:22:42,629 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 893
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 880
12 Dec 2020 23:22:42,630 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
12 Dec 2020 23:22:42,631 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=132244, sink.event.drain.attempt=893, sink.batch.complete=0, sink.event.drain.sucess=880, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
12 Dec 2020 23:22:42,635 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6e8bfdc2 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
12 Dec 2020 23:22:42,635 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
12 Dec 2020 23:22:42,635 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@16facacd counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
12 Dec 2020 23:22:42,636 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
12 Dec 2020 23:22:42,636 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607811625268.tmp
12 Dec 2020 23:22:42,676 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607811625268.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607811625268
12 Dec 2020 23:22:42,689 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
12 Dec 2020 23:22:42,690 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607811622655
12 Dec 2020 23:22:42,690 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607811762689
12 Dec 2020 23:22:42,690 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
12 Dec 2020 23:22:42,690 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
12 Dec 2020 23:22:42,690 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
12 Dec 2020 23:22:42,690 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
12 Dec 2020 23:22:42,691 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
12 Dec 2020 23:22:42,691 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
12 Dec 2020 23:22:42,691 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 833
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 833
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607811622609
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607811762692
12 Dec 2020 23:22:42,692 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
12 Dec 2020 23:22:42,693 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 0
12 Dec 2020 23:22:42,693 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 1713
12 Dec 2020 23:22:42,693 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 1713
12 Dec 2020 23:22:42,693 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 1729
12 Dec 2020 23:22:42,693 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 1713
12 Dec 2020 23:22:42,693 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
12 Dec 2020 23:22:42,698 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 00:24:48,185 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 00:24:48,199 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 00:24:48,226 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,227 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:24:48,228 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,228 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:24:48,228 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,229 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,229 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:24:48,229 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,230 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:24:48,231 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:24:48,231 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 00:24:48,231 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,233 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,233 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,237 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,237 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,237 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:24:48,238 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:24:48,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:24:48,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,240 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,240 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,240 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:24:48,240 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:24:48,241 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 00:24:48,261 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 00:24:48,263 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 00:24:48,264 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 00:24:48,276 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
13 Dec 2020 00:24:48,281 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
13 Dec 2020 00:24:48,282 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 00:24:48,352 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 00:24:48,377 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, HDFS]
13 Dec 2020 00:24:48,379 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59166e48 counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
13 Dec 2020 00:24:48,379 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
13 Dec 2020 00:24:48,820 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
13 Dec 2020 00:24:48,820 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
13 Dec 2020 00:24:48,821 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 00:24:48,822 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 00:24:48,832 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 00:24:48,835 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 00:24:48,835 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 00:24:50,921 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 00:24:50,922 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 00:24:51,081 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 00:24:51,483 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491076.tmp
13 Dec 2020 00:24:54,074 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:24:55,112 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491077.tmp
13 Dec 2020 00:24:55,160 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:24:57,233 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491078.tmp
13 Dec 2020 00:24:57,290 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:00,337 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491079.tmp
13 Dec 2020 00:25:00,389 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:04,429 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491080.tmp
13 Dec 2020 00:25:04,478 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:09,507 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491081.tmp
13 Dec 2020 00:25:09,554 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:14,588 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491082.tmp
13 Dec 2020 00:25:14,637 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:19,660 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491083.tmp
13 Dec 2020 00:25:19,693 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:24,713 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491084.tmp
13 Dec 2020 00:25:24,759 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:29,788 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491085.tmp
13 Dec 2020 00:25:29,812 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:34,853 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491086.tmp
13 Dec 2020 00:25:34,892 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:39,915 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491087.tmp
13 Dec 2020 00:25:39,939 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:44,960 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491088.tmp
13 Dec 2020 00:25:44,986 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:50,001 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491089.tmp
13 Dec 2020 00:25:50,022 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:25:55,063 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491090.tmp
13 Dec 2020 00:25:55,099 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:26:00,120 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491091.tmp
13 Dec 2020 00:26:00,160 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:26:05,189 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815491092.tmp
13 Dec 2020 00:26:05,219 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at jdk.internal.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 38 more
13 Dec 2020 00:26:09,274 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59166e48 counterGroup:{ name:null counters:{runner.backoffs.consecutive=17, runner.backoffs=17} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
13 Dec 2020 00:26:09,275 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 00:26:09,275 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 00:26:09,718 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 00:26:09,719 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 00:26:09,722 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 00:26:09,722 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59166e48 counterGroup:{ name:null counters:{runner.backoffs.consecutive=17, runner.backoffs=17} } }
13 Dec 2020 00:26:09,723 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://localhost:9000/twitterraw/FlumeData
13 Dec 2020 00:26:09,724 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://localhost:9000/twitterraw/FlumeData.1607815491092.tmp
13 Dec 2020 00:26:09,724 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:443)  - HDFSWriter is already closed: hdfs://localhost:9000/twitterraw/FlumeData.1607815491092.tmp
13 Dec 2020 00:26:09,744 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:462)  - failed to rename() file (hdfs://localhost:9000/twitterraw/FlumeData.1607815491092.tmp). Exception follows.
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor8.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:904)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1661)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1577)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1574)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1589)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:680)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:677)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 30 more
13 Dec 2020 00:28:40,872 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 00:28:40,882 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 00:28:40,893 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,894 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:40,894 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,894 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:40,894 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,895 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:40,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,897 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:40,897 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:28:40,898 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 00:28:40,898 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,899 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,899 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,903 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,903 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,903 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:40,904 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,904 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:28:40,904 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,904 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:28:40,904 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,904 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,905 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,905 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,905 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:40,905 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:40,906 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 00:28:40,922 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 00:28:40,923 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 00:28:40,925 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 00:28:40,938 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
13 Dec 2020 00:28:40,944 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
13 Dec 2020 00:28:40,944 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 00:28:41,004 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 00:28:41,023 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, HDFS]
13 Dec 2020 00:28:41,026 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59db4644 counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
13 Dec 2020 00:28:41,026 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
13 Dec 2020 00:28:41,236 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
13 Dec 2020 00:28:41,236 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
13 Dec 2020 00:28:41,236 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 00:28:41,238 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 00:28:41,246 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 00:28:41,248 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 00:28:41,248 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 00:28:42,487 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 00:28:42,488 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 00:28:42,676 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 00:28:43,049 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722677.tmp
13 Dec 2020 00:28:45,169 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:28:46,236 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722678.tmp
13 Dec 2020 00:28:46,293 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:28:48,363 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722679.tmp
13 Dec 2020 00:28:48,422 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:28:48,825 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 00:28:48,826 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,827 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:48,828 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,828 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:48,828 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,828 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,829 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:48,829 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,829 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:48,829 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:28:48,835 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 00:28:48,835 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,835 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,835 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,835 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,835 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,836 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:28:48,836 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,836 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:28:48,836 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:28:48,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:28:48,838 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:28:48,838 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 00:28:48,852 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 00:28:48,853 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 00:28:48,855 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 00:28:48,855 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
13 Dec 2020 00:28:48,858 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 00:28:48,870 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 00:28:48,871 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, HDFS]
13 Dec 2020 00:28:51,483 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722680.tmp
13 Dec 2020 00:28:51,551 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:28:55,594 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722681.tmp
13 Dec 2020 00:28:55,648 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:00,685 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722682.tmp
13 Dec 2020 00:29:00,730 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:05,768 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722683.tmp
13 Dec 2020 00:29:05,806 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:09,746 WARN  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter$ScheduledRenameCallable.call:384)  - Renaming file: hdfs://localhost:9000/twitterraw/FlumeData.1607815491092.tmp failed. Will retry again in 180 seconds.
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@36ed5905[Not completed, task = org.apache.flume.sink.hdfs.BucketWriter$8@32d2cb87] rejected from java.util.concurrent.ThreadPoolExecutor@73efc1bc[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 18]
	at java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)
	at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)
	at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:721)
	at org.apache.flume.sink.hdfs.BucketWriter.renameBucket(BucketWriter.java:677)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1600(BucketWriter.java:60)
	at org.apache.flume.sink.hdfs.BucketWriter$ScheduledRenameCallable.call(BucketWriter.java:382)
	at org.apache.flume.sink.hdfs.BucketWriter$ScheduledRenameCallable.call(BucketWriter.java:367)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 00:29:09,747 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 00:29:09,747 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607815488835
13 Dec 2020 00:29:09,747 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607815749747
13 Dec 2020 00:29:09,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 00:29:09,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 00:29:09,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 0
13 Dec 2020 00:29:09,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 00:29:09,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 0
13 Dec 2020 00:29:09,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 0
13 Dec 2020 00:29:09,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 36
13 Dec 2020 00:29:09,751 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 0
13 Dec 2020 00:29:09,751 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 0
13 Dec 2020 00:29:09,751 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 17
13 Dec 2020 00:29:09,751 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59166e48 counterGroup:{ name:null counters:{runner.interruptions=1, runner.backoffs.consecutive=17, runner.backoffs=17} } }
13 Dec 2020 00:29:09,755 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
13 Dec 2020 00:29:09,755 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
13 Dec 2020 00:29:09,755 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
13 Dec 2020 00:29:09,755 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607815488820
13 Dec 2020 00:29:09,755 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607815749755
13 Dec 2020 00:29:09,755 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
13 Dec 2020 00:29:09,755 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 1101
13 Dec 2020 00:29:09,763 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 1101
13 Dec 2020 00:29:09,763 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 1101
13 Dec 2020 00:29:09,763 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 17
13 Dec 2020 00:29:09,763 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 0
13 Dec 2020 00:29:09,763 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 00:29:09,769 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 00:29:10,270 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.handleConfigurationEvent:100)  - Interrupted while trying to handle configuration event
13 Dec 2020 00:29:10,836 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722684.tmp
13 Dec 2020 00:29:10,863 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:15,897 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722685.tmp
13 Dec 2020 00:29:15,952 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:20,976 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722686.tmp
13 Dec 2020 00:29:20,995 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:26,024 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722687.tmp
13 Dec 2020 00:29:26,042 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:31,065 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722688.tmp
13 Dec 2020 00:29:31,086 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:36,106 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://localhost:9000/twitterraw/FlumeData.1607815722689.tmp
13 Dec 2020 00:29:36,131 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:366)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1212)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1191)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1129)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:531)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:528)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:542)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.flume.sink.hdfs.HDFSDataStream.doOpen(HDFSDataStream.java:81)
	at org.apache.flume.sink.hdfs.HDFSDataStream.open(HDFSDataStream.java:108)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:257)
	at org.apache.flume.sink.hdfs.BucketWriter$1.call(BucketWriter.java:247)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 39 more
13 Dec 2020 00:29:36,522 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59db4644 counterGroup:{ name:null counters:{runner.backoffs.consecutive=13, runner.backoffs=13} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
13 Dec 2020 00:29:36,523 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 00:29:36,524 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 00:29:36,702 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 00:29:36,705 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 00:29:36,706 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 00:29:36,707 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59db4644 counterGroup:{ name:null counters:{runner.backoffs.consecutive=13, runner.backoffs=13} } }
13 Dec 2020 00:29:36,708 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://localhost:9000/twitterraw/FlumeData
13 Dec 2020 00:29:36,708 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://localhost:9000/twitterraw/FlumeData.1607815722689.tmp
13 Dec 2020 00:29:36,709 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:443)  - HDFSWriter is already closed: hdfs://localhost:9000/twitterraw/FlumeData.1607815722689.tmp
13 Dec 2020 00:29:36,730 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:462)  - failed to rename() file (hdfs://localhost:9000/twitterraw/FlumeData.1607815722689.tmp). Exception follows.
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:904)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1661)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1577)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1574)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1589)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:680)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:677)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 30 more
13 Dec 2020 00:31:41,241 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 00:31:41,242 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,243 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:31:41,243 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,243 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:31:41,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,244 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:31:41,245 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,247 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:31:41,247 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:31:41,247 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 00:31:41,248 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,248 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,248 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,249 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,249 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,249 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:31:41,250 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,250 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:31:41,250 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,251 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:31:41,251 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,251 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,252 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,252 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,252 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:31:41,253 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:31:41,259 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 00:31:41,270 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 00:31:41,271 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 00:31:41,271 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 00:31:41,272 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
13 Dec 2020 00:31:41,272 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 00:31:41,280 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 00:31:41,282 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, HDFS]
13 Dec 2020 00:32:36,732 WARN  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter$ScheduledRenameCallable.call:384)  - Renaming file: hdfs://localhost:9000/twitterraw/FlumeData.1607815722689.tmp failed. Will retry again in 180 seconds.
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@4e2db467[Not completed, task = org.apache.flume.sink.hdfs.BucketWriter$8@4a1239fd] rejected from java.util.concurrent.ThreadPoolExecutor@b2f33c6[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 14]
	at java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)
	at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)
	at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:721)
	at org.apache.flume.sink.hdfs.BucketWriter.renameBucket(BucketWriter.java:677)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1600(BucketWriter.java:60)
	at org.apache.flume.sink.hdfs.BucketWriter$ScheduledRenameCallable.call(BucketWriter.java:382)
	at org.apache.flume.sink.hdfs.BucketWriter$ScheduledRenameCallable.call(BucketWriter.java:367)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 00:32:36,734 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 00:32:36,734 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607815721248
13 Dec 2020 00:32:36,734 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607815956734
13 Dec 2020 00:32:36,734 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 00:32:36,734 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 00:32:36,734 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 0
13 Dec 2020 00:32:36,735 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 00:32:36,735 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 0
13 Dec 2020 00:32:36,735 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 0
13 Dec 2020 00:32:36,735 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 28
13 Dec 2020 00:32:36,735 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 0
13 Dec 2020 00:32:36,735 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 0
13 Dec 2020 00:32:36,735 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 13
13 Dec 2020 00:32:36,735 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59db4644 counterGroup:{ name:null counters:{runner.interruptions=1, runner.backoffs.consecutive=13, runner.backoffs=13} } }
13 Dec 2020 00:32:36,738 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
13 Dec 2020 00:32:36,739 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
13 Dec 2020 00:32:36,739 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
13 Dec 2020 00:32:36,739 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607815721236
13 Dec 2020 00:32:36,739 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607815956739
13 Dec 2020 00:32:36,740 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
13 Dec 2020 00:32:36,740 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 759
13 Dec 2020 00:32:36,740 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 759
13 Dec 2020 00:32:36,740 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 759
13 Dec 2020 00:32:36,741 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 13
13 Dec 2020 00:32:36,741 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 0
13 Dec 2020 00:32:36,741 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 00:32:36,744 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 00:32:37,245 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.handleConfigurationEvent:100)  - Interrupted while trying to handle configuration event
13 Dec 2020 00:32:55,955 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 00:32:55,966 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 00:32:55,979 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,980 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:32:55,981 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,981 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:32:55,982 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,982 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,982 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:32:55,983 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,984 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:32:55,984 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:32:55,985 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 00:32:55,985 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,986 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,987 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,991 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,991 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,991 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:32:55,993 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,994 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:32:55,995 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,995 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:32:55,995 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,995 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,996 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,996 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,996 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:32:55,997 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:32:55,997 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 00:32:56,013 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 00:32:56,014 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 00:32:56,016 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 00:32:56,032 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
13 Dec 2020 00:32:56,042 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
13 Dec 2020 00:32:56,043 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 00:32:56,116 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 00:32:56,135 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, HDFS]
13 Dec 2020 00:32:56,138 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@12bacb7f counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
13 Dec 2020 00:32:56,138 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
13 Dec 2020 00:32:56,355 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
13 Dec 2020 00:32:56,355 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
13 Dec 2020 00:32:56,356 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 00:32:56,357 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 00:32:56,364 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 00:32:56,366 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 00:32:56,367 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 00:32:57,788 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 00:32:57,788 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 00:32:57,987 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 00:32:58,347 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607815977988.tmp
13 Dec 2020 00:33:00,322 INFO  [Thread-9] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:44:25,881 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@12bacb7f counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
13 Dec 2020 00:44:25,882 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 00:44:25,883 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 00:44:25,912 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 00:44:25,913 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 00:44:25,917 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 00:44:25,919 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@12bacb7f counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 00:44:25,920 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 00:44:25,920 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:464)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	... 3 more
13 Dec 2020 00:44:30,922 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 00:44:30,926 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607815977988.tmp
13 Dec 2020 00:44:30,954 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607815977988.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607815977988
13 Dec 2020 00:44:30,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 00:44:30,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607815976367
13 Dec 2020 00:44:30,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607816670970
13 Dec 2020 00:44:30,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 9
13 Dec 2020 00:44:30,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 00:44:30,971 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 00:44:30,971 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 00:44:30,971 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
13 Dec 2020 00:44:30,972 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
13 Dec 2020 00:44:30,972 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 00:44:30,972 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 9102
13 Dec 2020 00:44:30,972 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 9000
13 Dec 2020 00:44:30,973 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 1
13 Dec 2020 00:44:30,973 INFO  [lifecycleSupervisor-1-8] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@12bacb7f counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 00:44:30,973 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
13 Dec 2020 00:44:30,976 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
13 Dec 2020 00:44:30,976 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
13 Dec 2020 00:44:30,979 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607815976355
13 Dec 2020 00:44:30,979 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607816670976
13 Dec 2020 00:44:30,980 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
13 Dec 2020 00:44:30,980 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 102
13 Dec 2020 00:44:30,980 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 9102
13 Dec 2020 00:44:30,981 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 9102
13 Dec 2020 00:44:30,981 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 9103
13 Dec 2020 00:44:30,982 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 9000
13 Dec 2020 00:44:30,982 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 00:44:30,985 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 00:49:39,100 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 00:49:39,111 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 00:49:39,125 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,126 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:49:39,127 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,128 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:49:39,128 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,128 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,129 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:49:39,129 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,130 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:49:39,130 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:49:39,131 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 00:49:39,131 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,135 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,135 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,139 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,139 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,139 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:49:39,140 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,141 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:49:39,141 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,141 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:twitter
13 Dec 2020 00:49:39,141 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,142 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,142 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,142 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,143 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:49:39,143 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:49:39,143 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 00:49:39,161 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 00:49:39,162 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 00:49:39,163 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 00:49:39,177 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel twitter type memory
13 Dec 2020 00:49:39,181 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel twitter
13 Dec 2020 00:49:39,182 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 00:49:39,241 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 00:49:39,258 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel twitter connected to [Twitter, HDFS]
13 Dec 2020 00:49:39,260 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@798fcedb counterGroup:{ name:null counters:{} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
13 Dec 2020 00:49:39,260 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel twitter
13 Dec 2020 00:49:39,471 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: twitter: Successfully registered new MBean.
13 Dec 2020 00:49:39,472 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: twitter started
13 Dec 2020 00:49:39,473 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 00:49:39,474 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 00:49:39,482 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 00:49:39,485 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 00:49:39,485 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 00:49:40,722 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 00:49:40,723 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 00:49:40,842 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 00:49:41,085 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980843.tmp
13 Dec 2020 00:49:42,842 INFO  [Thread-9] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:49:47,696 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980843.tmp
13 Dec 2020 00:49:47,737 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980843.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980843
13 Dec 2020 00:49:47,845 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980844.tmp
13 Dec 2020 00:49:48,777 INFO  [Thread-12] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:49:55,251 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980844.tmp
13 Dec 2020 00:49:55,277 INFO  [hdfs-HDFS-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980844.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980844
13 Dec 2020 00:49:55,333 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980845.tmp
13 Dec 2020 00:49:56,385 INFO  [Thread-14] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:50:01,809 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980845.tmp
13 Dec 2020 00:50:01,825 INFO  [hdfs-HDFS-call-runner-2] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980845.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980845
13 Dec 2020 00:50:01,873 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980846.tmp
13 Dec 2020 00:50:02,392 INFO  [Thread-16] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:50:08,621 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980846.tmp
13 Dec 2020 00:50:08,636 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980846.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980846
13 Dec 2020 00:50:08,683 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980847.tmp
13 Dec 2020 00:50:09,614 INFO  [Thread-18] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:50:15,486 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980847.tmp
13 Dec 2020 00:50:15,503 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980847.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980847
13 Dec 2020 00:50:15,537 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980848.tmp
13 Dec 2020 00:50:16,438 INFO  [Thread-20] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:50:24,447 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980848.tmp
13 Dec 2020 00:50:24,464 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980848.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980848
13 Dec 2020 00:50:24,519 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980849.tmp
13 Dec 2020 00:50:25,478 INFO  [Thread-22] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:50:32,166 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980849.tmp
13 Dec 2020 00:50:32,181 INFO  [hdfs-HDFS-call-runner-7] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980849.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980849
13 Dec 2020 00:50:32,226 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980850.tmp
13 Dec 2020 00:50:33,849 INFO  [Thread-24] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:50:40,493 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980850.tmp
13 Dec 2020 00:50:40,515 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980850.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980850
13 Dec 2020 00:50:40,540 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980851.tmp
13 Dec 2020 00:50:41,286 INFO  [Thread-26] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:50:47,252 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980851.tmp
13 Dec 2020 00:50:47,270 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980851.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980851
13 Dec 2020 00:50:47,303 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607816980852.tmp
13 Dec 2020 00:50:47,902 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@798fcedb counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{twitter=org.apache.flume.channel.MemoryChannel{name: twitter}} }
13 Dec 2020 00:50:47,905 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 00:50:47,905 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 00:50:47,915 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 00:50:47,916 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 00:50:47,923 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 00:50:47,924 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@798fcedb counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 00:50:47,925 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 00:50:47,925 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:464)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	... 3 more
13 Dec 2020 00:50:52,928 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 00:50:52,948 INFO  [Thread-28] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:50:52,965 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607816980852.tmp
13 Dec 2020 00:50:52,973 WARN  [DataStreamer for file /twitterraw/FlumeData.1607816980852.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 00:50:52,976 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607816980852.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607816980852
13 Dec 2020 00:50:52,984 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 00:50:52,984 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607816979485
13 Dec 2020 00:50:52,984 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607817052984
13 Dec 2020 00:50:52,984 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 9
13 Dec 2020 00:50:52,984 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 00:50:52,984 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 00:50:52,985 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 00:50:52,985 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 10
13 Dec 2020 00:50:52,985 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 10
13 Dec 2020 00:50:52,987 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 00:50:52,987 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 909
13 Dec 2020 00:50:52,987 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 900
13 Dec 2020 00:50:52,991 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 1
13 Dec 2020 00:50:52,995 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@798fcedb counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 00:50:52,996 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel twitter
13 Dec 2020 00:50:52,997 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: twitter}
13 Dec 2020 00:50:52,998 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: twitter stopped
13 Dec 2020 00:50:52,998 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.start.time == 1607816979472
13 Dec 2020 00:50:52,998 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.stop.time == 1607817052998
13 Dec 2020 00:50:52,998 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.capacity == 10000
13 Dec 2020 00:50:53,003 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.current.size == 9
13 Dec 2020 00:50:53,003 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.attempt == 909
13 Dec 2020 00:50:53,003 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.put.success == 909
13 Dec 2020 00:50:53,004 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.attempt == 910
13 Dec 2020 00:50:53,004 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: twitter. channel.event.take.success == 900
13 Dec 2020 00:50:53,004 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 00:50:53,008 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 00:55:07,987 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 00:55:08,019 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 00:55:08,046 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,056 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:55:08,056 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,063 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:55:08,063 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,063 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,064 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:55:08,064 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,065 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:55:08,065 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 00:55:08,065 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,066 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,067 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,067 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,082 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,082 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 00:55:08,082 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 00:55:08,083 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,084 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,084 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,084 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,084 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,084 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,084 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 00:55:08,085 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 00:55:08,089 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 00:55:08,090 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 00:55:08,090 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 00:55:08,133 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 00:55:08,142 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 00:55:08,145 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 00:55:08,178 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 00:55:08,196 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 00:55:08,197 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 00:55:08,390 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 00:55:08,407 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, HDFS]
13 Dec 2020 00:55:08,411 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2fadf493 counterGroup:{ name:null counters:{} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 00:55:08,411 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 00:55:08,625 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 00:55:08,626 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 00:55:08,626 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 00:55:08,628 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 00:55:08,632 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 00:55:08,656 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 00:55:08,656 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 00:55:10,084 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 00:55:10,085 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 00:55:10,265 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 00:55:10,645 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817310263.tmp
13 Dec 2020 00:55:12,664 INFO  [Thread-9] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:55:17,635 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817310263.tmp
13 Dec 2020 00:55:17,670 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817310263.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817310263
13 Dec 2020 00:55:17,747 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817310264.tmp
13 Dec 2020 00:55:18,932 INFO  [Thread-12] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:55:24,563 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817310264.tmp
13 Dec 2020 00:55:24,586 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817310264.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817310264
13 Dec 2020 00:55:24,637 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817310265.tmp
13 Dec 2020 00:55:25,917 INFO  [Thread-14] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 00:55:31,222 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2fadf493 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 00:55:31,224 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 00:55:31,224 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 00:55:31,317 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 00:55:31,318 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 00:55:31,322 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 00:55:31,324 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2fadf493 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 00:55:31,325 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 00:55:31,326 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:464)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	... 3 more
13 Dec 2020 00:55:36,327 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 00:55:36,341 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817310265.tmp
13 Dec 2020 00:55:36,351 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817310265.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817310265
13 Dec 2020 00:55:36,357 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 00:55:36,358 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607817308656
13 Dec 2020 00:55:36,358 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607817336357
13 Dec 2020 00:55:36,358 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 2
13 Dec 2020 00:55:36,358 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 00:55:36,358 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 00:55:36,364 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 00:55:36,365 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 3
13 Dec 2020 00:55:36,365 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 3
13 Dec 2020 00:55:36,365 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 00:55:36,365 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 295
13 Dec 2020 00:55:36,366 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 200
13 Dec 2020 00:55:36,366 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 1
13 Dec 2020 00:55:36,368 INFO  [lifecycleSupervisor-1-6] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2fadf493 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 00:55:36,370 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 00:55:36,370 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 00:55:36,371 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 00:55:36,371 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607817308626
13 Dec 2020 00:55:36,372 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607817336371
13 Dec 2020 00:55:36,372 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 10000
13 Dec 2020 00:55:36,372 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 95
13 Dec 2020 00:55:36,373 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 295
13 Dec 2020 00:55:36,373 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 295
13 Dec 2020 00:55:36,376 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 296
13 Dec 2020 00:55:36,376 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 200
13 Dec 2020 00:55:36,377 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 00:55:36,385 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 01:00:20,409 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 01:00:20,417 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 01:00:20,431 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,432 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 01:00:20,433 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,433 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 01:00:20,433 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,434 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,434 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsaChannel
13 Dec 2020 01:00:20,434 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 01:00:20,435 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,436 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 01:00:20,436 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 01:00:20,436 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,437 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,438 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,441 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,441 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,441 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 01:00:20,442 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 01:00:20,443 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,443 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsaChannel
13 Dec 2020 01:00:20,443 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,443 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,444 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 01:00:20,444 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,444 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,445 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,445 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 01:00:20,445 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 01:00:20,449 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 01:00:20,450 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 01:00:20,453 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 01:00:20,477 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 01:00:20,479 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 01:00:20,480 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 01:00:20,491 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 01:00:20,500 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 01:00:20,501 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 01:00:20,558 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 01:00:20,565 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 01:00:20,571 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@778b3161 counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}} }
13 Dec 2020 01:00:20,571 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 01:00:20,762 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 01:00:20,763 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 01:00:20,764 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 01:00:20,765 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 01:00:20,772 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 01:00:20,774 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 01:00:20,775 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 01:00:22,267 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 01:00:22,268 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 01:00:22,459 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 01:00:22,761 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622456.tmp
13 Dec 2020 01:00:24,872 INFO  [Thread-9] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:00:29,972 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622456.tmp
13 Dec 2020 01:00:30,014 INFO  [hdfs-HDFS-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622456.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622456
13 Dec 2020 01:00:30,090 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622457.tmp
13 Dec 2020 01:00:31,187 INFO  [Thread-12] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:00:37,663 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622457.tmp
13 Dec 2020 01:00:37,679 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622457.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622457
13 Dec 2020 01:00:37,729 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622458.tmp
13 Dec 2020 01:00:39,308 INFO  [Thread-14] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:00:45,996 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622458.tmp
13 Dec 2020 01:00:46,011 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622458.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622458
13 Dec 2020 01:00:46,047 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622459.tmp
13 Dec 2020 01:00:46,983 INFO  [Thread-16] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:00:52,665 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622459.tmp
13 Dec 2020 01:00:52,682 INFO  [hdfs-HDFS-call-runner-7] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622459.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622459
13 Dec 2020 01:00:52,704 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622460.tmp
13 Dec 2020 01:00:54,050 INFO  [Thread-18] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:01:00,423 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622460.tmp
13 Dec 2020 01:01:00,437 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622460.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622460
13 Dec 2020 01:01:00,469 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622461.tmp
13 Dec 2020 01:01:01,736 INFO  [Thread-20] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:01:08,370 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622461.tmp
13 Dec 2020 01:01:08,385 INFO  [hdfs-HDFS-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622461.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622461
13 Dec 2020 01:01:08,423 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622462.tmp
13 Dec 2020 01:01:09,435 INFO  [Thread-22] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:01:17,110 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622462.tmp
13 Dec 2020 01:01:17,129 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622462.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622462
13 Dec 2020 01:01:17,173 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622463.tmp
13 Dec 2020 01:01:18,478 INFO  [Thread-24] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:01:24,900 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622463.tmp
13 Dec 2020 01:01:24,914 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622463.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622463
13 Dec 2020 01:01:24,936 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607817622464.tmp
13 Dec 2020 01:01:25,749 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@778b3161 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}} }
13 Dec 2020 01:01:25,749 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 01:01:25,749 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 01:01:25,750 WARN  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.warn:99)  - Stream already closed.
13 Dec 2020 01:01:25,776 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 01:01:25,779 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@778b3161 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 01:01:25,780 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 01:01:25,783 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:464)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	... 3 more
13 Dec 2020 01:01:30,792 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 01:01:30,797 INFO  [Thread-26] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 01:01:30,812 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607817622464.tmp
13 Dec 2020 01:01:30,826 WARN  [DataStreamer for file /twitterraw/FlumeData.1607817622464.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 01:01:30,828 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607817622464.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607817622464
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607817620775
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607817690833
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 8
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 9
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 9
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 01:01:30,833 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 819
13 Dec 2020 01:01:30,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 800
13 Dec 2020 01:01:30,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 1
13 Dec 2020 01:01:30,834 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@778b3161 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 01:01:30,834 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 01:01:30,834 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 01:01:30,835 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 01:01:30,835 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607817620763
13 Dec 2020 01:01:30,837 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607817690835
13 Dec 2020 01:01:30,839 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 100
13 Dec 2020 01:01:30,840 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 19
13 Dec 2020 01:01:30,840 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 819
13 Dec 2020 01:01:30,846 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 819
13 Dec 2020 01:01:30,846 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 820
13 Dec 2020 01:01:30,846 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 800
13 Dec 2020 01:01:30,847 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 01:01:30,850 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:10:29,802 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:10:29,815 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:10:29,828 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,829 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,830 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:10:29,830 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,830 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,830 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:10:29,831 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,831 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:10:29,835 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:10:29,836 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 02:10:29,836 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,836 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,837 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:10:29,841 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,841 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,841 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:10:29,842 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:10:29,842 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,843 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,844 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,844 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:10:29,845 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,845 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,846 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,847 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:10:29,847 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:10:29,848 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:10:29,848 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:10:29,848 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:10:29,874 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 02:10:29,877 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:10:29,877 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:10:29,890 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:10:29,896 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:10:29,905 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:10:29,960 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:10:29,977 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 02:10:29,979 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59db4644 counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}} }
13 Dec 2020 02:10:29,981 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:10:30,188 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:10:30,188 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:10:30,189 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 02:10:30,190 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:10:30,197 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:10:30,200 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 02:10:30,200 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 02:10:31,668 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:10:31,669 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:10:31,875 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:10:32,233 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607821831868.tmp
13 Dec 2020 02:10:34,565 INFO  [Thread-9] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:10:39,473 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607821831868.tmp
13 Dec 2020 02:10:39,506 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607821831868.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607821831868
13 Dec 2020 02:10:39,604 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607821831869.tmp
13 Dec 2020 02:10:40,672 INFO  [Thread-12] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:10:48,388 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607821831869.tmp
13 Dec 2020 02:10:48,407 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607821831869.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607821831869
13 Dec 2020 02:10:48,467 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607821831870.tmp
13 Dec 2020 02:10:49,751 INFO  [Thread-14] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:10:55,113 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59db4644 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}} }
13 Dec 2020 02:10:55,113 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:10:55,113 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:10:55,219 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:10:55,220 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:10:55,228 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 02:10:55,233 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59db4644 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:10:55,234 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:10:55,235 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:464)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	... 3 more
13 Dec 2020 02:11:00,236 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 02:11:00,255 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607821831870.tmp
13 Dec 2020 02:11:00,272 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607821831870.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607821831870
13 Dec 2020 02:11:00,276 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 02:11:00,277 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607821830200
13 Dec 2020 02:11:00,277 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607821860276
13 Dec 2020 02:11:00,278 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 2
13 Dec 2020 02:11:00,283 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 02:11:00,283 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 02:11:00,283 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 02:11:00,283 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 3
13 Dec 2020 02:11:00,284 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 3
13 Dec 2020 02:11:00,284 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 02:11:00,287 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 286
13 Dec 2020 02:11:00,287 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 200
13 Dec 2020 02:11:00,289 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 1
13 Dec 2020 02:11:00,291 INFO  [lifecycleSupervisor-1-6] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@59db4644 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:11:00,293 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:11:00,293 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:11:00,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:11:00,294 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607821830188
13 Dec 2020 02:11:00,294 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607821860293
13 Dec 2020 02:11:00,294 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 10000
13 Dec 2020 02:11:00,294 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 86
13 Dec 2020 02:11:00,299 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 286
13 Dec 2020 02:11:00,299 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 286
13 Dec 2020 02:11:00,299 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 287
13 Dec 2020 02:11:00,299 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 200
13 Dec 2020 02:11:00,300 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:11:00,304 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:13:51,187 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:13:51,199 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:13:51,213 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,214 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,215 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:13:51,215 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,219 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,219 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:13:51,219 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,220 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:13:51,220 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:13:51,221 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 02:13:51,221 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,221 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,222 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,223 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:13:51,231 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,231 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,231 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:13:51,232 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:13:51,233 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,233 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,233 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,233 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:13:51,234 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,234 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,234 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,234 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:13:51,235 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:13:51,235 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:13:51,236 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:13:51,240 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:13:51,264 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 02:13:51,265 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:13:51,265 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:13:51,284 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:13:51,292 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:13:51,297 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:13:51,370 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:13:51,381 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 02:13:51,383 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@25b1fd8f counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}} }
13 Dec 2020 02:13:51,384 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:13:51,589 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:13:51,589 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:13:51,590 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 02:13:51,591 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:13:51,603 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:13:51,606 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 02:13:51,607 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 02:13:53,072 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:13:53,072 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:13:53,367 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:13:53,648 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822033363.tmp
13 Dec 2020 02:13:55,495 INFO  [Thread-9] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:14:01,112 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822033363.tmp
13 Dec 2020 02:14:01,163 INFO  [hdfs-HDFS-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822033363.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822033363
13 Dec 2020 02:14:01,250 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822033364.tmp
13 Dec 2020 02:14:02,843 INFO  [Thread-12] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:14:06,506 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@25b1fd8f counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}} }
13 Dec 2020 02:14:06,507 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:14:06,507 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:14:06,532 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:14:06,533 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:14:06,545 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 02:14:06,547 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@25b1fd8f counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:14:06,548 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:14:06,548 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:464)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	... 3 more
13 Dec 2020 02:14:11,549 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 02:14:11,555 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822033364.tmp
13 Dec 2020 02:14:11,571 INFO  [hdfs-HDFS-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822033364.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822033364
13 Dec 2020 02:14:11,583 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 02:14:11,583 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607822031607
13 Dec 2020 02:14:11,584 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607822051583
13 Dec 2020 02:14:11,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 1
13 Dec 2020 02:14:11,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 02:14:11,586 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 02:14:11,586 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 02:14:11,586 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 2
13 Dec 2020 02:14:11,587 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 2
13 Dec 2020 02:14:11,587 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 02:14:11,587 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 156
13 Dec 2020 02:14:11,587 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 100
13 Dec 2020 02:14:11,590 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 1
13 Dec 2020 02:14:11,590 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@25b1fd8f counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:14:11,591 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:14:11,591 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:14:11,592 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:14:11,592 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607822031589
13 Dec 2020 02:14:11,592 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607822051592
13 Dec 2020 02:14:11,593 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 10000
13 Dec 2020 02:14:11,593 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 56
13 Dec 2020 02:14:11,593 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 156
13 Dec 2020 02:14:11,593 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 156
13 Dec 2020 02:14:11,595 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 157
13 Dec 2020 02:14:11,595 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 100
13 Dec 2020 02:14:11,595 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:14:11,599 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:15:25,753 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:15:25,760 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:15:25,770 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,771 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,772 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:15:25,772 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,772 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,772 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:15:25,773 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,773 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:15:25,774 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:15:25,774 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka, HDFS Agent: TwitterAgent
13 Dec 2020 02:15:25,774 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,775 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,776 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,776 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:15:25,779 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,780 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,780 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:15:25,781 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:15:25,781 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,782 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,782 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,782 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:15:25,782 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,782 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,783 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,783 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:15:25,783 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:15:25,784 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:15:25,784 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:15:25,787 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:15:25,806 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sinkToKafka,
13 Dec 2020 02:15:25,807 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:15:25,807 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:15:25,821 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:15:25,824 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:15:25,828 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:15:25,880 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:15:25,889 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 02:15:25,895 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@25b1fd8f counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}} }
13 Dec 2020 02:15:25,895 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:15:26,080 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:15:26,081 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:15:26,081 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 02:15:26,082 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:15:26,091 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:15:26,097 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 02:15:26,097 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 02:15:27,312 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:15:27,312 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:15:27,445 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:15:27,626 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822127446.tmp
13 Dec 2020 02:15:29,688 INFO  [Thread-9] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:15:35,843 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822127446.tmp
13 Dec 2020 02:15:35,870 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822127446.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822127446
13 Dec 2020 02:15:35,939 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822127447.tmp
13 Dec 2020 02:15:37,207 INFO  [Thread-12] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:15:44,554 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822127447.tmp
13 Dec 2020 02:15:44,567 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822127447.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822127447
13 Dec 2020 02:15:44,611 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822127448.tmp
13 Dec 2020 02:15:45,688 INFO  [Thread-14] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:15:48,223 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@25b1fd8f counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}} }
13 Dec 2020 02:15:48,223 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:15:48,223 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:15:48,302 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:15:48,305 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 02:15:48,306 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@25b1fd8f counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:15:48,307 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:15:48,310 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:464)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:708)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:477)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	... 3 more
13 Dec 2020 02:15:48,306 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:15:53,312 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 02:15:53,321 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822127448.tmp
13 Dec 2020 02:15:53,343 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822127448.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822127448
13 Dec 2020 02:15:53,352 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 02:15:53,352 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607822126097
13 Dec 2020 02:15:53,352 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607822153352
13 Dec 2020 02:15:53,352 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 2
13 Dec 2020 02:15:53,353 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 0
13 Dec 2020 02:15:53,353 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 02:15:53,353 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 02:15:53,353 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 3
13 Dec 2020 02:15:53,353 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 3
13 Dec 2020 02:15:53,353 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 02:15:53,353 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 254
13 Dec 2020 02:15:53,353 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 200
13 Dec 2020 02:15:53,354 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 1
13 Dec 2020 02:15:53,354 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@25b1fd8f counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:15:53,355 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:15:53,356 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:15:53,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:15:53,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607822126081
13 Dec 2020 02:15:53,356 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607822153356
13 Dec 2020 02:15:53,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000
13 Dec 2020 02:15:53,359 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 54
13 Dec 2020 02:15:53,365 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 254
13 Dec 2020 02:15:53,366 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 254
13 Dec 2020 02:15:53,366 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 255
13 Dec 2020 02:15:53,366 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 200
13 Dec 2020 02:15:53,367 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:15:53,369 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:16:18,181 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:16:18,192 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:16:18,202 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:18,203 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,203 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:16:18,204 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:18,204 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:18,204 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:16:18,205 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,205 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:16:18,206 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka Agent: TwitterAgent
13 Dec 2020 02:16:18,206 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,206 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,206 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:18,208 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:18,208 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,211 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:16:18,211 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:16:18,212 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,212 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,213 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,213 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:18,213 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:18,214 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:16:18,214 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:16:18,214 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:18,214 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:18,215 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:16:18,221 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.isValid:373)  - Agent configuration for 'TwitterAgent' does not contain any valid channels. Marking it as invalid.
13 Dec 2020 02:16:18,222 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:154)  - Agent configuration invalid for agent 'TwitterAgent'. It will be removed.
13 Dec 2020 02:16:18,222 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: []
13 Dec 2020 02:16:18,222 WARN  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:139)  - No configuration found for this host:TwitterAgent
13 Dec 2020 02:16:18,228 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{} sinkRunners:{} channels:{} }
13 Dec 2020 02:16:35,212 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{} sinkRunners:{} channels:{} }
13 Dec 2020 02:16:35,215 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:16:35,219 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:16:48,371 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:16:48,380 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:16:48,391 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:48,392 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,392 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:16:48,393 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:48,393 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:48,393 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:16:48,394 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,399 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:16:48,400 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka Agent: TwitterAgent
13 Dec 2020 02:16:48,401 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,401 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,402 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:48,403 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:48,403 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,407 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:16:48,407 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:16:48,407 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,408 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,409 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,409 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:48,409 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:48,409 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:16:48,410 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:16:48,410 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:16:48,410 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:16:48,411 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:16:48,417 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.isValid:373)  - Agent configuration for 'TwitterAgent' does not contain any valid channels. Marking it as invalid.
13 Dec 2020 02:16:48,418 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:154)  - Agent configuration invalid for agent 'TwitterAgent'. It will be removed.
13 Dec 2020 02:16:48,418 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: []
13 Dec 2020 02:16:48,419 WARN  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:139)  - No configuration found for this host:TwitterAgent
13 Dec 2020 02:16:48,422 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{} sinkRunners:{} channels:{} }
13 Dec 2020 02:17:01,055 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{} sinkRunners:{} channels:{} }
13 Dec 2020 02:17:01,059 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:17:01,063 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:17:24,319 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:17:24,334 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:17:24,344 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:17:24,345 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,346 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:17:24,346 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:17:24,346 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:17:24,347 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:17:24,347 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,347 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:17:24,348 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka Agent: TwitterAgent
13 Dec 2020 02:17:24,349 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,349 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,349 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:17:24,350 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:17:24,350 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,353 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:17:24,354 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:17:24,354 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,355 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,355 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,356 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:17:24,356 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:17:24,359 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:17:24,360 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:17:24,360 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:17:24,360 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:17:24,361 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:17:24,368 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.isValid:373)  - Agent configuration for 'TwitterAgent' does not contain any valid channels. Marking it as invalid.
13 Dec 2020 02:17:24,368 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:154)  - Agent configuration invalid for agent 'TwitterAgent'. It will be removed.
13 Dec 2020 02:17:24,369 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: []
13 Dec 2020 02:17:24,369 WARN  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:139)  - No configuration found for this host:TwitterAgent
13 Dec 2020 02:17:24,378 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{} sinkRunners:{} channels:{} }
13 Dec 2020 02:17:40,007 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{} sinkRunners:{} channels:{} }
13 Dec 2020 02:17:40,010 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:17:40,013 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:18:17,988 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:18:17,997 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:18:18,008 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:18:18,009 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:18:18,009 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,009 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,010 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,010 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:18:18,010 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:18:18,011 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,012 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,012 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:18:18,012 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka Agent: TwitterAgent
13 Dec 2020 02:18:18,012 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:18:18,014 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,014 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,017 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:18:18,017 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,018 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:18,018 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:18:18,030 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:18:18,031 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:18:18,044 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:18:18,049 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:18:18,050 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:18:18,102 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:18:18,112 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:18:18,112 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:18:18,113 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:18:18,118 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:18:18,120 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2adb7585 counterGroup:{ name:null counters:{} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:18:18,120 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:18:18,319 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:18:18,319 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:18:18,320 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:18:18,320 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:18:18,330 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:18:18,383 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:18:18,561 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:18:18,562 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:18:18,570 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:18:18,572 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:18:19,670 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:18:19,671 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:18:19,937 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:18:26,674 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2adb7585 counterGroup:{ name:null counters:{} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:18:26,674 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:18:26,674 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:18:26,722 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:18:26,722 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:18:26,725 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:18:26,727 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2adb7585 counterGroup:{ name:null counters:{} } }
13 Dec 2020 02:18:26,728 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:18:26,729 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:18:31,730 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:18:31,756 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:18:31,756 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822298572
13 Dec 2020 02:18:31,756 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607822311756
13 Dec 2020 02:18:31,756 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 0
13 Dec 2020 02:18:31,756 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:18:31,756 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:18:31,756 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:18:31,757 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:18:31,757 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:18:31,757 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:18:31,757 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:18:31,757 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:18:31,757 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 83
13 Dec 2020 02:18:31,757 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 0
13 Dec 2020 02:18:31,757 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:18:31,758 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=0, sink.event.drain.attempt=83, sink.batch.complete=0, sink.event.drain.sucess=0, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:18:31,758 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2adb7585 counterGroup:{ name:null counters:{runner.deliveryErrors=1} } }
13 Dec 2020 02:18:31,759 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:18:31,759 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:18:31,759 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:18:31,760 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822298319
13 Dec 2020 02:18:31,760 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607822311759
13 Dec 2020 02:18:31,760 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:18:31,769 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 83
13 Dec 2020 02:18:31,769 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 83
13 Dec 2020 02:18:31,769 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 83
13 Dec 2020 02:18:31,769 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 84
13 Dec 2020 02:18:31,770 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 0
13 Dec 2020 02:18:31,770 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:18:31,772 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:18:41,757 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:18:41,765 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:18:41,776 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:18:41,777 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:18:41,777 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,778 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,778 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,778 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:18:41,779 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:18:41,782 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,783 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,783 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:18:41,783 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka Agent: TwitterAgent
13 Dec 2020 02:18:41,783 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:18:41,784 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,785 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,787 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:18:41,788 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,790 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:18:41,791 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:18:41,803 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:18:41,804 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:18:41,818 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:18:41,821 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:18:41,822 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:18:41,867 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:18:41,871 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:18:41,872 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:18:41,872 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:18:41,877 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:18:41,879 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2507518c counterGroup:{ name:null counters:{} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:18:41,879 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:18:42,082 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:18:42,083 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:18:42,083 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:18:42,084 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:18:42,093 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:18:42,167 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:18:42,335 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:18:42,339 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:18:42,348 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:18:42,350 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:18:43,660 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:18:43,661 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:18:44,160 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:18:48,630 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2507518c counterGroup:{ name:null counters:{} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:18:48,631 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:18:48,631 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:18:48,642 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:18:48,645 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:18:48,645 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:18:48,648 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2507518c counterGroup:{ name:null counters:{} } }
13 Dec 2020 02:18:48,649 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:18:48,650 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:18:53,652 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:18:53,673 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:18:53,673 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822322350
13 Dec 2020 02:18:53,674 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607822333673
13 Dec 2020 02:18:53,674 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 0
13 Dec 2020 02:18:53,674 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:18:53,675 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:18:53,675 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:18:53,675 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:18:53,676 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:18:53,676 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:18:53,676 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:18:53,676 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:18:53,679 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 70
13 Dec 2020 02:18:53,683 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 0
13 Dec 2020 02:18:53,683 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:18:53,683 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=0, sink.event.drain.attempt=70, sink.batch.complete=0, sink.event.drain.sucess=0, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:18:53,684 INFO  [lifecycleSupervisor-1-6] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@2507518c counterGroup:{ name:null counters:{runner.deliveryErrors=1} } }
13 Dec 2020 02:18:53,684 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:18:53,684 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:18:53,685 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:18:53,685 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822322083
13 Dec 2020 02:18:53,685 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607822333685
13 Dec 2020 02:18:53,685 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:18:53,686 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 70
13 Dec 2020 02:18:53,687 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 70
13 Dec 2020 02:18:53,687 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 70
13 Dec 2020 02:18:53,687 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 71
13 Dec 2020 02:18:53,687 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 0
13 Dec 2020 02:18:53,688 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:18:53,690 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:19:04,257 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:19:04,274 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:19:04,289 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:19:04,290 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:19:04,292 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,293 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,293 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,293 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:19:04,294 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:19:04,295 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,296 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,296 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:19:04,296 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka Agent: TwitterAgent
13 Dec 2020 02:19:04,297 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:19:04,299 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,299 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,303 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:19:04,303 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,303 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:19:04,304 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:19:04,326 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:19:04,327 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:19:04,339 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:19:04,346 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:19:04,349 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:19:04,401 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:19:04,412 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:19:04,413 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:19:04,413 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:19:04,418 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:19:04,421 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6c7ce0df counterGroup:{ name:null counters:{} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:19:04,421 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:19:04,626 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:19:04,627 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:19:04,627 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:19:04,628 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:19:04,640 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:19:04,687 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:19:04,860 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:19:04,862 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:19:04,872 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:19:04,873 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:19:06,001 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:19:06,002 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:19:06,372 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:19:06,402 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:19:15,111 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6c7ce0df counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:19:15,118 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:19:15,118 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:19:15,343 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:19:15,347 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:19:15,348 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:19:15,350 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6c7ce0df counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:19:15,351 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:19:15,352 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:19:20,353 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:19:20,378 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:19:20,378 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822344873
13 Dec 2020 02:19:20,378 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607822360378
13 Dec 2020 02:19:20,378 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 8517
13 Dec 2020 02:19:20,379 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:19:20,379 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:19:20,379 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:19:20,380 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:19:20,380 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:19:20,380 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:19:20,380 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:19:20,381 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:19:20,381 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 122
13 Dec 2020 02:19:20,381 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 100
13 Dec 2020 02:19:20,381 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:19:20,383 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=8517, sink.event.drain.attempt=122, sink.batch.complete=0, sink.event.drain.sucess=100, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:19:20,383 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@6c7ce0df counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:19:20,384 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:19:20,384 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:19:20,384 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:19:20,384 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822344627
13 Dec 2020 02:19:20,385 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607822360384
13 Dec 2020 02:19:20,386 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:19:20,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 22
13 Dec 2020 02:19:20,387 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 122
13 Dec 2020 02:19:20,388 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 122
13 Dec 2020 02:19:20,388 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 123
13 Dec 2020 02:19:20,388 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 100
13 Dec 2020 02:19:20,394 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:19:20,401 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:20:26,146 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:20:26,158 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:20:26,169 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,170 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,170 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:20:26,172 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,172 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,172 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:20:26,173 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,176 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:20:26,177 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 02:20:26,177 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,177 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,177 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,179 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,179 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,182 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:20:26,182 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:20:26,182 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,183 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,183 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,184 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,184 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,186 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,187 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:20:26,187 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:20:26,187 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:20:26,187 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:20:26,188 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:20:26,209 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 02:20:26,210 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:20:26,211 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:20:26,223 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:20:26,227 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:20:26,229 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:20:26,273 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:20:26,283 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:20:26,291 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:20:26,291 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:20:26,295 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:20:26,300 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka, HDFS]
13 Dec 2020 02:20:26,303 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:20:26,303 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:20:26,496 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:20:26,497 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:20:26,497 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:20:26,498 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 02:20:26,500 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:20:26,519 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:20:26,521 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 02:20:26,522 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 02:20:26,574 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:20:26,774 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:20:26,775 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:20:26,781 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:20:26,782 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:20:28,019 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:20:28,020 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:20:28,235 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:20:28,303 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:20:28,679 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822428236.tmp
13 Dec 2020 02:20:28,798 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:20:28,856 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:20:33,332 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:20:42,088 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{} } }} channels:{kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:20:42,088 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:20:42,089 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:20:42,103 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:20:42,104 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:20:42,107 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:20:42,110 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:20:42,110 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:20:42,111 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:20:47,112 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:20:47,147 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822426782
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607822447147
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 13547
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:20:47,148 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:20:47,149 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:20:47,149 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:20:47,149 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 111
13 Dec 2020 02:20:47,149 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 100
13 Dec 2020 02:20:47,149 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:20:47,149 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=13547, sink.event.drain.attempt=111, sink.batch.complete=0, sink.event.drain.sucess=100, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:20:47,150 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:20:47,151 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 02:20:47,151 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:20:47,152 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 02:20:47,153 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822428236.tmp
13 Dec 2020 02:20:47,186 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822428236.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822428236
13 Dec 2020 02:20:47,211 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 02:20:47,211 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607822426522
13 Dec 2020 02:20:47,211 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607822447211
13 Dec 2020 02:20:47,211 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 02:20:47,211 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 81
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 81
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 02:20:47,212 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822426497
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607822447213
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 0
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 181
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 181
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 195
13 Dec 2020 02:20:47,213 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 181
13 Dec 2020 02:20:47,214 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:20:47,233 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:21:40,865 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:21:40,873 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:21:40,885 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,886 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,886 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:21:40,887 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,888 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,888 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:21:40,889 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,889 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:21:40,890 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:21:40,890 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 02:21:40,891 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,891 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,892 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,892 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:21:40,895 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,895 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:21:40,896 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:21:40,897 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,897 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,898 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,898 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:21:40,901 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,901 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,901 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,902 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:21:40,902 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:21:40,903 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:21:40,903 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:21:40,905 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:21:40,920 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 02:21:40,920 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:21:40,920 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:21:40,935 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:21:40,939 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:21:40,943 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:21:40,943 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:21:40,944 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:21:41,030 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:21:41,041 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:21:41,041 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:21:41,047 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:21:41,050 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:21:41,066 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 02:21:41,067 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:21:41,071 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:21:41,071 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:21:41,084 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:21:41,353 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:21:41,354 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:21:41,360 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:21:41,360 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:21:41,360 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:21:41,370 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 02:21:41,371 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:21:41,379 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 02:21:41,382 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 02:21:41,387 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:21:41,451 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:21:41,634 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:21:41,640 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:21:41,647 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:21:41,648 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:21:42,820 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:21:42,821 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:21:43,003 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:21:43,061 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:21:43,518 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822502999.tmp
13 Dec 2020 02:21:43,737 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:21:43,784 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:21:46,103 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:21:50,152 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:21:50,153 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:21:50,154 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:21:50,197 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:21:50,201 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:21:50,201 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{} } }
13 Dec 2020 02:21:50,202 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:21:50,205 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:21:50,201 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:21:55,208 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:21:55,238 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:21:55,239 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822501648
13 Dec 2020 02:21:55,239 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607822515238
13 Dec 2020 02:21:55,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 0
13 Dec 2020 02:21:55,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:21:55,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:21:55,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:21:55,241 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:21:55,241 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:21:55,241 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:21:55,241 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:21:55,242 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:21:55,242 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 94
13 Dec 2020 02:21:55,242 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 0
13 Dec 2020 02:21:55,242 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:21:55,243 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=0, sink.event.drain.attempt=94, sink.batch.complete=0, sink.event.drain.sucess=0, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:21:55,247 INFO  [lifecycleSupervisor-1-4] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.deliveryErrors=1} } }
13 Dec 2020 02:21:55,247 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 02:21:55,248 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:21:55,249 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 02:21:55,249 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822502999.tmp
13 Dec 2020 02:21:55,291 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822502999.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822502999
13 Dec 2020 02:21:55,311 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 02:21:55,311 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607822501382
13 Dec 2020 02:21:55,311 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607822515311
13 Dec 2020 02:21:55,311 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 02:21:55,311 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 94
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 94
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 02:21:55,312 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607822501360
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607822515313
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 94
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 94
13 Dec 2020 02:21:55,313 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 96
13 Dec 2020 02:21:55,314 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 94
13 Dec 2020 02:21:55,316 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:21:55,317 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:21:55,318 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:21:55,319 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822501354
13 Dec 2020 02:21:55,320 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607822515318
13 Dec 2020 02:21:55,322 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:21:55,322 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 94
13 Dec 2020 02:21:55,322 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 94
13 Dec 2020 02:21:55,323 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 94
13 Dec 2020 02:21:55,325 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 95
13 Dec 2020 02:21:55,325 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 0
13 Dec 2020 02:21:55,332 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:21:55,336 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:22:40,494 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:22:40,501 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:22:40,516 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,517 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,518 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:22:40,518 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,518 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,519 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:22:40,519 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,519 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:22:40,520 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:22:40,522 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 02:22:40,522 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,522 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,523 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,523 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:22:40,526 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,527 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,527 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:22:40,530 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:22:40,530 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,531 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,531 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,531 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:22:40,531 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,531 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,532 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,532 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:22:40,532 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:22:40,533 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:22:40,533 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:22:40,536 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:22:40,559 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 02:22:40,560 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:22:40,560 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:22:40,574 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:22:40,581 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:22:40,585 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:22:40,586 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:22:40,587 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:22:40,646 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:22:40,652 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:22:40,652 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:22:40,655 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:22:40,661 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:22:40,674 ERROR [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadSinks:469)  - Sink HDFS has been removed due to an error during configuration
java.lang.InstantiationException: Incompatible sink and channel settings defined. sink's batch size is greater than the channels transaction capacity. Sink: HDFS, batch size = 1000, channel hdfsChannel, transaction capacity = 100
	at org.apache.flume.node.AbstractConfigurationProvider.checkSinkChannelCompatibility(AbstractConfigurationProvider.java:403)
	at org.apache.flume.node.AbstractConfigurationProvider.loadSinks(AbstractConfigurationProvider.java:462)
	at org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:106)
	at org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:145)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:22:40,678 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter]
13 Dec 2020 02:22:40,678 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:22:40,680 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@3d76a2fd counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:22:40,681 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:22:40,691 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:22:40,893 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:22:40,893 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:22:40,896 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:22:40,896 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:22:40,897 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:22:40,898 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:22:40,904 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:22:40,955 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:22:41,207 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:22:41,212 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:22:41,218 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:22:41,219 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:22:42,359 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:22:42,360 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:22:42,669 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:23:17,517 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@3d76a2fd counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:23:17,527 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:23:17,528 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:23:17,575 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:23:17,575 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:23:17,579 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:23:17,579 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@3d76a2fd counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:23:17,580 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:23:17,581 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:23:22,582 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:23:22,610 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:23:22,610 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822561219
13 Dec 2020 02:23:22,611 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607822602610
13 Dec 2020 02:23:22,611 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 33478
13 Dec 2020 02:23:22,612 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:23:22,612 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:23:22,613 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:23:22,613 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:23:22,613 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:23:22,613 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:23:22,614 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:23:22,614 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:23:22,614 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 446
13 Dec 2020 02:23:22,615 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 400
13 Dec 2020 02:23:22,615 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:23:22,618 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=33478, sink.event.drain.attempt=446, sink.batch.complete=0, sink.event.drain.sucess=400, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:23:22,619 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@3d76a2fd counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:23:22,619 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:23:22,627 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:23:22,627 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:23:22,633 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607822560896
13 Dec 2020 02:23:22,633 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607822602627
13 Dec 2020 02:23:22,633 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000
13 Dec 2020 02:23:22,634 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 446
13 Dec 2020 02:23:22,634 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 446
13 Dec 2020 02:23:22,634 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 446
13 Dec 2020 02:23:22,634 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 0
13 Dec 2020 02:23:22,634 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 0
13 Dec 2020 02:23:22,634 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:23:22,634 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:23:22,634 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:23:22,635 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822560893
13 Dec 2020 02:23:22,635 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607822602634
13 Dec 2020 02:23:22,636 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:23:22,636 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 46
13 Dec 2020 02:23:22,637 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 446
13 Dec 2020 02:23:22,637 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 446
13 Dec 2020 02:23:22,637 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 447
13 Dec 2020 02:23:22,638 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 400
13 Dec 2020 02:23:22,638 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:23:22,641 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:24:16,155 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:24:16,163 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:24:16,178 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,179 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,179 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:24:16,180 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,180 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,180 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:24:16,181 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,181 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:24:16,182 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:24:16,183 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 02:24:16,183 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,184 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,186 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,186 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:24:16,190 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,190 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,191 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:24:16,192 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:24:16,192 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,192 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,193 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,193 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:24:16,193 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,193 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,194 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,194 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:24:16,195 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:24:16,195 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:16,195 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:16,196 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:24:16,221 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 02:24:16,221 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:24:16,224 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:24:16,240 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:24:16,243 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:24:16,249 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:24:16,249 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:24:16,251 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:24:16,317 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:24:16,328 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:24:16,329 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:24:16,329 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:24:16,336 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:24:16,348 ERROR [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadSinks:469)  - Sink HDFS has been removed due to an error during configuration
java.lang.InstantiationException: Incompatible sink and channel settings defined. sink's batch size is greater than the channels transaction capacity. Sink: HDFS, batch size = 1000, channel hdfsChannel, transaction capacity = 100
	at org.apache.flume.node.AbstractConfigurationProvider.checkSinkChannelCompatibility(AbstractConfigurationProvider.java:403)
	at org.apache.flume.node.AbstractConfigurationProvider.loadSinks(AbstractConfigurationProvider.java:462)
	at org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:106)
	at org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:145)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:24:16,355 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter]
13 Dec 2020 02:24:16,355 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:24:16,360 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@158ad747 counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:24:16,360 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:24:16,369 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:24:16,580 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:24:16,580 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:24:16,586 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:24:16,586 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:24:16,587 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:24:16,590 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:24:16,601 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:24:16,650 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:24:16,819 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:24:16,825 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:24:16,831 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:24:16,831 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:24:17,905 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:24:17,907 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:24:18,240 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:24:18,251 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:24:37,246 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@158ad747 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:24:37,255 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:24:37,256 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:24:37,412 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:24:37,413 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:24:37,415 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:24:37,416 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@158ad747 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:24:37,416 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:24:37,417 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:24:42,419 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:24:42,443 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:24:42,443 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822656831
13 Dec 2020 02:24:42,443 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607822682443
13 Dec 2020 02:24:42,443 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 15111
13 Dec 2020 02:24:42,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:24:42,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:24:42,444 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:24:42,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:24:42,445 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:24:42,446 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:24:42,446 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:24:42,446 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:24:42,446 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 267
13 Dec 2020 02:24:42,447 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 200
13 Dec 2020 02:24:42,447 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:24:42,447 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=15111, sink.event.drain.attempt=267, sink.batch.complete=0, sink.event.drain.sucess=200, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:24:42,448 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@158ad747 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:24:42,448 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:24:42,449 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:24:42,449 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:24:42,449 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607822656586
13 Dec 2020 02:24:42,459 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607822682449
13 Dec 2020 02:24:42,459 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000
13 Dec 2020 02:24:42,459 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 267
13 Dec 2020 02:24:42,460 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 267
13 Dec 2020 02:24:42,460 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 267
13 Dec 2020 02:24:42,460 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 0
13 Dec 2020 02:24:42,460 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 0
13 Dec 2020 02:24:42,461 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:24:42,461 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:24:42,461 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:24:42,462 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822656580
13 Dec 2020 02:24:42,462 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607822682461
13 Dec 2020 02:24:42,462 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:24:42,463 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 67
13 Dec 2020 02:24:42,463 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 267
13 Dec 2020 02:24:42,463 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 267
13 Dec 2020 02:24:42,463 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 268
13 Dec 2020 02:24:42,464 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 200
13 Dec 2020 02:24:42,464 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:24:42,470 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:24:58,832 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:24:58,841 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:24:58,852 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,853 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,854 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:24:58,854 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,854 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,855 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:24:58,856 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,856 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:24:58,857 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:24:58,857 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 02:24:58,858 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,858 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,860 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:24:58,863 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,863 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,864 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:24:58,864 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:24:58,865 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,865 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,865 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,866 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:24:58,866 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,866 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,867 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,867 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:24:58,867 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:24:58,868 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:24:58,868 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:24:58,868 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:24:58,890 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 02:24:58,891 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:24:58,892 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:24:58,906 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:24:58,914 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:24:58,920 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:24:58,920 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:24:58,921 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:24:58,992 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:24:59,000 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:24:59,001 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:24:59,001 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:24:59,010 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:24:59,028 ERROR [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadSinks:469)  - Sink HDFS has been removed due to an error during configuration
java.lang.InstantiationException: Incompatible sink and channel settings defined. sink's batch size is greater than the channels transaction capacity. Sink: HDFS, batch size = 1000, channel hdfsChannel, transaction capacity = 100
	at org.apache.flume.node.AbstractConfigurationProvider.checkSinkChannelCompatibility(AbstractConfigurationProvider.java:403)
	at org.apache.flume.node.AbstractConfigurationProvider.loadSinks(AbstractConfigurationProvider.java:462)
	at org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:106)
	at org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:145)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:24:59,035 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter]
13 Dec 2020 02:24:59,035 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:24:59,042 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@ae5435c counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:24:59,042 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:24:59,050 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:24:59,333 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:24:59,334 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:24:59,343 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:24:59,343 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:24:59,344 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:24:59,359 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:24:59,367 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:24:59,429 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:24:59,618 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:24:59,622 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:24:59,626 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:24:59,629 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:25:00,713 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:25:00,715 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:25:01,457 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:25:01,478 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:25:09,685 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@ae5435c counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:25:09,692 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:25:09,692 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:25:09,814 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:25:09,814 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:25:09,818 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:25:09,818 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@ae5435c counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:25:09,818 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:25:09,819 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:25:14,820 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:25:14,844 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:25:14,844 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822699629
13 Dec 2020 02:25:14,844 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607822714844
13 Dec 2020 02:25:14,844 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 7974
13 Dec 2020 02:25:14,844 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:25:14,844 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 129
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 100
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:25:14,845 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=7974, sink.event.drain.attempt=129, sink.batch.complete=0, sink.event.drain.sucess=100, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:25:14,846 INFO  [lifecycleSupervisor-1-4] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@ae5435c counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:25:14,849 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:25:14,849 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:25:14,849 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:25:14,850 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607822699343
13 Dec 2020 02:25:14,851 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607822714849
13 Dec 2020 02:25:14,851 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000
13 Dec 2020 02:25:14,851 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 129
13 Dec 2020 02:25:14,852 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 129
13 Dec 2020 02:25:14,852 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 129
13 Dec 2020 02:25:14,852 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 0
13 Dec 2020 02:25:14,852 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 0
13 Dec 2020 02:25:14,853 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:25:14,853 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:25:14,855 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:25:14,855 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822699334
13 Dec 2020 02:25:14,858 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607822714855
13 Dec 2020 02:25:14,859 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:25:14,859 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 29
13 Dec 2020 02:25:14,859 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 129
13 Dec 2020 02:25:14,860 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 129
13 Dec 2020 02:25:14,863 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 130
13 Dec 2020 02:25:14,863 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 100
13 Dec 2020 02:25:14,864 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:25:14,867 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:25:19,593 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:25:19,600 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:25:19,615 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,615 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,616 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:25:19,617 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,617 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,618 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:25:19,618 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,621 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:25:19,622 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:25:19,622 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 02:25:19,623 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,623 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,624 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,624 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:25:19,627 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,627 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,628 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:25:19,628 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:25:19,629 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,629 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,629 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,629 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:25:19,629 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,630 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,630 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,630 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:25:19,631 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:25:19,631 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:25:19,631 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:25:19,632 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:25:19,651 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 02:25:19,651 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:25:19,652 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:25:19,663 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:25:19,666 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:25:19,671 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:25:19,671 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:25:19,672 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:25:19,724 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:25:19,730 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:25:19,730 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:25:19,734 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:25:19,737 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:25:19,749 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 02:25:19,749 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:25:19,754 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4ac99f8 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4b5b38ef counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:25:19,755 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:25:19,759 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:25:19,965 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:25:19,965 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:25:19,969 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:25:19,971 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:25:19,971 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:25:19,973 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 02:25:19,974 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:25:19,979 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 02:25:19,983 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 02:25:19,988 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:25:20,035 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:25:20,223 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:25:20,224 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:25:20,229 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:25:20,230 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:25:21,404 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:25:21,404 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:25:21,654 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:25:22,138 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822721648.tmp
13 Dec 2020 02:25:22,245 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:25:22,304 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:25:24,778 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:26:38,548 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822721648.tmp
13 Dec 2020 02:26:38,593 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822721648.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822721648
13 Dec 2020 02:26:38,676 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822721649.tmp
13 Dec 2020 02:26:40,013 INFO  [Thread-16] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:27:55,562 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822721649.tmp
13 Dec 2020 02:27:55,576 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822721649.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822721649
13 Dec 2020 02:27:55,619 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822721650.tmp
13 Dec 2020 02:27:57,066 INFO  [Thread-21] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:29:15,943 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822721650.tmp
13 Dec 2020 02:29:15,958 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822721650.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822721650
13 Dec 2020 02:29:15,997 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822721651.tmp
13 Dec 2020 02:29:17,157 INFO  [Thread-26] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:30:32,054 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822721651.tmp
13 Dec 2020 02:30:32,066 INFO  [hdfs-HDFS-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822721651.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822721651
13 Dec 2020 02:30:32,095 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822721652.tmp
13 Dec 2020 02:30:33,513 INFO  [Thread-30] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:31:52,336 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822721652.tmp
13 Dec 2020 02:31:52,351 INFO  [hdfs-HDFS-call-runner-2] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822721652.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822721652
13 Dec 2020 02:31:52,391 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607822721653.tmp
13 Dec 2020 02:31:53,962 INFO  [Thread-35] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:32:44,703 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4ac99f8 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4b5b38ef counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:32:44,703 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:32:44,703 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:32:44,754 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:32:44,756 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:32:44,760 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:32:44,761 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4ac99f8 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:32:44,761 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:32:44,762 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:32:49,763 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:32:49,788 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:32:49,788 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607822720230
13 Dec 2020 02:32:49,788 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607823169788
13 Dec 2020 02:32:49,789 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 438958
13 Dec 2020 02:32:49,789 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:32:49,789 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:32:49,789 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:32:49,789 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:32:49,789 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:32:49,789 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:32:49,789 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:32:49,790 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:32:49,790 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 5677
13 Dec 2020 02:32:49,790 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 5600
13 Dec 2020 02:32:49,790 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:32:49,790 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=438958, sink.event.drain.attempt=5677, sink.batch.complete=0, sink.event.drain.sucess=5600, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:32:49,792 INFO  [lifecycleSupervisor-1-6] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4ac99f8 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:32:49,792 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 02:32:49,793 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4b5b38ef counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:32:49,793 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 02:32:49,794 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607822721653.tmp
13 Dec 2020 02:32:49,806 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607822721653.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607822721653
13 Dec 2020 02:32:49,810 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 02:32:49,810 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607822719983
13 Dec 2020 02:32:49,810 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607823169810
13 Dec 2020 02:32:49,810 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 5
13 Dec 2020 02:32:49,811 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 02:32:49,811 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 02:32:49,811 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 02:32:49,813 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 6
13 Dec 2020 02:32:49,815 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 6
13 Dec 2020 02:32:49,815 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 02:32:49,815 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 5677
13 Dec 2020 02:32:49,815 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 5677
13 Dec 2020 02:32:49,816 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 02:32:49,816 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:32:49,816 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:32:49,818 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:32:49,818 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607822719971
13 Dec 2020 02:32:49,819 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607823169818
13 Dec 2020 02:32:49,821 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 10000
13 Dec 2020 02:32:49,821 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 02:32:49,822 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 5677
13 Dec 2020 02:32:49,822 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 5677
13 Dec 2020 02:32:49,822 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 5679
13 Dec 2020 02:32:49,823 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 5677
13 Dec 2020 02:32:49,827 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:32:49,828 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:32:49,828 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:32:49,828 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607822719965
13 Dec 2020 02:32:49,829 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607823169828
13 Dec 2020 02:32:49,829 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:32:49,830 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 77
13 Dec 2020 02:32:49,831 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 5677
13 Dec 2020 02:32:49,832 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 5677
13 Dec 2020 02:32:49,832 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 5678
13 Dec 2020 02:32:49,834 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 5600
13 Dec 2020 02:32:49,834 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:32:49,838 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:34:36,798 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:34:36,807 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:34:36,818 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:34:36,819 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,820 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:34:36,820 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:34:36,820 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:34:36,821 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,821 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:34:36,821 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:34:36,822 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 02:34:36,823 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,823 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,823 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:34:36,824 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,825 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:34:36,828 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:34:36,829 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,829 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,830 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,830 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:34:36,831 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:34:36,831 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:34:36,831 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:34:36,831 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:34:36,831 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:34:36,832 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:34:36,832 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:34:36,832 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:34:36,858 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 02:34:36,858 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:34:36,859 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:34:36,870 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:34:36,874 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:34:36,875 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:34:36,875 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:34:36,876 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:34:36,936 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:34:36,944 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:34:36,945 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:34:36,945 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:34:36,949 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:34:36,963 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 02:34:36,963 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:34:36,967 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:34:36,968 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:34:36,975 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:34:37,205 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:34:37,205 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:34:37,211 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:34:37,211 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:34:37,211 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:34:37,219 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 02:34:37,221 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:34:37,226 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 02:34:37,226 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 02:34:37,249 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:34:37,292 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:34:37,528 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:34:37,530 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:34:37,535 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:34:37,539 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:34:38,623 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:34:38,624 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:34:38,757 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:34:39,013 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278757.tmp
13 Dec 2020 02:34:39,116 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:34:39,183 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:34:40,969 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,095 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278757.tmp
13 Dec 2020 02:34:41,126 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278757.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278757
13 Dec 2020 02:34:41,172 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278758.tmp
13 Dec 2020 02:34:41,263 INFO  [Thread-13] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,285 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278758.tmp
13 Dec 2020 02:34:41,323 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278758.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278758
13 Dec 2020 02:34:41,357 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278759.tmp
13 Dec 2020 02:34:41,414 INFO  [Thread-15] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,430 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278759.tmp
13 Dec 2020 02:34:41,438 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278759.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278759
13 Dec 2020 02:34:41,471 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278760.tmp
13 Dec 2020 02:34:41,524 INFO  [Thread-17] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,543 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278760.tmp
13 Dec 2020 02:34:41,553 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278760.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278760
13 Dec 2020 02:34:41,583 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278761.tmp
13 Dec 2020 02:34:41,631 INFO  [Thread-19] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,653 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278761.tmp
13 Dec 2020 02:34:41,664 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278761.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278761
13 Dec 2020 02:34:41,685 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278762.tmp
13 Dec 2020 02:34:41,713 INFO  [Thread-21] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,722 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278762.tmp
13 Dec 2020 02:34:41,730 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278762.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278762
13 Dec 2020 02:34:41,768 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278763.tmp
13 Dec 2020 02:34:41,799 INFO  [Thread-23] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,814 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278763.tmp
13 Dec 2020 02:34:41,828 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278763.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278763
13 Dec 2020 02:34:41,852 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278764.tmp
13 Dec 2020 02:34:41,885 INFO  [Thread-25] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,900 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278764.tmp
13 Dec 2020 02:34:41,907 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278764.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278764
13 Dec 2020 02:34:41,931 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278765.tmp
13 Dec 2020 02:34:41,963 INFO  [Thread-27] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:41,978 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278765.tmp
13 Dec 2020 02:34:41,990 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278765.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278765
13 Dec 2020 02:34:42,016 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278766.tmp
13 Dec 2020 02:34:42,060 INFO  [Thread-29] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,078 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278766.tmp
13 Dec 2020 02:34:42,089 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278766.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278766
13 Dec 2020 02:34:42,115 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278767.tmp
13 Dec 2020 02:34:42,145 INFO  [Thread-31] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,162 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278767.tmp
13 Dec 2020 02:34:42,174 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278767.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278767
13 Dec 2020 02:34:42,197 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278768.tmp
13 Dec 2020 02:34:42,225 INFO  [Thread-33] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,240 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278768.tmp
13 Dec 2020 02:34:42,255 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278768.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278768
13 Dec 2020 02:34:42,280 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278769.tmp
13 Dec 2020 02:34:42,326 INFO  [Thread-35] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,348 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278769.tmp
13 Dec 2020 02:34:42,368 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278769.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278769
13 Dec 2020 02:34:42,389 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278770.tmp
13 Dec 2020 02:34:42,425 INFO  [Thread-37] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,442 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278770.tmp
13 Dec 2020 02:34:42,451 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278770.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278770
13 Dec 2020 02:34:42,478 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278771.tmp
13 Dec 2020 02:34:42,514 INFO  [Thread-39] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,526 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278771.tmp
13 Dec 2020 02:34:42,536 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278771.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278771
13 Dec 2020 02:34:42,560 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278772.tmp
13 Dec 2020 02:34:42,595 INFO  [Thread-41] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,611 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278772.tmp
13 Dec 2020 02:34:42,623 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278772.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278772
13 Dec 2020 02:34:42,637 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278773.tmp
13 Dec 2020 02:34:42,676 INFO  [Thread-43] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,689 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278773.tmp
13 Dec 2020 02:34:42,698 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278773.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278773
13 Dec 2020 02:34:42,716 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278774.tmp
13 Dec 2020 02:34:42,742 INFO  [Thread-45] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,750 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278774.tmp
13 Dec 2020 02:34:42,756 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278774.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:42,760 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278774.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278774
13 Dec 2020 02:34:42,774 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278775.tmp
13 Dec 2020 02:34:42,804 INFO  [Thread-47] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,812 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278775.tmp
13 Dec 2020 02:34:42,819 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278775.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:42,823 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278775.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278775
13 Dec 2020 02:34:42,850 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278776.tmp
13 Dec 2020 02:34:42,901 INFO  [Thread-49] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:42,916 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278776.tmp
13 Dec 2020 02:34:42,935 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278776.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278776
13 Dec 2020 02:34:42,952 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278777.tmp
13 Dec 2020 02:34:42,991 INFO  [Thread-51] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,006 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278777.tmp
13 Dec 2020 02:34:43,013 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278777.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278777
13 Dec 2020 02:34:43,033 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278778.tmp
13 Dec 2020 02:34:43,071 INFO  [Thread-53] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,086 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278778.tmp
13 Dec 2020 02:34:43,095 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278778.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278778
13 Dec 2020 02:34:43,124 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278779.tmp
13 Dec 2020 02:34:43,181 INFO  [Thread-55] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,203 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278779.tmp
13 Dec 2020 02:34:43,218 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278779.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278779
13 Dec 2020 02:34:43,240 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278780.tmp
13 Dec 2020 02:34:43,284 INFO  [Thread-57] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,301 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278780.tmp
13 Dec 2020 02:34:43,309 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278780.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278780
13 Dec 2020 02:34:43,340 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278781.tmp
13 Dec 2020 02:34:43,375 INFO  [Thread-59] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,391 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278781.tmp
13 Dec 2020 02:34:43,404 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278781.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278781
13 Dec 2020 02:34:43,426 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278782.tmp
13 Dec 2020 02:34:43,474 INFO  [Thread-61] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,488 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278782.tmp
13 Dec 2020 02:34:43,498 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278782.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278782
13 Dec 2020 02:34:43,523 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278783.tmp
13 Dec 2020 02:34:43,556 INFO  [Thread-63] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,580 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278783.tmp
13 Dec 2020 02:34:43,586 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278783.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278783
13 Dec 2020 02:34:43,608 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278784.tmp
13 Dec 2020 02:34:43,650 INFO  [Thread-65] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,661 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278784.tmp
13 Dec 2020 02:34:43,670 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278784.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278784
13 Dec 2020 02:34:43,691 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278785.tmp
13 Dec 2020 02:34:43,741 INFO  [Thread-67] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,752 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278785.tmp
13 Dec 2020 02:34:43,758 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278785.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278785
13 Dec 2020 02:34:43,776 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278786.tmp
13 Dec 2020 02:34:43,805 INFO  [Thread-69] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,814 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278786.tmp
13 Dec 2020 02:34:43,824 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278786.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278786
13 Dec 2020 02:34:43,843 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278787.tmp
13 Dec 2020 02:34:43,864 INFO  [Thread-71] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,875 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278787.tmp
13 Dec 2020 02:34:43,884 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278787.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278787
13 Dec 2020 02:34:43,902 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278788.tmp
13 Dec 2020 02:34:43,951 INFO  [Thread-73] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:43,970 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278788.tmp
13 Dec 2020 02:34:43,982 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278788.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:43,983 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278788.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278788
13 Dec 2020 02:34:44,010 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278789.tmp
13 Dec 2020 02:34:44,042 INFO  [Thread-75] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,050 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278789.tmp
13 Dec 2020 02:34:44,056 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278789.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278789
13 Dec 2020 02:34:44,070 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278790.tmp
13 Dec 2020 02:34:44,107 INFO  [Thread-77] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,128 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278790.tmp
13 Dec 2020 02:34:44,139 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278790.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278790
13 Dec 2020 02:34:44,152 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278791.tmp
13 Dec 2020 02:34:44,184 INFO  [Thread-79] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,199 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278791.tmp
13 Dec 2020 02:34:44,205 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278791.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278791
13 Dec 2020 02:34:44,214 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278792.tmp
13 Dec 2020 02:34:44,235 INFO  [Thread-81] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,242 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278792.tmp
13 Dec 2020 02:34:44,249 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278792.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278792
13 Dec 2020 02:34:44,263 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278793.tmp
13 Dec 2020 02:34:44,284 INFO  [Thread-83] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,289 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278793.tmp
13 Dec 2020 02:34:44,294 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278793.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278793
13 Dec 2020 02:34:44,306 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278794.tmp
13 Dec 2020 02:34:44,326 INFO  [Thread-85] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,331 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278794.tmp
13 Dec 2020 02:34:44,335 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278794.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278794
13 Dec 2020 02:34:44,349 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278795.tmp
13 Dec 2020 02:34:44,383 INFO  [Thread-87] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,392 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278795.tmp
13 Dec 2020 02:34:44,401 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278795.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278795
13 Dec 2020 02:34:44,415 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278796.tmp
13 Dec 2020 02:34:44,447 INFO  [Thread-89] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,453 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278796.tmp
13 Dec 2020 02:34:44,459 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278796.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278796
13 Dec 2020 02:34:44,471 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278797.tmp
13 Dec 2020 02:34:44,486 INFO  [Thread-91] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,493 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278797.tmp
13 Dec 2020 02:34:44,498 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278797.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278797
13 Dec 2020 02:34:44,512 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278798.tmp
13 Dec 2020 02:34:44,529 INFO  [Thread-93] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,534 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278798.tmp
13 Dec 2020 02:34:44,539 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278798.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278798
13 Dec 2020 02:34:44,553 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278799.tmp
13 Dec 2020 02:34:44,575 INFO  [Thread-95] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,584 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278799.tmp
13 Dec 2020 02:34:44,592 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278799.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278799
13 Dec 2020 02:34:44,603 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278800.tmp
13 Dec 2020 02:34:44,623 INFO  [Thread-97] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,634 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278800.tmp
13 Dec 2020 02:34:44,644 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278800.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:44,647 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278800.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278800
13 Dec 2020 02:34:44,667 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278801.tmp
13 Dec 2020 02:34:44,691 INFO  [Thread-99] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,698 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278801.tmp
13 Dec 2020 02:34:44,703 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278801.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278801
13 Dec 2020 02:34:44,723 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278802.tmp
13 Dec 2020 02:34:44,751 INFO  [Thread-101] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,757 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278802.tmp
13 Dec 2020 02:34:44,763 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278802.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:44,765 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278802.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278802
13 Dec 2020 02:34:44,780 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278803.tmp
13 Dec 2020 02:34:44,799 INFO  [Thread-103] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,806 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278803.tmp
13 Dec 2020 02:34:44,818 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278803.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278803
13 Dec 2020 02:34:44,840 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278804.tmp
13 Dec 2020 02:34:44,862 INFO  [Thread-105] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,876 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278804.tmp
13 Dec 2020 02:34:44,884 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278804.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278804
13 Dec 2020 02:34:44,900 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278805.tmp
13 Dec 2020 02:34:44,914 INFO  [Thread-107] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,920 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278805.tmp
13 Dec 2020 02:34:44,927 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278805.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278805
13 Dec 2020 02:34:44,944 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278806.tmp
13 Dec 2020 02:34:44,959 INFO  [Thread-109] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:44,967 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278806.tmp
13 Dec 2020 02:34:44,977 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278806.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278806
13 Dec 2020 02:34:44,988 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278807.tmp
13 Dec 2020 02:34:45,020 INFO  [Thread-111] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,028 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278807.tmp
13 Dec 2020 02:34:45,033 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278807.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278807
13 Dec 2020 02:34:45,043 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278808.tmp
13 Dec 2020 02:34:45,061 INFO  [Thread-113] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,066 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278808.tmp
13 Dec 2020 02:34:45,070 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278808.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:45,073 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278808.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278808
13 Dec 2020 02:34:45,092 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278809.tmp
13 Dec 2020 02:34:45,125 INFO  [Thread-115] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,135 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278809.tmp
13 Dec 2020 02:34:45,140 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278809.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278809
13 Dec 2020 02:34:45,170 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278810.tmp
13 Dec 2020 02:34:45,222 INFO  [Thread-117] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,232 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278810.tmp
13 Dec 2020 02:34:45,236 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278810.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:45,237 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278810.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278810
13 Dec 2020 02:34:45,257 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278811.tmp
13 Dec 2020 02:34:45,299 INFO  [Thread-119] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,309 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278811.tmp
13 Dec 2020 02:34:45,322 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278811.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278811
13 Dec 2020 02:34:45,346 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278812.tmp
13 Dec 2020 02:34:45,370 INFO  [Thread-121] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,382 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278812.tmp
13 Dec 2020 02:34:45,388 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278812.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278812
13 Dec 2020 02:34:45,401 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278813.tmp
13 Dec 2020 02:34:45,428 INFO  [Thread-123] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,437 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278813.tmp
13 Dec 2020 02:34:45,445 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278813.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278813
13 Dec 2020 02:34:45,479 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278814.tmp
13 Dec 2020 02:34:45,509 INFO  [Thread-125] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,517 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278814.tmp
13 Dec 2020 02:34:45,522 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278814.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278814
13 Dec 2020 02:34:45,535 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278815.tmp
13 Dec 2020 02:34:45,552 INFO  [Thread-127] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,558 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278815.tmp
13 Dec 2020 02:34:45,561 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278815.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:45,569 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278815.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278815
13 Dec 2020 02:34:45,584 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278816.tmp
13 Dec 2020 02:34:45,617 INFO  [Thread-129] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,634 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278816.tmp
13 Dec 2020 02:34:45,640 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278816.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278816
13 Dec 2020 02:34:45,651 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278817.tmp
13 Dec 2020 02:34:45,669 INFO  [Thread-131] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,683 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278817.tmp
13 Dec 2020 02:34:45,693 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278817.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278817
13 Dec 2020 02:34:45,712 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278818.tmp
13 Dec 2020 02:34:45,728 INFO  [Thread-133] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,735 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278818.tmp
13 Dec 2020 02:34:45,739 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278818.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:45,755 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278818.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278818
13 Dec 2020 02:34:45,770 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278819.tmp
13 Dec 2020 02:34:45,788 INFO  [Thread-135] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,800 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278819.tmp
13 Dec 2020 02:34:45,810 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278819.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278819
13 Dec 2020 02:34:45,821 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278820.tmp
13 Dec 2020 02:34:45,846 INFO  [Thread-137] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,853 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278820.tmp
13 Dec 2020 02:34:45,859 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278820.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278820
13 Dec 2020 02:34:45,875 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278821.tmp
13 Dec 2020 02:34:45,899 INFO  [Thread-139] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,914 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278821.tmp
13 Dec 2020 02:34:45,919 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278821.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278821
13 Dec 2020 02:34:45,930 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278822.tmp
13 Dec 2020 02:34:45,941 INFO  [Thread-141] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:45,951 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278822.tmp
13 Dec 2020 02:34:45,956 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278822.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278822
13 Dec 2020 02:34:45,968 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278823.tmp
13 Dec 2020 02:34:45,989 INFO  [Thread-143] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,001 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278823.tmp
13 Dec 2020 02:34:46,005 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278823.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278823
13 Dec 2020 02:34:46,015 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278824.tmp
13 Dec 2020 02:34:46,033 INFO  [Thread-145] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,042 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278824.tmp
13 Dec 2020 02:34:46,047 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278824.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278824
13 Dec 2020 02:34:46,056 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278825.tmp
13 Dec 2020 02:34:46,075 INFO  [Thread-147] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,080 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278825.tmp
13 Dec 2020 02:34:46,087 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278825.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278825
13 Dec 2020 02:34:46,104 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278826.tmp
13 Dec 2020 02:34:46,117 INFO  [Thread-149] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,123 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278826.tmp
13 Dec 2020 02:34:46,127 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278826.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278826
13 Dec 2020 02:34:46,137 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278827.tmp
13 Dec 2020 02:34:46,149 INFO  [Thread-151] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,156 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278827.tmp
13 Dec 2020 02:34:46,161 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278827.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278827
13 Dec 2020 02:34:46,170 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278828.tmp
13 Dec 2020 02:34:46,181 INFO  [Thread-153] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,192 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278828.tmp
13 Dec 2020 02:34:46,198 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278828.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278828
13 Dec 2020 02:34:46,207 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278829.tmp
13 Dec 2020 02:34:46,219 INFO  [Thread-155] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,226 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278829.tmp
13 Dec 2020 02:34:46,234 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278829.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:46,236 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278829.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278829
13 Dec 2020 02:34:46,245 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278830.tmp
13 Dec 2020 02:34:46,258 INFO  [Thread-157] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,281 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278830.tmp
13 Dec 2020 02:34:46,292 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278830.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278830
13 Dec 2020 02:34:46,323 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278831.tmp
13 Dec 2020 02:34:46,345 INFO  [Thread-159] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,353 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278831.tmp
13 Dec 2020 02:34:46,367 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278831.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:46,370 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278831.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278831
13 Dec 2020 02:34:46,388 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278832.tmp
13 Dec 2020 02:34:46,411 INFO  [Thread-161] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,424 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278832.tmp
13 Dec 2020 02:34:46,436 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278832.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:46,438 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278832.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278832
13 Dec 2020 02:34:46,454 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278833.tmp
13 Dec 2020 02:34:46,490 INFO  [Thread-163] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,515 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278833.tmp
13 Dec 2020 02:34:46,521 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278833.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:46,524 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278833.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278833
13 Dec 2020 02:34:46,545 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278834.tmp
13 Dec 2020 02:34:46,595 INFO  [Thread-165] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,606 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278834.tmp
13 Dec 2020 02:34:46,614 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278834.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278834
13 Dec 2020 02:34:46,639 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278835.tmp
13 Dec 2020 02:34:46,659 INFO  [Thread-167] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,670 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278835.tmp
13 Dec 2020 02:34:46,677 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278835.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278835
13 Dec 2020 02:34:46,732 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278836.tmp
13 Dec 2020 02:34:46,763 INFO  [Thread-169] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,780 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278836.tmp
13 Dec 2020 02:34:46,793 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278836.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278836
13 Dec 2020 02:34:46,813 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278837.tmp
13 Dec 2020 02:34:46,833 INFO  [Thread-171] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,840 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278837.tmp
13 Dec 2020 02:34:46,846 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278837.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278837
13 Dec 2020 02:34:46,861 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278838.tmp
13 Dec 2020 02:34:46,881 INFO  [Thread-173] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,890 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278838.tmp
13 Dec 2020 02:34:46,897 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278838.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278838
13 Dec 2020 02:34:46,914 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278839.tmp
13 Dec 2020 02:34:46,944 INFO  [Thread-175] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:46,951 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278839.tmp
13 Dec 2020 02:34:46,957 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278839.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:46,958 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278839.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278839
13 Dec 2020 02:34:46,977 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278840.tmp
13 Dec 2020 02:34:47,005 INFO  [Thread-177] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,012 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278840.tmp
13 Dec 2020 02:34:47,018 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278840.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278840
13 Dec 2020 02:34:47,035 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278841.tmp
13 Dec 2020 02:34:47,067 INFO  [Thread-179] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,075 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278841.tmp
13 Dec 2020 02:34:47,082 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278841.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:47,085 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278841.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278841
13 Dec 2020 02:34:47,123 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278842.tmp
13 Dec 2020 02:34:47,180 INFO  [Thread-181] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,194 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278842.tmp
13 Dec 2020 02:34:47,205 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278842.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278842
13 Dec 2020 02:34:47,247 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278843.tmp
13 Dec 2020 02:34:47,287 INFO  [Thread-183] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,303 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278843.tmp
13 Dec 2020 02:34:47,311 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278843.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278843
13 Dec 2020 02:34:47,343 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278844.tmp
13 Dec 2020 02:34:47,379 INFO  [Thread-185] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,391 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278844.tmp
13 Dec 2020 02:34:47,409 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278844.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278844
13 Dec 2020 02:34:47,448 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278845.tmp
13 Dec 2020 02:34:47,515 INFO  [Thread-187] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,525 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278845.tmp
13 Dec 2020 02:34:47,535 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278845.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278845
13 Dec 2020 02:34:47,565 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278846.tmp
13 Dec 2020 02:34:47,619 INFO  [Thread-189] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,629 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278846.tmp
13 Dec 2020 02:34:47,635 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278846.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278846
13 Dec 2020 02:34:47,673 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278847.tmp
13 Dec 2020 02:34:47,719 INFO  [Thread-191] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,733 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278847.tmp
13 Dec 2020 02:34:47,740 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278847.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278847
13 Dec 2020 02:34:47,780 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278848.tmp
13 Dec 2020 02:34:47,842 INFO  [Thread-193] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,853 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278848.tmp
13 Dec 2020 02:34:47,859 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278848.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278848
13 Dec 2020 02:34:47,894 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278849.tmp
13 Dec 2020 02:34:47,934 INFO  [Thread-195] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:47,949 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278849.tmp
13 Dec 2020 02:34:47,955 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278849.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278849
13 Dec 2020 02:34:47,994 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278850.tmp
13 Dec 2020 02:34:48,059 INFO  [Thread-197] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,077 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278850.tmp
13 Dec 2020 02:34:48,082 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278850.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278850
13 Dec 2020 02:34:48,120 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278851.tmp
13 Dec 2020 02:34:48,179 INFO  [Thread-199] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,188 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278851.tmp
13 Dec 2020 02:34:48,194 WARN  [DataStreamer for file /twitterraw/FlumeData.1607823278851.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 02:34:48,197 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278851.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278851
13 Dec 2020 02:34:48,234 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:34:48,234 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:34:48,234 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:34:48,237 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:34:48,245 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:34:48,239 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278852.tmp
13 Dec 2020 02:34:48,255 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:34:48,255 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:34:48,255 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:34:48,256 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:34:48,303 INFO  [Thread-201] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,314 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278852.tmp
13 Dec 2020 02:34:48,324 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278852.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278852
13 Dec 2020 02:34:48,343 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278853.tmp
13 Dec 2020 02:34:48,358 INFO  [Thread-203] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,366 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278853.tmp
13 Dec 2020 02:34:48,371 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278853.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278853
13 Dec 2020 02:34:48,384 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278854.tmp
13 Dec 2020 02:34:48,400 INFO  [Thread-205] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,407 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278854.tmp
13 Dec 2020 02:34:48,414 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278854.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278854
13 Dec 2020 02:34:48,434 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278855.tmp
13 Dec 2020 02:34:48,457 INFO  [Thread-207] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,463 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278855.tmp
13 Dec 2020 02:34:48,468 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278855.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278855
13 Dec 2020 02:34:48,480 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278856.tmp
13 Dec 2020 02:34:48,495 INFO  [Thread-209] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,501 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278856.tmp
13 Dec 2020 02:34:48,505 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278856.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278856
13 Dec 2020 02:34:48,521 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278857.tmp
13 Dec 2020 02:34:48,535 INFO  [Thread-211] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,541 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278857.tmp
13 Dec 2020 02:34:48,545 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278857.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278857
13 Dec 2020 02:34:48,556 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278858.tmp
13 Dec 2020 02:34:48,569 INFO  [Thread-213] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,582 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278858.tmp
13 Dec 2020 02:34:48,587 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278858.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278858
13 Dec 2020 02:34:48,600 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278859.tmp
13 Dec 2020 02:34:48,615 INFO  [Thread-215] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,620 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278859.tmp
13 Dec 2020 02:34:48,624 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278859.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278859
13 Dec 2020 02:34:48,634 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278860.tmp
13 Dec 2020 02:34:48,650 INFO  [Thread-217] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,657 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278860.tmp
13 Dec 2020 02:34:48,661 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278860.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278860
13 Dec 2020 02:34:48,673 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278861.tmp
13 Dec 2020 02:34:48,691 INFO  [Thread-219] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,698 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278861.tmp
13 Dec 2020 02:34:48,712 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278861.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278861
13 Dec 2020 02:34:48,723 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278862.tmp
13 Dec 2020 02:34:48,737 INFO  [Thread-221] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,743 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278862.tmp
13 Dec 2020 02:34:48,747 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278862.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278862
13 Dec 2020 02:34:48,760 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278863.tmp
13 Dec 2020 02:34:48,785 INFO  [Thread-223] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,804 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278863.tmp
13 Dec 2020 02:34:48,810 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278863.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278863
13 Dec 2020 02:34:48,825 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278864.tmp
13 Dec 2020 02:34:48,845 INFO  [Thread-225] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,851 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278864.tmp
13 Dec 2020 02:34:48,856 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278864.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278864
13 Dec 2020 02:34:48,866 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278865.tmp
13 Dec 2020 02:34:48,880 INFO  [Thread-227] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,889 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278865.tmp
13 Dec 2020 02:34:48,894 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278865.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278865
13 Dec 2020 02:34:48,922 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278866.tmp
13 Dec 2020 02:34:48,950 INFO  [Thread-229] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:48,960 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278866.tmp
13 Dec 2020 02:34:48,966 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278866.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278866
13 Dec 2020 02:34:48,985 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278867.tmp
13 Dec 2020 02:34:49,009 INFO  [Thread-231] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:49,014 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278867.tmp
13 Dec 2020 02:34:49,017 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278867.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278867
13 Dec 2020 02:34:49,026 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278868.tmp
13 Dec 2020 02:34:49,046 INFO  [Thread-233] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:49,055 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278868.tmp
13 Dec 2020 02:34:49,072 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278868.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278868
13 Dec 2020 02:34:49,083 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278869.tmp
13 Dec 2020 02:34:49,095 INFO  [Thread-235] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:49,103 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278869.tmp
13 Dec 2020 02:34:49,107 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278869.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278869
13 Dec 2020 02:34:49,127 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278870.tmp
13 Dec 2020 02:34:49,150 INFO  [Thread-237] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:49,164 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278870.tmp
13 Dec 2020 02:34:49,170 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278870.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278870
13 Dec 2020 02:34:49,186 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278871.tmp
13 Dec 2020 02:34:49,211 INFO  [Thread-239] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:49,224 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278871.tmp
13 Dec 2020 02:34:49,228 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278871.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278871
13 Dec 2020 02:34:49,245 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823278872.tmp
13 Dec 2020 02:34:52,268 INFO  [Thread-241] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:34:53,257 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607823277539
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607823293275
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 9497
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 116
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 100
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:34:53,275 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=9497, sink.event.drain.attempt=116, sink.batch.complete=0, sink.event.drain.sucess=100, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:34:53,278 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 02:34:53,278 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:34:53,278 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:34:53,279 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 02:34:53,279 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823278872.tmp
13 Dec 2020 02:34:53,286 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823278872.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823278872
13 Dec 2020 02:34:53,292 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 02:34:53,292 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607823277226
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607823293292
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 1
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 116
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 116
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 116
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 116
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607823277211
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607823293293
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 10000
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 116
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 116
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 118
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 116
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:34:53,293 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:34:53,294 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:34:53,294 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607823277205
13 Dec 2020 02:34:53,294 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607823293293
13 Dec 2020 02:34:53,294 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:34:53,295 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 16
13 Dec 2020 02:34:53,295 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 116
13 Dec 2020 02:34:53,295 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 116
13 Dec 2020 02:34:53,295 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 117
13 Dec 2020 02:34:53,295 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 100
13 Dec 2020 02:34:53,296 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:34:53,302 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 02:35:25,000 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 02:35:25,022 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 02:35:25,055 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,059 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,059 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:35:25,061 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,061 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,062 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:35:25,062 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,063 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:35:25,064 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:35:25,065 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 02:35:25,065 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,066 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,069 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,070 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:35:25,075 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,076 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,076 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 02:35:25,078 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:35:25,079 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,079 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,079 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,080 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 02:35:25,080 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,080 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,081 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,082 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:35:25,082 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 02:35:25,082 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 02:35:25,083 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 02:35:25,083 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 02:35:25,176 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 02:35:25,178 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 02:35:25,178 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 02:35:25,205 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 02:35:25,228 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 02:35:25,235 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 02:35:25,236 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 02:35:25,242 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 02:35:25,470 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 02:35:25,498 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 02:35:25,502 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 02:35:25,507 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 02:35:25,527 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 02:35:25,558 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 02:35:25,559 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 02:35:25,571 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@24c1e41b counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@54d1ac5b counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:35:25,575 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 02:35:25,591 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 02:35:26,232 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 02:35:26,233 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 02:35:26,234 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 02:35:26,235 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 02:35:26,235 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 02:35:26,244 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 02:35:26,245 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 02:35:26,257 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 02:35:26,263 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 02:35:26,266 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 02:35:26,351 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 02:35:26,614 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 02:35:26,621 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 02:35:26,628 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 02:35:26,636 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 02:35:28,190 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 02:35:28,190 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 02:35:28,410 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:35:28,853 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823328408.tmp
13 Dec 2020 02:35:29,007 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:35:29,026 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 02:35:29,066 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 02:35:31,370 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:45:31,252 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
13 Dec 2020 02:45:31,263 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 02:45:31,285 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823328408.tmp
13 Dec 2020 02:45:31,322 INFO  [hdfs-HDFS-call-runner-2] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823328408.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823328408
13 Dec 2020 02:45:31,363 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607823931264.tmp
13 Dec 2020 02:45:33,218 INFO  [Thread-33] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 02:46:38,388 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@24c1e41b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@54d1ac5b counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 02:46:38,388 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 02:46:38,388 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 02:46:38,414 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 02:46:38,416 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 02:46:38,421 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 02:46:38,424 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@24c1e41b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:46:38,425 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 02:46:38,425 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 02:46:43,427 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 02:46:43,449 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 02:46:43,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607823326636
13 Dec 2020 02:46:43,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607824003449
13 Dec 2020 02:46:43,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 668025
13 Dec 2020 02:46:43,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 02:46:43,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 02:46:43,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 02:46:43,450 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 02:46:43,451 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 02:46:43,451 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 02:46:43,451 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 02:46:43,452 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 02:46:43,452 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 9250
13 Dec 2020 02:46:43,452 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 9200
13 Dec 2020 02:46:43,452 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 02:46:43,453 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=668025, sink.event.drain.attempt=9250, sink.batch.complete=0, sink.event.drain.sucess=9200, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 02:46:43,457 INFO  [lifecycleSupervisor-1-5] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@24c1e41b counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 02:46:43,457 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 02:46:43,459 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@54d1ac5b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 02:46:43,459 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 02:46:43,460 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607823931264.tmp
13 Dec 2020 02:46:43,473 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607823931264.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607823931264
13 Dec 2020 02:46:43,477 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 02:46:43,477 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607823326263
13 Dec 2020 02:46:43,477 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607824003477
13 Dec 2020 02:46:43,478 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 02:46:43,478 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 02:46:43,478 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 02:46:43,478 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 02:46:43,479 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 2
13 Dec 2020 02:46:43,479 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 2
13 Dec 2020 02:46:43,480 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 02:46:43,480 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 9250
13 Dec 2020 02:46:43,481 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 9250
13 Dec 2020 02:46:43,487 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 02:46:43,487 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 02:46:43,487 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 02:46:43,488 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 02:46:43,489 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607823326235
13 Dec 2020 02:46:43,489 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607824003488
13 Dec 2020 02:46:43,490 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 100000
13 Dec 2020 02:46:43,491 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 02:46:43,496 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 9250
13 Dec 2020 02:46:43,496 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 9250
13 Dec 2020 02:46:43,496 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 9252
13 Dec 2020 02:46:43,497 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 9250
13 Dec 2020 02:46:43,497 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 02:46:43,498 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 02:46:43,498 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 02:46:43,498 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607823326233
13 Dec 2020 02:46:43,499 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607824003498
13 Dec 2020 02:46:43,499 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 02:46:43,499 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 50
13 Dec 2020 02:46:43,500 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 9250
13 Dec 2020 02:46:43,500 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 9250
13 Dec 2020 02:46:43,503 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 9251
13 Dec 2020 02:46:43,503 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 9200
13 Dec 2020 02:46:43,503 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 02:46:43,506 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 12:50:54,125 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 12:50:54,134 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 12:50:54,147 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,149 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,149 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 12:50:54,149 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,150 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,150 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 12:50:54,151 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,156 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 12:50:54,157 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 12:50:54,157 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 12:50:54,157 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,158 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,159 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,160 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 12:50:54,165 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,166 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,166 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 12:50:54,167 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 12:50:54,168 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,168 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,168 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,171 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 12:50:54,171 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,172 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,173 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,174 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 12:50:54,174 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 12:50:54,175 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 12:50:54,175 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 12:50:54,175 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 12:50:54,195 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 12:50:54,197 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 12:50:54,197 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 12:50:54,215 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 12:50:54,222 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 12:50:54,226 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 12:50:54,227 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 12:50:54,228 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 12:50:54,300 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 12:50:54,311 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 12:50:54,312 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 12:50:54,314 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 12:50:54,323 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 12:50:54,335 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 12:50:54,335 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 12:50:54,340 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@49e9750 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4de3deff counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 12:50:54,340 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 12:50:54,355 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 12:50:54,597 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 12:50:54,597 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 12:50:54,599 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 12:50:54,601 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 12:50:54,601 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 12:50:54,605 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 12:50:54,606 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 12:50:54,612 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 12:50:54,615 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 12:50:54,633 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 12:50:54,671 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 12:50:54,866 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 12:50:54,867 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 12:50:54,874 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 12:50:54,877 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 12:50:55,934 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 12:50:55,935 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 12:50:56,020 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 12:50:56,242 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607860256020.tmp
13 Dec 2020 12:50:56,310 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 12:50:56,324 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 12:51:02,074 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 12:56:39,904 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 12:56:40,627 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 12:57:25,193 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607860256020.tmp
13 Dec 2020 12:57:25,237 INFO  [hdfs-HDFS-call-runner-3] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607860256020.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607860256020
13 Dec 2020 12:57:25,295 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607860256021.tmp
13 Dec 2020 12:57:32,220 INFO  [Thread-26] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:01:41,054 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@49e9750 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4de3deff counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 13:01:41,054 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 13:01:41,055 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 13:01:41,140 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 13:01:41,141 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 13:01:41,144 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 13:01:41,145 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@49e9750 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 13:01:41,146 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 13:01:41,146 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 13:01:46,148 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 13:01:46,165 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 13:01:46,165 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607860254877
13 Dec 2020 13:01:46,166 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607860906165
13 Dec 2020 13:01:46,166 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 615350
13 Dec 2020 13:01:46,166 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 13:01:46,166 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 13:01:46,166 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 13:01:46,166 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 13:01:46,166 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 13:01:46,166 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 13:01:46,167 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 13:01:46,167 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 13:01:46,167 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 1674
13 Dec 2020 13:01:46,168 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 1600
13 Dec 2020 13:01:46,168 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 13:01:46,168 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=615350, sink.event.drain.attempt=1674, sink.batch.complete=0, sink.event.drain.sucess=1600, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 13:01:46,172 INFO  [lifecycleSupervisor-1-9] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@49e9750 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 13:01:46,173 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 13:01:46,173 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4de3deff counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 13:01:46,175 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 13:01:46,175 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607860256021.tmp
13 Dec 2020 13:01:46,188 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607860256021.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607860256021
13 Dec 2020 13:01:46,192 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 13:01:46,192 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607860254615
13 Dec 2020 13:01:46,192 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607860906192
13 Dec 2020 13:01:46,192 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 1
13 Dec 2020 13:01:46,192 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 13:01:46,192 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 13:01:46,192 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 13:01:46,192 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 2
13 Dec 2020 13:01:46,193 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 2
13 Dec 2020 13:01:46,193 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 13:01:46,193 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 1674
13 Dec 2020 13:01:46,193 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 1674
13 Dec 2020 13:01:46,193 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 13:01:46,193 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 13:01:46,193 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 13:01:46,193 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 13:01:46,194 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607860254601
13 Dec 2020 13:01:46,194 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607860906193
13 Dec 2020 13:01:46,194 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 10000
13 Dec 2020 13:01:46,194 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 13:01:46,194 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 1674
13 Dec 2020 13:01:46,194 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 1674
13 Dec 2020 13:01:46,194 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 1676
13 Dec 2020 13:01:46,194 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 1674
13 Dec 2020 13:01:46,195 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 13:01:46,200 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 13:01:46,200 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 13:01:46,200 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607860254597
13 Dec 2020 13:01:46,202 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607860906200
13 Dec 2020 13:01:46,202 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 13:01:46,202 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 74
13 Dec 2020 13:01:46,202 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 1674
13 Dec 2020 13:01:46,203 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 1674
13 Dec 2020 13:01:46,203 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 1675
13 Dec 2020 13:01:46,204 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 1600
13 Dec 2020 13:01:46,205 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 13:01:46,207 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 13:20:02,123 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 13:20:02,135 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 13:20:02,151 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,153 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,153 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:20:02,154 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,155 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,156 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:20:02,157 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,162 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:20:02,163 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:20:02,164 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 13:20:02,165 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,165 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,167 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,171 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:20:02,176 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,178 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,179 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:20:02,180 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:20:02,181 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,181 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,182 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,182 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:20:02,182 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,183 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,183 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,184 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:20:02,185 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:20:02,185 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:20:02,186 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:20:02,186 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 13:20:02,217 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 13:20:02,222 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 13:20:02,222 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 13:20:02,239 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 13:20:02,244 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 13:20:02,251 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 13:20:02,252 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 13:20:02,253 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 13:20:02,353 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 13:20:02,362 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 13:20:02,362 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 13:20:02,363 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 13:20:02,370 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 13:20:02,386 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 13:20:02,386 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 13:20:02,394 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4ac99f8 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4b5b38ef counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 13:20:02,394 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 13:20:02,406 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 13:20:02,746 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 13:20:02,747 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 13:20:02,751 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 13:20:02,752 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 13:20:02,752 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 13:20:02,755 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 13:20:02,755 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 13:20:02,764 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 13:20:02,771 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 13:20:02,770 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 13:20:02,856 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 13:20:03,168 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 13:20:03,171 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 13:20:03,177 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 13:20:03,179 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 13:20:04,853 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 13:20:04,854 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 13:20:05,022 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 13:20:05,053 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 13:20:05,390 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607862005022.tmp
13 Dec 2020 13:20:05,488 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 13:20:05,543 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 13:20:07,525 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:35:07,459 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
13 Dec 2020 13:35:07,471 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 13:35:07,487 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607862005022.tmp
13 Dec 2020 13:35:07,528 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607862005022.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607862005022
13 Dec 2020 13:35:07,554 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607862907472.tmp
13 Dec 2020 13:35:10,358 INFO  [Thread-43] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:00,622 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4ac99f8 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4b5b38ef counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 13:40:00,623 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 13:40:00,624 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 13:40:00,712 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 13:40:00,714 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 13:40:00,719 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 13:40:00,721 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4ac99f8 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 13:40:00,722 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 13:40:00,723 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 13:40:05,727 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 13:40:05,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 13:40:05,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607862003179
13 Dec 2020 13:40:05,748 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607863205748
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 1191796
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 4121
13 Dec 2020 13:40:05,749 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 4100
13 Dec 2020 13:40:05,750 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 13:40:05,750 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=1191796, sink.event.drain.attempt=4121, sink.batch.complete=0, sink.event.drain.sucess=4100, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 13:40:05,752 INFO  [lifecycleSupervisor-1-5] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4ac99f8 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 13:40:05,752 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 13:40:05,753 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4b5b38ef counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 13:40:05,753 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 13:40:05,753 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607862907472.tmp
13 Dec 2020 13:40:05,762 INFO  [hdfs-HDFS-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607862907472.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607862907472
13 Dec 2020 13:40:05,766 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 13:40:05,766 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607862002771
13 Dec 2020 13:40:05,766 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607863205766
13 Dec 2020 13:40:05,767 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 13:40:05,767 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 13:40:05,767 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 13:40:05,768 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 13:40:05,768 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 2
13 Dec 2020 13:40:05,769 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 2
13 Dec 2020 13:40:05,769 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 13:40:05,769 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 4121
13 Dec 2020 13:40:05,769 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 4121
13 Dec 2020 13:40:05,770 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 13:40:05,770 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 13:40:05,771 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 13:40:05,776 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 13:40:05,776 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607862002752
13 Dec 2020 13:40:05,778 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607863205776
13 Dec 2020 13:40:05,778 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 100000
13 Dec 2020 13:40:05,778 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 13:40:05,778 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 4121
13 Dec 2020 13:40:05,782 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 4121
13 Dec 2020 13:40:05,782 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 4123
13 Dec 2020 13:40:05,783 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 4121
13 Dec 2020 13:40:05,783 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 13:40:05,783 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 13:40:05,783 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 13:40:05,784 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607862002747
13 Dec 2020 13:40:05,784 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607863205783
13 Dec 2020 13:40:05,785 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 13:40:05,785 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 21
13 Dec 2020 13:40:05,785 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 4121
13 Dec 2020 13:40:05,785 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 4121
13 Dec 2020 13:40:05,786 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 4122
13 Dec 2020 13:40:05,786 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 4100
13 Dec 2020 13:40:05,786 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 13:40:05,790 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 13:40:51,270 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 13:40:51,277 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 13:40:51,290 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,291 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:40:51,291 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:40:51,292 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:40:51,292 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,292 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:40:51,292 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:40:51,293 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 13:40:51,294 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,294 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,294 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:40:51,295 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:40:51,296 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:40:51,296 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,299 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:40:51,299 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:40:51,299 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,300 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,300 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,301 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:40:51,301 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:40:51,301 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:40:51,301 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:40:51,301 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:40:51,302 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:40:51,302 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:40:51,302 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:40:51,303 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 13:40:51,322 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 13:40:51,322 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 13:40:51,324 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 13:40:51,339 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 13:40:51,341 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 13:40:51,344 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 13:40:51,344 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 13:40:51,351 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 13:40:51,408 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 13:40:51,416 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 13:40:51,416 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 13:40:51,416 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 13:40:51,423 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 13:40:51,435 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 13:40:51,435 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 13:40:51,437 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@1ef456d counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@198a215d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 13:40:51,437 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 13:40:51,447 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 13:40:51,654 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 13:40:51,655 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 13:40:51,659 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 13:40:51,660 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 13:40:51,660 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 13:40:51,661 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 13:40:51,661 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 13:40:51,669 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 13:40:51,674 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 13:40:51,674 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 13:40:51,729 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 13:40:51,913 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 13:40:51,916 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 13:40:51,924 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 13:40:51,924 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 13:40:53,117 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 13:40:53,118 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 13:40:53,261 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 13:40:53,280 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 13:40:53,546 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253248.tmp
13 Dec 2020 13:40:53,610 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 13:40:53,647 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 13:40:55,335 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:55,441 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253248.tmp
13 Dec 2020 13:40:55,468 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253248.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253248
13 Dec 2020 13:40:55,512 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253249.tmp
13 Dec 2020 13:40:55,542 INFO  [Thread-13] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:55,558 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253249.tmp
13 Dec 2020 13:40:55,574 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253249.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253249
13 Dec 2020 13:40:55,599 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253250.tmp
13 Dec 2020 13:40:55,635 INFO  [Thread-15] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:55,650 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253250.tmp
13 Dec 2020 13:40:55,662 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253250.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253250
13 Dec 2020 13:40:55,683 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253251.tmp
13 Dec 2020 13:40:55,731 INFO  [Thread-17] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:55,748 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253251.tmp
13 Dec 2020 13:40:55,754 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253251.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253251
13 Dec 2020 13:40:55,776 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253252.tmp
13 Dec 2020 13:40:55,819 INFO  [Thread-19] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:55,831 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253252.tmp
13 Dec 2020 13:40:55,838 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253252.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253252
13 Dec 2020 13:40:55,863 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253253.tmp
13 Dec 2020 13:40:55,889 INFO  [Thread-21] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:55,900 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253253.tmp
13 Dec 2020 13:40:55,907 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253253.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253253
13 Dec 2020 13:40:55,931 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253254.tmp
13 Dec 2020 13:40:55,960 INFO  [Thread-23] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:55,967 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253254.tmp
13 Dec 2020 13:40:55,974 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253254.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253254
13 Dec 2020 13:40:55,993 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253255.tmp
13 Dec 2020 13:40:56,020 INFO  [Thread-25] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:56,029 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253255.tmp
13 Dec 2020 13:40:56,043 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253255.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:40:56,051 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253255.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253255
13 Dec 2020 13:40:56,070 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253256.tmp
13 Dec 2020 13:40:56,111 INFO  [Thread-27] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:56,127 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253256.tmp
13 Dec 2020 13:40:56,134 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253256.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253256
13 Dec 2020 13:40:56,154 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253257.tmp
13 Dec 2020 13:40:56,198 INFO  [Thread-29] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:56,212 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253257.tmp
13 Dec 2020 13:40:56,219 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253257.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253257
13 Dec 2020 13:40:56,242 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253258.tmp
13 Dec 2020 13:40:56,269 INFO  [Thread-31] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:56,278 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253258.tmp
13 Dec 2020 13:40:56,286 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253258.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253258
13 Dec 2020 13:40:56,304 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253259.tmp
13 Dec 2020 13:40:56,328 INFO  [Thread-33] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:56,339 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253259.tmp
13 Dec 2020 13:40:56,346 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253259.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253259
13 Dec 2020 13:40:56,365 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253260.tmp
13 Dec 2020 13:40:56,387 INFO  [Thread-35] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:56,401 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253260.tmp
13 Dec 2020 13:40:56,407 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253260.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253260
13 Dec 2020 13:40:56,423 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253261.tmp
13 Dec 2020 13:40:56,484 INFO  [Thread-37] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:56,495 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253261.tmp
13 Dec 2020 13:40:56,502 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253261.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253261
13 Dec 2020 13:40:56,525 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253262.tmp
13 Dec 2020 13:40:56,770 INFO  [Thread-39] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:56,783 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253262.tmp
13 Dec 2020 13:40:56,790 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253262.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253262
13 Dec 2020 13:40:56,809 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253263.tmp
13 Dec 2020 13:40:57,059 INFO  [Thread-41] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:57,072 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253263.tmp
13 Dec 2020 13:40:57,086 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253263.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253263
13 Dec 2020 13:40:57,111 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253264.tmp
13 Dec 2020 13:40:57,145 INFO  [Thread-43] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:57,157 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253264.tmp
13 Dec 2020 13:40:57,173 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253264.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253264
13 Dec 2020 13:40:57,193 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253265.tmp
13 Dec 2020 13:40:57,404 INFO  [Thread-45] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:57,411 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253265.tmp
13 Dec 2020 13:40:57,416 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253265.tmp block BP-516599829-10.123.252.237-1599831982550:blk_1073742071_1253] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)
13 Dec 2020 13:40:57,417 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253265.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253265
13 Dec 2020 13:40:57,432 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253266.tmp
13 Dec 2020 13:40:57,678 INFO  [Thread-47] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:57,700 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253266.tmp
13 Dec 2020 13:40:57,708 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253266.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253266
13 Dec 2020 13:40:57,733 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253267.tmp
13 Dec 2020 13:40:57,858 INFO  [Thread-49] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:57,871 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253267.tmp
13 Dec 2020 13:40:57,877 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253267.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253267
13 Dec 2020 13:40:57,907 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253268.tmp
13 Dec 2020 13:40:58,085 INFO  [Thread-51] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:58,098 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253268.tmp
13 Dec 2020 13:40:58,113 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253268.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253268
13 Dec 2020 13:40:58,152 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253269.tmp
13 Dec 2020 13:40:58,200 INFO  [Thread-53] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:58,214 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253269.tmp
13 Dec 2020 13:40:58,224 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253269.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253269
13 Dec 2020 13:40:58,257 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253270.tmp
13 Dec 2020 13:40:58,292 INFO  [Thread-55] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:58,298 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253270.tmp
13 Dec 2020 13:40:58,307 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253270.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:40:58,309 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253270.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253270
13 Dec 2020 13:40:58,326 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253271.tmp
13 Dec 2020 13:40:58,896 INFO  [Thread-57] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:58,918 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253271.tmp
13 Dec 2020 13:40:58,926 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253271.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253271
13 Dec 2020 13:40:58,955 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253272.tmp
13 Dec 2020 13:40:58,999 INFO  [Thread-59] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:59,012 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253272.tmp
13 Dec 2020 13:40:59,018 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253272.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:40:59,020 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253272.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253272
13 Dec 2020 13:40:59,040 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253273.tmp
13 Dec 2020 13:40:59,220 INFO  [Thread-61] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:59,228 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253273.tmp
13 Dec 2020 13:40:59,243 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253273.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253273
13 Dec 2020 13:40:59,263 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253274.tmp
13 Dec 2020 13:40:59,788 INFO  [Thread-63] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:59,798 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253274.tmp
13 Dec 2020 13:40:59,803 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253274.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253274
13 Dec 2020 13:40:59,817 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253275.tmp
13 Dec 2020 13:40:59,853 INFO  [Thread-65] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:40:59,863 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253275.tmp
13 Dec 2020 13:40:59,870 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253275.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:40:59,872 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253275.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253275
13 Dec 2020 13:40:59,888 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253276.tmp
13 Dec 2020 13:41:00,337 INFO  [Thread-67] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:00,353 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253276.tmp
13 Dec 2020 13:41:00,363 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253276.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:41:00,365 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253276.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253276
13 Dec 2020 13:41:00,397 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253277.tmp
13 Dec 2020 13:41:00,457 INFO  [Thread-69] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:00,465 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253277.tmp
13 Dec 2020 13:41:00,472 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253277.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:41:00,473 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253277.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253277
13 Dec 2020 13:41:00,509 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253278.tmp
13 Dec 2020 13:41:01,079 INFO  [Thread-71] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:01,089 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253278.tmp
13 Dec 2020 13:41:01,094 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253278.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253278
13 Dec 2020 13:41:01,107 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253279.tmp
13 Dec 2020 13:41:01,159 INFO  [Thread-73] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:01,168 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253279.tmp
13 Dec 2020 13:41:01,175 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253279.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:41:01,176 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253279.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253279
13 Dec 2020 13:41:01,197 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253280.tmp
13 Dec 2020 13:41:01,224 INFO  [Thread-75] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:01,230 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253280.tmp
13 Dec 2020 13:41:01,235 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253280.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253280
13 Dec 2020 13:41:01,253 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253281.tmp
13 Dec 2020 13:41:01,560 INFO  [Thread-77] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:01,569 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253281.tmp
13 Dec 2020 13:41:01,575 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253281.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253281
13 Dec 2020 13:41:01,603 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253282.tmp
13 Dec 2020 13:41:01,632 INFO  [Thread-79] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:01,639 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253282.tmp
13 Dec 2020 13:41:01,645 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253282.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253282
13 Dec 2020 13:41:01,666 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253283.tmp
13 Dec 2020 13:41:01,946 INFO  [Thread-81] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:01,967 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253283.tmp
13 Dec 2020 13:41:01,976 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253283.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:41:01,979 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253283.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253283
13 Dec 2020 13:41:01,991 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253284.tmp
13 Dec 2020 13:41:02,163 INFO  [Thread-83] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:02,173 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253284.tmp
13 Dec 2020 13:41:02,181 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253284.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253284
13 Dec 2020 13:41:02,194 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253285.tmp
13 Dec 2020 13:41:02,219 INFO  [Thread-85] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:02,232 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253285.tmp
13 Dec 2020 13:41:02,239 WARN  [DataStreamer for file /twitterraw/FlumeData.1607863253285.tmp] (org.apache.hadoop.hdfs.DataStreamer.closeResponder:988)  - Caught exception
java.lang.InterruptedException
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.Thread.join(Thread.java:1305)
	at java.base/java.lang.Thread.join(Thread.java:1380)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)
13 Dec 2020 13:41:02,241 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253285.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253285
13 Dec 2020 13:41:02,277 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253286.tmp
13 Dec 2020 13:41:02,318 INFO  [Thread-87] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:02,334 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253286.tmp
13 Dec 2020 13:41:02,344 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253286.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253286
13 Dec 2020 13:41:02,363 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253287.tmp
13 Dec 2020 13:41:02,398 INFO  [Thread-89] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:02,404 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253287.tmp
13 Dec 2020 13:41:02,411 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253287.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253287
13 Dec 2020 13:41:02,424 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863253288.tmp
13 Dec 2020 13:41:02,673 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@1ef456d counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@198a215d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 13:41:02,674 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 13:41:02,674 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 13:41:02,922 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 13:41:02,927 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 13:41:02,928 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 13:41:02,932 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@1ef456d counterGroup:{ name:null counters:{} } }
13 Dec 2020 13:41:02,933 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 13:41:02,934 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 13:41:05,442 INFO  [Thread-91] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:41:07,935 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607863251924
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607863267946
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 0
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 13:41:07,947 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 13:41:07,948 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 13:41:07,948 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 13:41:07,948 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 41
13 Dec 2020 13:41:07,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 0
13 Dec 2020 13:41:07,949 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 13:41:07,949 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=0, sink.event.drain.attempt=41, sink.batch.complete=0, sink.event.drain.sucess=0, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 13:41:07,952 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 13:41:07,952 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@198a215d counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 13:41:07,952 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@1ef456d counterGroup:{ name:null counters:{runner.deliveryErrors=1} } }
13 Dec 2020 13:41:07,954 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 13:41:07,954 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863253288.tmp
13 Dec 2020 13:41:07,964 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863253288.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863253288
13 Dec 2020 13:41:07,967 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 13:41:07,967 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607863251674
13 Dec 2020 13:41:07,967 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607863267967
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 41
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 41
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 41
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 41
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 13:41:07,968 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607863251660
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607863267969
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 100000
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 41
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 41
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 43
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 41
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607863251655
13 Dec 2020 13:41:07,969 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607863267969
13 Dec 2020 13:41:07,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 13:41:07,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 41
13 Dec 2020 13:41:07,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 41
13 Dec 2020 13:41:07,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 41
13 Dec 2020 13:41:07,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 42
13 Dec 2020 13:41:07,970 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 0
13 Dec 2020 13:41:07,970 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 13:41:07,979 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 13:44:59,560 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 13:44:59,568 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 13:44:59,587 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,588 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,588 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:44:59,589 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,589 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,589 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:44:59,590 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,590 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:44:59,591 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:44:59,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 13:44:59,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,592 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,594 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,594 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:44:59,597 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,598 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,598 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:44:59,599 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:44:59,600 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,600 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,601 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,601 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:44:59,601 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,602 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,602 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,603 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:44:59,603 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:44:59,603 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:44:59,604 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:44:59,604 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 13:44:59,633 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 13:44:59,634 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 13:44:59,634 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 13:44:59,652 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 13:44:59,658 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 13:44:59,661 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 13:44:59,662 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 13:44:59,663 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 13:44:59,728 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 13:44:59,735 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 13:44:59,737 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 13:44:59,737 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 13:44:59,745 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 13:44:59,758 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 13:44:59,758 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 13:44:59,760 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@287b05b2 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@63295e0d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 13:44:59,761 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 13:44:59,767 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 13:44:59,959 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 13:44:59,959 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 13:44:59,963 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 13:44:59,964 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 13:44:59,964 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 13:44:59,966 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 13:44:59,966 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 13:44:59,978 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 13:44:59,981 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 13:44:59,983 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 13:45:00,036 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 13:45:00,220 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 13:45:00,223 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 13:45:00,224 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 13:45:00,225 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 13:45:01,316 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 13:45:01,316 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 13:45:01,455 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 13:45:01,489 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 13:45:01,718 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607863501454.tmp
13 Dec 2020 13:45:01,829 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 13:45:01,858 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 13:45:04,039 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 13:50:35,302 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 13:56:50,404 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@287b05b2 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@63295e0d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 13:56:50,405 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 13:56:50,405 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 13:56:50,481 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 13:56:50,485 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 13:56:50,490 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 13:56:50,490 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@287b05b2 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 13:56:50,491 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 13:56:50,492 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 13:56:55,493 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 13:56:55,511 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 13:56:55,511 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607863500225
13 Dec 2020 13:56:55,511 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607864215510
13 Dec 2020 13:56:55,511 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 688260
13 Dec 2020 13:56:55,511 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 13:56:55,511 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 13:56:55,511 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 2995
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 2900
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 13:56:55,512 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=688260, sink.event.drain.attempt=2995, sink.batch.complete=0, sink.event.drain.sucess=2900, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 13:56:55,519 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@287b05b2 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 13:56:55,520 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 13:56:55,520 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@63295e0d counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 13:56:55,522 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 13:56:55,523 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607863501454.tmp
13 Dec 2020 13:56:55,568 INFO  [hdfs-HDFS-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607863501454.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607863501454
13 Dec 2020 13:56:55,584 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 13:56:55,584 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607863499981
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607864215584
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 13:56:55,585 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 2995
13 Dec 2020 13:56:55,589 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 2995
13 Dec 2020 13:56:55,590 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 13:56:55,590 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 13:56:55,591 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 13:56:55,591 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 13:56:55,591 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607863499964
13 Dec 2020 13:56:55,591 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607864215591
13 Dec 2020 13:56:55,592 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000000
13 Dec 2020 13:56:55,593 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 13:56:55,595 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 2995
13 Dec 2020 13:56:55,595 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 2995
13 Dec 2020 13:56:55,598 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 2997
13 Dec 2020 13:56:55,598 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 2995
13 Dec 2020 13:56:55,598 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 13:56:55,598 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 13:56:55,599 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 13:56:55,599 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607863499959
13 Dec 2020 13:56:55,599 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607864215599
13 Dec 2020 13:56:55,600 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 13:56:55,602 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 95
13 Dec 2020 13:56:55,603 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 2995
13 Dec 2020 13:56:55,603 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 2995
13 Dec 2020 13:56:55,604 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 2996
13 Dec 2020 13:56:55,604 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 2900
13 Dec 2020 13:56:55,604 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 13:56:55,607 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 13:57:22,438 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 13:57:22,447 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 13:57:22,460 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,461 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,461 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:57:22,461 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,463 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,463 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:57:22,463 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,463 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:57:22,464 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:57:22,465 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 13:57:22,465 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,465 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,466 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,466 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:57:22,469 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,469 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,472 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 13:57:22,472 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:57:22,473 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,473 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,473 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,473 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 13:57:22,474 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,476 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,476 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,477 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:57:22,478 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 13:57:22,478 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 13:57:22,479 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 13:57:22,480 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 13:57:22,508 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 13:57:22,508 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 13:57:22,509 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 13:57:22,520 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 13:57:22,525 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 13:57:22,531 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 13:57:22,533 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 13:57:22,537 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 13:57:22,615 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 13:57:22,622 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 13:57:22,622 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 13:57:22,623 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 13:57:22,631 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 13:57:22,641 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 13:57:22,643 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 13:57:22,649 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 13:57:22,650 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 13:57:22,654 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 13:57:22,859 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 13:57:22,859 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 13:57:22,864 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 13:57:22,865 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 13:57:22,865 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 13:57:22,869 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 13:57:22,870 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 13:57:22,877 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 13:57:22,879 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 13:57:22,903 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 13:57:22,943 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 13:57:23,126 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 13:57:23,132 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 13:57:23,134 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 13:57:23,136 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 13:57:24,406 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 13:57:24,406 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 13:57:24,615 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 13:57:24,970 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607864244610.tmp
13 Dec 2020 13:57:25,094 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 13:57:25,128 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 13:57:27,746 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 14:00:04,760 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 14:00:04,760 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 14:00:04,767 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 14:00:04,885 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 14:00:04,886 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 14:00:04,889 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 14:00:04,892 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 14:00:04,893 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 14:00:04,893 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 14:00:09,894 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 14:00:09,911 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 14:00:09,912 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607864243136
13 Dec 2020 14:00:09,912 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607864409911
13 Dec 2020 14:00:09,912 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 152028
13 Dec 2020 14:00:09,913 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 14:00:09,913 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 14:00:09,914 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 14:00:09,918 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 14:00:09,919 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 14:00:09,919 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 14:00:09,919 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 14:00:09,920 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 14:00:09,920 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 646
13 Dec 2020 14:00:09,920 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 600
13 Dec 2020 14:00:09,922 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 14:00:09,923 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=152028, sink.event.drain.attempt=646, sink.batch.complete=0, sink.event.drain.sucess=600, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 14:00:09,924 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 14:00:09,924 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 14:00:09,924 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 14:00:09,927 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 14:00:09,927 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607864244610.tmp
13 Dec 2020 14:00:09,964 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607864244610.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607864244610
13 Dec 2020 14:00:09,998 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 14:00:09,998 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607864242879
13 Dec 2020 14:00:09,998 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607864409998
13 Dec 2020 14:00:09,999 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 14:00:09,999 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 14:00:10,000 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 14:00:10,000 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 14:00:10,000 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
13 Dec 2020 14:00:10,000 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
13 Dec 2020 14:00:10,001 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 14:00:10,001 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 646
13 Dec 2020 14:00:10,001 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 646
13 Dec 2020 14:00:10,002 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 14:00:10,002 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 14:00:10,003 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 14:00:10,007 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 14:00:10,009 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607864242865
13 Dec 2020 14:00:10,010 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607864410007
13 Dec 2020 14:00:10,010 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000000
13 Dec 2020 14:00:10,010 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 14:00:10,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 646
13 Dec 2020 14:00:10,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 646
13 Dec 2020 14:00:10,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 648
13 Dec 2020 14:00:10,012 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 646
13 Dec 2020 14:00:10,015 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 14:00:10,019 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 14:00:10,019 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 14:00:10,019 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607864242859
13 Dec 2020 14:00:10,020 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607864410019
13 Dec 2020 14:00:10,020 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 14:00:10,021 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 46
13 Dec 2020 14:00:10,021 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 646
13 Dec 2020 14:00:10,021 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 646
13 Dec 2020 14:00:10,022 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 647
13 Dec 2020 14:00:10,023 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 600
13 Dec 2020 14:00:10,023 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 14:00:10,026 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 14:03:02,203 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 14:03:02,215 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 14:03:02,225 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,226 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,226 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:03:02,227 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,227 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,227 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:03:02,228 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,228 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:03:02,229 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:03:02,229 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 14:03:02,230 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,230 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,231 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,232 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:03:02,235 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,235 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,236 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:03:02,237 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:03:02,237 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,237 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:03:02,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,239 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,240 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,240 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:03:02,240 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:03:02,241 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:03:02,241 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:03:02,241 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 14:03:02,262 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 14:03:02,264 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 14:03:02,264 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 14:03:02,280 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 14:03:02,285 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 14:03:02,289 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 14:03:02,291 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 14:03:02,292 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 14:03:02,352 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 14:03:02,358 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 14:03:02,358 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 14:03:02,359 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 14:03:02,363 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 14:03:02,375 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 14:03:02,375 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 14:03:02,379 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 14:03:02,380 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 14:03:02,383 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 14:03:02,590 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 14:03:02,590 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 14:03:02,594 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 14:03:02,594 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 14:03:02,595 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 14:03:02,596 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 14:03:02,597 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 14:03:02,606 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 14:03:02,609 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 14:03:02,610 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 14:03:02,662 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 14:03:02,840 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 14:03:02,842 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 14:03:02,845 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 14:03:02,846 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 14:03:04,177 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 14:03:04,177 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 14:03:04,318 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 14:03:04,717 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607864584319.tmp
13 Dec 2020 14:03:04,817 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 14:03:04,854 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 14:03:08,761 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 14:04:09,786 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 14:04:09,786 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 14:04:09,786 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 14:04:09,847 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 14:04:09,850 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 14:04:09,853 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 14:04:09,854 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 14:04:09,854 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 14:04:09,855 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 14:04:14,856 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 14:04:14,881 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 14:04:14,881 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607864582846
13 Dec 2020 14:04:14,881 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607864654881
13 Dec 2020 14:04:14,881 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 63402
13 Dec 2020 14:04:14,881 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 14:04:14,881 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 14:04:14,881 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 319
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 300
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 14:04:14,882 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=63402, sink.event.drain.attempt=319, sink.batch.complete=0, sink.event.drain.sucess=300, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 14:04:14,887 INFO  [lifecycleSupervisor-1-5] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@66caa7ac counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 14:04:14,888 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 14:04:14,888 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@21ccfbca counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 14:04:14,889 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 14:04:14,891 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607864584319.tmp
13 Dec 2020 14:04:14,913 INFO  [hdfs-HDFS-call-runner-2] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607864584319.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607864584319
13 Dec 2020 14:04:14,930 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 14:04:14,930 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607864582609
13 Dec 2020 14:04:14,931 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607864654930
13 Dec 2020 14:04:14,931 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 14:04:14,931 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 14:04:14,932 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 14:04:14,932 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 14:04:14,932 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
13 Dec 2020 14:04:14,932 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
13 Dec 2020 14:04:14,932 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 14:04:14,932 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 319
13 Dec 2020 14:04:14,933 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 319
13 Dec 2020 14:04:14,933 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 14:04:14,933 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 14:04:14,933 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 14:04:14,934 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 14:04:14,935 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607864582594
13 Dec 2020 14:04:14,935 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607864654934
13 Dec 2020 14:04:14,935 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000000
13 Dec 2020 14:04:14,935 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 14:04:14,936 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 319
13 Dec 2020 14:04:14,936 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 319
13 Dec 2020 14:04:14,936 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 321
13 Dec 2020 14:04:14,937 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 319
13 Dec 2020 14:04:14,938 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 14:04:14,938 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 14:04:14,939 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 14:04:14,939 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607864582590
13 Dec 2020 14:04:14,939 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607864654939
13 Dec 2020 14:04:14,939 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 14:04:14,940 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 19
13 Dec 2020 14:04:14,940 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 319
13 Dec 2020 14:04:14,943 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 319
13 Dec 2020 14:04:14,943 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 320
13 Dec 2020 14:04:14,943 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 300
13 Dec 2020 14:04:14,943 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 14:04:14,947 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 14:04:30,045 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 14:04:30,056 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 14:04:30,069 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,070 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,071 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:04:30,071 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,072 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,072 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:04:30,072 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,073 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:04:30,074 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:04:30,074 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 14:04:30,074 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,074 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,079 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,080 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:04:30,083 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,084 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,084 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:04:30,085 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:04:30,085 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,086 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,086 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,086 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:04:30,086 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,086 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,087 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,087 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:04:30,087 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:04:30,088 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:04:30,088 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:04:30,088 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 14:04:30,108 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 14:04:30,108 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 14:04:30,109 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 14:04:30,125 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 14:04:30,129 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 14:04:30,134 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 14:04:30,135 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 14:04:30,138 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 14:04:30,199 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 14:04:30,208 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 14:04:30,209 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 14:04:30,209 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 14:04:30,212 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 14:04:30,222 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 14:04:30,223 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 14:04:30,227 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@24c1e41b counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@54d1ac5b counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 14:04:30,228 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 14:04:30,235 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 14:04:30,436 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 14:04:30,437 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 14:04:30,441 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 14:04:30,441 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 14:04:30,441 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 14:04:30,445 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 14:04:30,446 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 14:04:30,452 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 14:04:30,460 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 14:04:30,459 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 14:04:30,503 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 14:04:30,679 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 14:04:30,682 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 14:04:30,691 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 14:04:30,692 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 14:04:31,926 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 14:04:31,927 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 14:04:32,112 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 14:04:32,531 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607864672110.tmp
13 Dec 2020 14:04:32,644 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 14:04:34,775 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 14:34:34,241 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
13 Dec 2020 14:34:34,258 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607864672110.tmp
13 Dec 2020 14:34:34,293 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607864672110.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607864672110
13 Dec 2020 14:34:34,551 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 14:34:34,616 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607866474552.tmp
13 Dec 2020 14:34:37,052 INFO  [Thread-73] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 14:40:24,380 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 14:44:45,959 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@24c1e41b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@54d1ac5b counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 14:44:45,959 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 14:44:45,959 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 14:44:45,985 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 14:44:45,985 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 14:44:45,989 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 14:44:45,989 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@24c1e41b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 14:44:45,990 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 14:44:45,992 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 14:44:50,993 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 14:44:51,009 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 14:44:51,009 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607864670692
13 Dec 2020 14:44:51,009 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607867091009
13 Dec 2020 14:44:51,009 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 2403810
13 Dec 2020 14:44:51,009 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 14:44:51,010 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 14:44:51,010 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 14:44:51,010 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 14:44:51,010 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 14:44:51,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 14:44:51,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 14:44:51,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 14:44:51,011 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 13165
13 Dec 2020 14:44:51,012 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 13100
13 Dec 2020 14:44:51,012 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 14:44:51,014 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=2403810, sink.event.drain.attempt=13165, sink.batch.complete=0, sink.event.drain.sucess=13100, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 14:44:51,015 INFO  [lifecycleSupervisor-1-8] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@24c1e41b counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 14:44:51,015 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 14:44:51,021 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@54d1ac5b counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 14:44:51,022 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 14:44:51,023 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607866474552.tmp
13 Dec 2020 14:44:51,033 INFO  [hdfs-HDFS-call-runner-2] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607866474552.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607866474552
13 Dec 2020 14:44:51,041 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 14:44:51,042 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607864670460
13 Dec 2020 14:44:51,042 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607867091041
13 Dec 2020 14:44:51,042 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 14:44:51,042 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 14:44:51,042 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 14:44:51,043 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 14:44:51,043 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 2
13 Dec 2020 14:44:51,043 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 2
13 Dec 2020 14:44:51,043 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 14:44:51,044 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 13165
13 Dec 2020 14:44:51,046 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 13165
13 Dec 2020 14:44:51,046 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 14:44:51,051 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 14:44:51,051 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 14:44:51,051 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 14:44:51,051 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607864670441
13 Dec 2020 14:44:51,052 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607867091051
13 Dec 2020 14:44:51,052 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 1000000
13 Dec 2020 14:44:51,053 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 14:44:51,053 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 13165
13 Dec 2020 14:44:51,053 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 13165
13 Dec 2020 14:44:51,054 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 13167
13 Dec 2020 14:44:51,054 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 13165
13 Dec 2020 14:44:51,054 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 14:44:51,054 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 14:44:51,055 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 14:44:51,056 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607864670437
13 Dec 2020 14:44:51,057 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607867091055
13 Dec 2020 14:44:51,057 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 14:44:51,059 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 65
13 Dec 2020 14:44:51,059 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 13165
13 Dec 2020 14:44:51,060 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 13165
13 Dec 2020 14:44:51,060 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 13166
13 Dec 2020 14:44:51,060 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 13100
13 Dec 2020 14:44:51,060 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 14:44:51,063 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 14:46:17,388 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 14:46:17,397 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 14:46:17,408 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,409 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,409 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:46:17,410 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,411 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,411 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:46:17,411 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,412 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:46:17,413 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:46:17,413 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 14:46:17,413 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,414 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,415 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,415 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:46:17,419 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,419 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,419 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 14:46:17,420 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:46:17,421 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,421 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,421 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,421 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 14:46:17,422 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,422 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,422 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,423 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:46:17,423 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 14:46:17,424 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 14:46:17,424 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 14:46:17,424 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 14:46:17,456 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 14:46:17,460 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 14:46:17,460 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 14:46:17,479 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 14:46:17,482 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 14:46:17,483 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 14:46:17,483 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 14:46:17,484 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 14:46:17,551 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 14:46:17,560 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 14:46:17,562 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 14:46:17,563 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 14:46:17,567 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 14:46:17,577 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 14:46:17,579 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 14:46:17,583 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 14:46:17,583 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 14:46:17,591 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 14:46:17,795 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 14:46:17,795 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 14:46:17,799 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 14:46:17,799 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 14:46:17,800 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 14:46:17,809 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 14:46:17,810 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 14:46:17,818 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 14:46:17,820 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 14:46:17,834 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 14:46:17,864 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 14:46:18,028 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 14:46:18,031 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 14:46:18,035 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 14:46:18,036 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 14:46:19,193 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 14:46:19,194 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 14:46:19,395 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 14:46:19,730 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607867179395.tmp
13 Dec 2020 14:46:19,865 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 14:46:19,897 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 14:46:22,510 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 15:06:10,741 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:06:33,979 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:06:39,886 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:06:40,502 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:07:09,911 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:16:31,240 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 15:16:31,240 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 15:16:31,241 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 15:16:31,300 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 15:16:31,304 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 15:16:31,307 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 15:16:31,307 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 15:16:31,308 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 15:16:31,309 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 15:16:36,310 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 15:16:36,340 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 15:16:36,341 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607867178036
13 Dec 2020 15:16:36,341 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607868996340
13 Dec 2020 15:16:36,341 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 1802144
13 Dec 2020 15:16:36,341 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
13 Dec 2020 15:16:36,341 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 15:16:36,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 15:16:36,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 15:16:36,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 15:16:36,342 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 15:16:36,343 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 15:16:36,343 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 15:16:36,343 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 12491
13 Dec 2020 15:16:36,343 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 12400
13 Dec 2020 15:16:36,344 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
13 Dec 2020 15:16:36,344 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=1802144, sink.event.drain.attempt=12491, sink.batch.complete=0, sink.event.drain.sucess=12400, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
13 Dec 2020 15:16:36,345 INFO  [lifecycleSupervisor-1-6] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
13 Dec 2020 15:16:36,346 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 15:16:36,346 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 15:16:36,347 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 15:16:36,347 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607867179395.tmp
13 Dec 2020 15:16:36,377 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607867179395.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607867179395
13 Dec 2020 15:16:36,394 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 15:16:36,394 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607867177820
13 Dec 2020 15:16:36,395 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607868996394
13 Dec 2020 15:16:36,395 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 15:16:36,395 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 15:16:36,395 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 15:16:36,396 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 15:16:36,396 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 1
13 Dec 2020 15:16:36,396 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 1
13 Dec 2020 15:16:36,397 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 15:16:36,397 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 12491
13 Dec 2020 15:16:36,398 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 12491
13 Dec 2020 15:16:36,398 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 15:16:36,399 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 15:16:36,399 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 15:16:36,400 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 15:16:36,400 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607867177799
13 Dec 2020 15:16:36,400 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607868996400
13 Dec 2020 15:16:36,400 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 10000000
13 Dec 2020 15:16:36,407 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 15:16:36,407 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 12491
13 Dec 2020 15:16:36,407 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 12491
13 Dec 2020 15:16:36,408 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 12493
13 Dec 2020 15:16:36,408 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 12491
13 Dec 2020 15:16:36,408 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 15:16:36,408 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 15:16:36,409 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 15:16:36,409 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607867177795
13 Dec 2020 15:16:36,410 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607868996409
13 Dec 2020 15:16:36,410 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 15:16:36,411 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 91
13 Dec 2020 15:16:36,411 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 12491
13 Dec 2020 15:16:36,411 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 12491
13 Dec 2020 15:16:36,411 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 12492
13 Dec 2020 15:16:36,412 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 12400
13 Dec 2020 15:16:36,412 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 15:16:36,416 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 15:17:19,704 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 15:17:19,715 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 15:17:19,728 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,730 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,731 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 15:17:19,731 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,731 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,732 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 15:17:19,732 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,733 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 15:17:19,734 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 15:17:19,734 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 15:17:19,734 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,735 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,736 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,736 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 15:17:19,740 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,740 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,740 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 15:17:19,741 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 15:17:19,742 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,742 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,742 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,743 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 15:17:19,743 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,743 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,744 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,744 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 15:17:19,744 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 15:17:19,745 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 15:17:19,745 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 15:17:19,745 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 15:17:19,773 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 15:17:19,774 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 15:17:19,775 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 15:17:19,788 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 15:17:19,793 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 15:17:19,799 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 15:17:19,799 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 15:17:19,800 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 15:17:19,859 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 15:17:19,865 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 15:17:19,865 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 15:17:19,870 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 15:17:19,874 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 15:17:19,885 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 15:17:19,885 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 15:17:19,889 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 15:17:19,890 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 15:17:19,899 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 15:17:20,104 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 15:17:20,104 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 15:17:20,105 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 15:17:20,107 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 15:17:20,107 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 15:17:20,114 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 15:17:20,114 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 15:17:20,125 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 15:17:20,125 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 15:17:20,141 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 15:17:20,171 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 15:17:20,348 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 15:17:20,353 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 15:17:20,354 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 15:17:20,358 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 15:17:21,397 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 15:17:21,398 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 15:17:21,527 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 15:17:21,769 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607869041528.tmp
13 Dec 2020 15:17:21,838 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 15:17:21,859 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 15:17:23,443 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 15:28:11,000 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-2 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,000 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-2 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,000 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-2 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,000 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-2 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,003 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-2 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,003 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,003 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,005 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,005 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,005 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 15:28:11,006 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
13 Dec 2020 15:28:11,009 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
Caused by: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
13 Dec 2020 15:28:16,022 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-1 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
13 Dec 2020 15:28:16,040 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-1 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
13 Dec 2020 15:28:16,048 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-1 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
13 Dec 2020 15:28:16,102 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
13 Dec 2020 15:28:16,103 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
Caused by: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
13 Dec 2020 15:28:34,482 INFO  [ResponseProcessor for block BP-516599829-10.123.252.237-1599831982550:blk_1073742101_1283] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1092)  - Slow ReadProcessor read fields for block BP-516599829-10.123.252.237-1599831982550:blk_1073742101_1283 took 53798ms (threshold=30000ms); ack: seqno: 278 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 1583938 flag: 0 flag: 0, targets: [DatanodeInfoWithStorage[10.123.252.235:9866,DS-063195f3-efdb-4583-87c5-d6070047a841,DISK], DatanodeInfoWithStorage[10.123.252.236:9866,DS-e43f43f6-1ba1-470d-8520-5eaa9e16da49,DISK]]
13 Dec 2020 15:28:34,488 INFO  [ResponseProcessor for block BP-516599829-10.123.252.237-1599831982550:blk_1073742101_1283] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1092)  - Slow ReadProcessor read fields for block BP-516599829-10.123.252.237-1599831982550:blk_1073742101_1283 took 52153ms (threshold=30000ms); ack: seqno: 279 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 2402842 flag: 0 flag: 0, targets: [DatanodeInfoWithStorage[10.123.252.235:9866,DS-063195f3-efdb-4583-87c5-d6070047a841,DISK], DatanodeInfoWithStorage[10.123.252.236:9866,DS-e43f43f6-1ba1-470d-8520-5eaa9e16da49,DISK]]
13 Dec 2020 15:29:37,357 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-1 due to org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.. Going to request metadata update now
13 Dec 2020 15:29:45,749 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
13 Dec 2020 15:29:45,752 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
Caused by: org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition.
13 Dec 2020 15:30:14,795 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:37:33,535 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:37:40,453 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:37:40,998 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 15:38:09,576 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 16:17:23,357 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
13 Dec 2020 16:17:23,374 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607869041528.tmp
13 Dec 2020 16:17:23,396 INFO  [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607869041528.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607869041528
13 Dec 2020 16:17:23,532 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 16:17:23,597 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607872643533.tmp
13 Dec 2020 16:17:24,964 INFO  [Thread-133] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 16:18:03,829 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 16:26:00,790 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.deliveryErrors=3, runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 16:26:00,790 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 16:26:00,790 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 16:26:00,863 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 16:26:00,864 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 16:26:00,868 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 16:26:00,869 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.deliveryErrors=3, runner.backoffs.consecutive=0} } }
13 Dec 2020 16:26:00,870 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 16:26:00,870 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1343)
	at java.base/java.util.concurrent.CountDownLatch.await(CountDownLatch.java:232)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
13 Dec 2020 16:26:05,871 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 16:26:05,893 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 16:26:05,893 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607869040358
13 Dec 2020 16:26:05,893 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607873165893
13 Dec 2020 16:26:05,893 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 4044015
13 Dec 2020 16:26:05,893 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 4
13 Dec 2020 16:26:05,893 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 16:26:05,893 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 1
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 35687
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 35300
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 4
13 Dec 2020 16:26:05,894 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=1, sink.event.write.fail=4, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=4044015, sink.event.drain.attempt=35687, sink.batch.complete=0, sink.event.drain.sucess=35300, sink.connection.creation.count=0, channel.rollback.count=4, sink.batch.empty=0}
13 Dec 2020 16:26:05,903 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 16:26:05,903 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@4db78d4d counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
13 Dec 2020 16:26:05,903 INFO  [lifecycleSupervisor-1-7] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@38f41db4 counterGroup:{ name:null counters:{runner.deliveryErrors=4, runner.backoffs.consecutive=0} } }
13 Dec 2020 16:26:05,906 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 16:26:05,906 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607872643533.tmp
13 Dec 2020 16:26:05,920 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607872643533.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607872643533
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607869040125
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607873165924
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 1
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 2
13 Dec 2020 16:26:05,924 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 2
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 35387
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 35387
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607869040107
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607873165925
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 10000000
13 Dec 2020 16:26:05,925 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 35387
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 35387
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 35389
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 35387
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607869040104
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607873165926
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
13 Dec 2020 16:26:05,926 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 87
13 Dec 2020 16:26:05,927 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 35387
13 Dec 2020 16:26:05,927 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 35387
13 Dec 2020 16:26:05,934 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 35688
13 Dec 2020 16:26:05,934 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 35300
13 Dec 2020 16:26:05,934 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 12
13 Dec 2020 16:26:05,943 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
13 Dec 2020 17:02:49,206 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 17:02:49,220 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 17:02:49,235 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,236 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,236 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 17:02:49,237 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,237 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,238 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 17:02:49,238 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,240 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 17:02:49,241 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 17:02:49,241 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 17:02:49,241 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,241 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,243 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,243 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 17:02:49,246 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,246 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,246 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 17:02:49,247 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 17:02:49,250 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,250 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,250 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,251 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 17:02:49,251 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,251 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,252 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,252 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 17:02:49,252 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 17:02:49,252 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 17:02:49,253 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 17:02:49,256 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 17:02:49,280 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 17:02:49,281 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 17:02:49,281 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 17:02:49,295 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 17:02:49,301 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 17:02:49,307 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 17:02:49,307 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 17:02:49,308 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 17:02:49,368 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 17:02:49,374 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 17:02:49,374 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 17:02:49,374 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 17:02:49,379 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 17:02:49,389 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 17:02:49,391 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 17:02:49,395 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@41d3d5f9 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7d6cc4ec counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 17:02:49,395 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 17:02:49,398 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 17:02:49,609 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 17:02:49,610 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 17:02:49,614 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 17:02:49,614 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 17:02:49,614 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 17:02:49,616 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 17:02:49,616 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 17:02:49,621 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 17:02:49,625 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 17:02:49,630 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 17:02:49,681 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 17:02:49,900 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 17:02:49,902 INFO  [lifecycleSupervisor-1-2] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 17:02:49,911 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 17:02:49,914 INFO  [lifecycleSupervisor-1-2] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 17:02:50,960 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 17:02:50,962 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 17:02:51,089 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 17:02:51,143 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 17:02:51,357 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607875371090.tmp
13 Dec 2020 17:02:51,443 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 17:02:51,511 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 17:02:53,309 INFO  [Thread-10] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 17:04:03,364 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 17:05:11,268 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 17:10:11,390 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 17:32:11,150 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 17:53:34,652 INFO  [DataStreamer for file /twitterraw/FlumeData.1607875371090.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 17:57:25,853 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:00:13,273 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:02:53,234 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
13 Dec 2020 18:02:53,266 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607875371090.tmp
13 Dec 2020 18:02:53,281 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 18:02:53,338 INFO  [hdfs-HDFS-call-runner-7] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607875371090.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607875371090
13 Dec 2020 18:02:53,544 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607878973282.tmp
13 Dec 2020 18:02:54,528 INFO  [Thread-135] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 18:04:31,485 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:06:10,956 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:08:09,006 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:08:20,452 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:19:29,367 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:19:29,595 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:19:29,807 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:19:29,847 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:19:29,897 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
13 Dec 2020 18:19:30,102 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
Caused by: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
13 Dec 2020 18:19:37,058 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
org.apache.flume.ChannelFullException: Space for commit to queue couldn't be acquired. Sinks are likely not keeping up with sources, or the buffer size is too tight
	at org.apache.flume.channel.MemoryChannel$MemoryTransaction.doCommit(MemoryChannel.java:129)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:270)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:21:38,409 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:21:39,665 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-1 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:21:39,976 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:21:40,432 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
13 Dec 2020 18:21:40,976 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
Caused by: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
13 Dec 2020 18:21:51,262 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
org.apache.flume.ChannelFullException: Space for commit to queue couldn't be acquired. Sinks are likely not keeping up with sources, or the buffer size is too tight
	at org.apache.flume.channel.MemoryChannel$MemoryTransaction.doCommit(MemoryChannel.java:129)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:270)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:21:55,426 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
org.apache.flume.ChannelFullException: Space for commit to queue couldn't be acquired. Sinks are likely not keeping up with sources, or the buffer size is too tight
	at org.apache.flume.channel.MemoryChannel$MemoryTransaction.doCommit(MemoryChannel.java:129)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:270)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:22:00,831 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
org.apache.flume.ChannelFullException: Space for commit to queue couldn't be acquired. Sinks are likely not keeping up with sources, or the buffer size is too tight
	at org.apache.flume.channel.MemoryChannel$MemoryTransaction.doCommit(MemoryChannel.java:129)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:270)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:22:07,059 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
org.apache.flume.ChannelFullException: Space for commit to queue couldn't be acquired. Sinks are likely not keeping up with sources, or the buffer size is too tight
	at org.apache.flume.channel.MemoryChannel$MemoryTransaction.doCommit(MemoryChannel.java:129)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:270)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 18:22:38,231 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:454)  - HDFS IO error
java.io.IOException: Callable timed out after 30000 ms on file: hdfs://node-master:9000/twitterraw/FlumeData.1607878973282.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:741)
	at org.apache.flume.sink.hdfs.BucketWriter.doFlush(BucketWriter.java:517)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:479)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:441)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.util.concurrent.TimeoutException
	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:734)
	... 6 more
13 Dec 2020 18:22:41,044 ERROR [hdfs-HDFS-call-runner-4] (org.apache.flume.sink.hdfs.AbstractHDFSWriter.hflushOrSync:269)  - Error while trying to hflushOrSync!
13 Dec 2020 18:32:09,788 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-1 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:32:20,132 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-0 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:32:20,148 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@41d3d5f9 counterGroup:{ name:null counters:{runner.deliveryErrors=2, runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7d6cc4ec counterGroup:{ name:null counters:{runner.backoffs.consecutive=1, runner.backoffs=1} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 18:32:20,239 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
13 Dec 2020 18:32:20,239 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
13 Dec 2020 18:32:20,237 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.producer.internals.Sender.completeBatch:568)  - [Producer clientId=producer-1] Received invalid metadata error in produce request on partition twitterraw-2 due to org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.. Going to request metadata update now
13 Dec 2020 18:32:20,248 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
13 Dec 2020 18:32:20,249 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:94)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:64)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
Caused by: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
13 Dec 2020 18:32:20,350 WARN  [DataStreamer for file /twitterraw/FlumeData.1607878973282.tmp block BP-516599829-10.123.252.237-1599831982550:blk_1073742105_1287] (org.apache.hadoop.hdfs.DataStreamer.run:826)  - DataStreamer Exception
java.io.IOException: Broken pipe
	at java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:113)
	at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:79)
	at java.base/sun.nio.ch.IOUtil.write(IOUtil.java:50)
	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)
	at java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)
	at java.base/java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:775)
13 Dec 2020 18:32:22,483 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
13 Dec 2020 18:32:22,483 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.base/java.util.zip.Inflater.ensureOpen(Inflater.java:740)
	at java.base/java.util.zip.Inflater.inflate(Inflater.java:377)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:153)
	at java.base/java.util.zip.GZIPInputStream.read(GZIPInputStream.java:118)
	at java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.base/java.io.InputStreamReader.read(InputStreamReader.java:185)
	at java.base/java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)
	at java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
13 Dec 2020 18:32:24,244 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
13 Dec 2020 18:32:24,244 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@41d3d5f9 counterGroup:{ name:null counters:{runner.deliveryErrors=3, runner.backoffs.consecutive=0} } }
13 Dec 2020 18:32:24,245 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
13 Dec 2020 18:32:24,245 INFO  [lifecycleSupervisor-1-4] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:STOP} }
13 Dec 2020 18:32:24,421 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
13 Dec 2020 18:32:24,421 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607875369914
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607880744421
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 4629821
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 3
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 0
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 55100
13 Dec 2020 18:32:24,422 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 54800
13 Dec 2020 18:32:24,423 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 3
13 Dec 2020 18:32:24,423 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=0, sink.event.write.fail=3, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=4629821, sink.event.drain.attempt=55100, sink.batch.complete=0, sink.event.drain.sucess=54800, sink.connection.creation.count=0, channel.rollback.count=3, sink.batch.empty=0}
13 Dec 2020 18:32:24,425 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
13 Dec 2020 18:32:24,426 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@7d6cc4ec counterGroup:{ name:null counters:{runner.backoffs.consecutive=1, runner.backoffs=1} } }
13 Dec 2020 18:32:24,426 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout:761)  - Unexpected Exception null
java.lang.InterruptedException
	at java.base/java.util.concurrent.FutureTask.awaitDone(FutureTask.java:418)
	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:203)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:734)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:605)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:412)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 18:32:24,427 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
java.lang.InterruptedException
	at java.base/java.util.concurrent.FutureTask.awaitDone(FutureTask.java:418)
	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:203)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:734)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:605)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:412)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
13 Dec 2020 18:32:24,427 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:464)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.InterruptedException
	at java.base/java.util.concurrent.FutureTask.awaitDone(FutureTask.java:418)
	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:203)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:734)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:605)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:412)
	... 3 more
13 Dec 2020 18:32:29,428 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
13 Dec 2020 18:32:34,437 INFO  [ResponseProcessor for block BP-516599829-10.123.252.237-1599831982550:blk_1073742105_1287] (org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run:1092)  - Slow ReadProcessor read fields for block BP-516599829-10.123.252.237-1599831982550:blk_1073742105_1287 took 35750ms (threshold=30000ms); ack: seqno: 742 reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 1708648 flag: 0 flag: 0, targets: [DatanodeInfoWithStorage[10.123.252.236:9866,DS-e43f43f6-1ba1-470d-8520-5eaa9e16da49,DISK], DatanodeInfoWithStorage[10.123.252.235:9866,DS-063195f3-efdb-4583-87c5-d6070047a841,DISK]]
13 Dec 2020 18:32:34,439 WARN  [DataStreamer for file /twitterraw/FlumeData.1607878973282.tmp block BP-516599829-10.123.252.237-1599831982550:blk_1073742105_1287] (org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode:1571)  - Error Recovery for BP-516599829-10.123.252.237-1599831982550:blk_1073742105_1287 in pipeline [DatanodeInfoWithStorage[10.123.252.236:9866,DS-e43f43f6-1ba1-470d-8520-5eaa9e16da49,DISK], DatanodeInfoWithStorage[10.123.252.235:9866,DS-063195f3-efdb-4583-87c5-d6070047a841,DISK]]: datanode 0(DatanodeInfoWithStorage[10.123.252.236:9866,DS-e43f43f6-1ba1-470d-8520-5eaa9e16da49,DISK]) is bad.
13 Dec 2020 18:32:34,808 WARN  [DataStreamer for file /twitterraw/FlumeData.1607878973282.tmp block BP-516599829-10.123.252.237-1599831982550:blk_1073742105_1287] (org.apache.hadoop.hdfs.DataStreamer.run:826)  - DataStreamer Exception
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:1042)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.updateBlockForPipeline(DataStreamer.java:1625)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1502)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:34,815 WARN  [hdfs-HDFS-call-runner-1] (org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync:732)  - Error while syncing
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:1042)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.updateBlockForPipeline(DataStreamer.java:1625)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1502)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:34,816 ERROR [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.AbstractHDFSWriter.hflushOrSync:269)  - Error while trying to hflushOrSync!
13 Dec 2020 18:32:34,817 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:435)  - pre-close flush failed
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:1042)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.updateBlockForPipeline(DataStreamer.java:1625)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1502)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:34,817 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607878973282.tmp
13 Dec 2020 18:32:34,817 ERROR [hdfs-HDFS-call-runner-7] (org.apache.flume.sink.hdfs.AbstractHDFSWriter.hflushOrSync:269)  - Error while trying to hflushOrSync!
13 Dec 2020 18:32:34,818 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter$CloseHandler.close:348)  - Closing file: hdfs://node-master:9000/twitterraw/FlumeData.1607878973282.tmp failed. Will retry again in 180 seconds.
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:1042)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.updateBlockForPipeline(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.updateBlockForPipeline(DataStreamer.java:1625)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1502)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:34,818 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter$CloseHandler.close:358)  - Unsuccessfully attempted to close hdfs://node-master:9000/twitterraw/FlumeData.1607878973282.tmp 2147483647 times. Initializing lease recovery.
13 Dec 2020 18:32:34,927 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.recoverLease:404)  - Lease recovery failed for hdfs://node-master:9000/twitterraw/FlumeData.1607878973282.tmp
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.recoverLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.recoverLease(ClientNamenodeProtocolTranslatorPB.java:700)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.recoverLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.recoverLease(DFSClient.java:881)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:295)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:292)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.recoverLease(DistributedFileSystem.java:307)
	at org.apache.flume.sink.hdfs.BucketWriter.recoverLease(BucketWriter.java:402)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1400(BucketWriter.java:60)
	at org.apache.flume.sink.hdfs.BucketWriter$CloseHandler.close(BucketWriter.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter.doClose(BucketWriter.java:440)
	at org.apache.flume.sink.hdfs.BucketWriter.close(BucketWriter.java:426)
	at org.apache.flume.sink.hdfs.HDFSEventSink.stop(HDFSEventSink.java:497)
	at org.apache.flume.sink.DefaultSinkProcessor.stop(DefaultSinkProcessor.java:52)
	at org.apache.flume.SinkRunner.stop(SinkRunner.java:113)
	at org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise(LifecycleSupervisor.java:170)
	at org.apache.flume.node.Application.stopAllComponents(Application.java:140)
	at org.apache.flume.node.Application.stop(Application.java:112)
	at org.apache.flume.node.Application$1.run(Application.java:369)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 32 more
13 Dec 2020 18:32:34,943 WARN  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:462)  - failed to rename() file (hdfs://node-master:9000/twitterraw/FlumeData.1607878973282.tmp). Exception follows.
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:904)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1661)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1577)
	at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1574)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1589)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:680)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:677)
	at org.apache.flume.sink.hdfs.BucketWriter$8$1.run(BucketWriter.java:727)
	at org.apache.flume.auth.SimpleAuthenticator.execute(SimpleAuthenticator.java:50)
	at org.apache.flume.sink.hdfs.BucketWriter$8.call(BucketWriter.java:724)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 30 more
13 Dec 2020 18:32:52,492 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 30 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:53,496 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 31 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:54,500 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 32 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:55,509 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 33 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:56,512 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 34 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:57,517 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 35 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:58,520 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 36 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:32:59,522 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 37 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:00,529 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 38 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:01,531 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 39 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:02,534 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 40 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:03,536 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 41 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:04,541 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 42 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:05,544 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 43 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:06,547 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 44 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:07,550 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 45 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:08,552 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 46 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:09,560 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 47 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:10,563 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 48 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:11,565 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 49 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:12,568 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 50 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:13,577 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 51 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:14,579 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 52 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:15,582 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 53 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:16,588 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 54 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:17,591 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 55 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:18,594 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 56 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:19,597 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 57 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:20,600 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 58 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:21,602 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 59 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:22,605 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 60 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:23,609 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 61 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:24,611 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 62 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:25,613 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 63 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:26,615 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 64 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:27,621 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 65 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:28,624 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 66 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:29,626 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 67 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:30,632 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 68 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:31,637 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 69 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:32,641 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 70 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:33,643 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 71 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:34,647 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 72 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:35,649 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 73 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:36,654 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 74 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:37,659 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 75 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:38,662 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 76 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:39,666 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 77 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 18:33:40,672 WARN  [LeaseRenewer:hadoop@node-master:9000] (org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run:438)  - Failed to renew lease for [DFSClient_NONMAPREDUCE_-1831308620_31] for 78 seconds.  Will retry shortly ...
java.net.ConnectException: Call From BDDST-Group4-node0/10.123.252.237 to node-master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at java.base/jdk.internal.reflect.GeneratedConstructorAccessor9.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:755)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy14.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy15.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:568)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.renew(LeaseRenewer.java:395)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:415)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$600(LeaseRenewer.java:76)
	at org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:307)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:779)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:804)
	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:421)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1606)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	... 20 more
13 Dec 2020 20:33:24,909 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start:62)  - Configuration provider starting
13 Dec 2020 20:33:24,929 INFO  [conf-file-poller-0] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run:138)  - Reloading configuration file:./conf/flume-twitter-to-file.conf
13 Dec 2020 20:33:24,948 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,950 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,951 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 20:33:24,951 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,951 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,952 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 20:33:24,952 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,952 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 20:33:24,953 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 20:33:24,953 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty:1117)  - Added sinks: ToKafka , HDFS Agent: TwitterAgent
13 Dec 2020 20:33:24,954 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,955 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,955 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,956 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 20:33:24,960 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,961 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,961 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:ToKafka
13 Dec 2020 20:33:24,961 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 20:33:24,961 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,962 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,962 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,962 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:hdfsChannel
13 Dec 2020 20:33:24,962 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,962 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,962 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,963 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 20:33:24,963 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:kafkaChannel
13 Dec 2020 20:33:24,963 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:Twitter
13 Dec 2020 20:33:24,964 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addComponentConfig:1203)  - Processing:HDFS
13 Dec 2020 20:33:24,964 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateConfigFilterSet:623)  - Agent configuration for 'TwitterAgent' has no configfilters.
13 Dec 2020 20:33:25,002 WARN  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks:841)  - no context for sink,
13 Dec 2020 20:33:25,003 INFO  [conf-file-poller-0] (org.apache.flume.conf.FlumeConfiguration.validateConfiguration:163)  - Post-validation flume configuration contains configuration for agents: [TwitterAgent]
13 Dec 2020 20:33:25,003 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:151)  - Creating channels
13 Dec 2020 20:33:25,029 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel hdfsChannel type memory
13 Dec 2020 20:33:25,041 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel hdfsChannel
13 Dec 2020 20:33:25,049 INFO  [conf-file-poller-0] (org.apache.flume.channel.DefaultChannelFactory.create:42)  - Creating instance of channel kafkaChannel type memory
13 Dec 2020 20:33:25,049 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadChannels:205)  - Created channel kafkaChannel
13 Dec 2020 20:33:25,054 INFO  [conf-file-poller-0] (org.apache.flume.source.DefaultSourceFactory.create:41)  - Creating instance of source Twitter, type com.casper.TwitterSource
13 Dec 2020 20:33:25,200 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: ToKafka, type: org.apache.flume.sink.kafka.KafkaSink
13 Dec 2020 20:33:25,215 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:367)  - topic is deprecated. Please use the parameter kafka.topic
13 Dec 2020 20:33:25,215 WARN  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.translateOldProps:378)  - brokerList is deprecated. Please use the parameter kafka.bootstrap.servers
13 Dec 2020 20:33:25,216 INFO  [conf-file-poller-0] (org.apache.flume.sink.kafka.KafkaSink.configure:318)  - Using the static topic twitterraw. This may be overridden by event headers
13 Dec 2020 20:33:25,234 INFO  [conf-file-poller-0] (org.apache.flume.sink.DefaultSinkFactory.create:42)  - Creating instance of sink: HDFS, type: hdfs
13 Dec 2020 20:33:25,265 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel hdfsChannel connected to [Twitter, HDFS]
13 Dec 2020 20:33:25,265 INFO  [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.getConfiguration:120)  - Channel kafkaChannel connected to [Twitter, ToKafka]
13 Dec 2020 20:33:25,268 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:162)  - Starting new configuration:{ sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:IDLE} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@174205a1 counterGroup:{ name:null counters:{} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@bde8f1e counterGroup:{ name:null counters:{} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
13 Dec 2020 20:33:25,269 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel hdfsChannel
13 Dec 2020 20:33:25,277 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:169)  - Starting Channel kafkaChannel
13 Dec 2020 20:33:25,516 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: kafkaChannel: Successfully registered new MBean.
13 Dec 2020 20:33:25,517 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: kafkaChannel started
13 Dec 2020 20:33:25,517 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: CHANNEL, name: hdfsChannel: Successfully registered new MBean.
13 Dec 2020 20:33:25,518 INFO  [lifecycleSupervisor-1-0] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: CHANNEL, name: hdfsChannel started
13 Dec 2020 20:33:25,518 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink ToKafka
13 Dec 2020 20:33:25,520 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:196)  - Starting Sink HDFS
13 Dec 2020 20:33:25,525 INFO  [conf-file-poller-0] (org.apache.flume.node.Application.startAllComponents:207)  - Starting Source Twitter
13 Dec 2020 20:33:25,526 INFO  [lifecycleSupervisor-1-5] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: HDFS: Successfully registered new MBean.
13 Dec 2020 20:33:25,526 INFO  [lifecycleSupervisor-1-5] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: HDFS started
13 Dec 2020 20:33:25,536 INFO  [Twitter Stream consumer-1[initializing]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Establishing connection.
13 Dec 2020 20:33:25,605 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.config.AbstractConfig.logAll:279)  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [node-master:9092, node1:19092, node2:19092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

13 Dec 2020 20:33:25,918 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:109)  - Kafka version : 2.0.1
13 Dec 2020 20:33:25,918 INFO  [lifecycleSupervisor-1-1] (org.apache.kafka.common.utils.AppInfoParser$AppInfo.<init>:110)  - Kafka commitId : fa14705e51bd2ce5
13 Dec 2020 20:33:25,931 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.register:119)  - Monitored counter group for type: SINK, name: ToKafka: Successfully registered new MBean.
13 Dec 2020 20:33:25,931 INFO  [lifecycleSupervisor-1-1] (org.apache.flume.instrumentation.MonitoredCounterGroup.start:95)  - Component type: SINK, name: ToKafka started
13 Dec 2020 20:33:27,105 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Connection established.
13 Dec 2020 20:33:27,109 INFO  [Twitter Stream consumer-1[Establishing connection]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Receiving status stream.
13 Dec 2020 20:33:27,237 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 20:33:27,748 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607888007237.tmp
13 Dec 2020 20:33:28,115 WARN  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.NetworkClient.processDisconnection:671)  - [Producer clientId=producer-1] Connection to node -3 could not be established. Broker may not be available.
13 Dec 2020 20:33:28,133 INFO  [kafka-producer-network-thread | producer-1] (org.apache.kafka.clients.Metadata.update:285)  - Cluster ID: D-Q07R6tRuKQsLc9ECkk-Q
13 Dec 2020 20:33:30,146 INFO  [Thread-11] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 21:02:57,536 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 21:17:36,625 INFO  [DataStreamer for file /twitterraw/FlumeData.1607888007237.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 21:25:43,054 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 21:32:13,374 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 21:33:30,078 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
13 Dec 2020 21:33:30,106 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 21:33:30,140 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607888007237.tmp
13 Dec 2020 21:33:30,745 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607888007237.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607888007237
13 Dec 2020 21:33:30,806 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607891610106.tmp
13 Dec 2020 21:33:31,432 INFO  [Thread-135] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 22:05:38,868 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 22:18:53,742 INFO  [DataStreamer for file /twitterraw/FlumeData.1607891610106.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 22:33:30,844 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
13 Dec 2020 22:33:30,847 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607891610106.tmp
13 Dec 2020 22:33:30,867 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607891610106.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607891610106
13 Dec 2020 22:33:30,908 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 22:33:30,929 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607895210909.tmp
13 Dec 2020 22:33:31,913 INFO  [Thread-258] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 22:35:07,719 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 23:01:11,115 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 23:05:09,276 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 23:08:36,120 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 23:15:33,310 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 23:17:51,056 INFO  [DataStreamer for file /twitterraw/FlumeData.1607895210909.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 23:27:11,283 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 23:29:22,313 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 23:33:30,965 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
13 Dec 2020 23:33:30,975 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607895210909.tmp
13 Dec 2020 23:33:30,984 INFO  [hdfs-HDFS-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607895210909.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607895210909
13 Dec 2020 23:33:31,073 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
13 Dec 2020 23:33:31,092 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607898811074.tmp
13 Dec 2020 23:33:32,672 INFO  [Thread-382] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
13 Dec 2020 23:35:14,562 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
13 Dec 2020 23:35:14,841 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 00:04:00,149 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 00:04:15,310 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 00:15:31,199 INFO  [DataStreamer for file /twitterraw/FlumeData.1607898811074.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 00:19:40,065 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 00:19:40,653 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 00:19:57,561 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 00:20:09,247 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 00:20:33,719 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 00:33:31,125 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 00:33:31,133 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607898811074.tmp
14 Dec 2020 00:33:31,149 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607898811074.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607898811074
14 Dec 2020 00:33:31,156 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 00:33:31,203 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607902411156.tmp
14 Dec 2020 00:33:32,151 INFO  [Thread-505] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 01:15:57,152 INFO  [DataStreamer for file /twitterraw/FlumeData.1607902411156.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 01:33:31,246 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 01:33:31,259 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607902411156.tmp
14 Dec 2020 01:33:31,281 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607902411156.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607902411156
14 Dec 2020 01:33:31,469 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 01:33:31,502 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607906011470.tmp
14 Dec 2020 01:33:32,609 INFO  [Thread-628] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 01:54:45,300 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 01:55:09,283 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 01:55:33,671 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 01:59:27,535 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 02:04:06,377 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 02:04:41,045 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 02:04:50,214 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 02:17:00,295 INFO  [DataStreamer for file /twitterraw/FlumeData.1607906011470.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 02:28:34,256 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 02:33:31,533 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 02:33:31,543 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607906011470.tmp
14 Dec 2020 02:33:31,562 INFO  [hdfs-HDFS-call-runner-3] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607906011470.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607906011470
14 Dec 2020 02:33:31,571 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 02:33:31,640 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607909611572.tmp
14 Dec 2020 02:33:33,142 INFO  [Thread-752] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 03:16:00,456 INFO  [DataStreamer for file /twitterraw/FlumeData.1607909611572.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 03:18:14,483 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 03:33:31,673 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 03:33:31,686 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607909611572.tmp
14 Dec 2020 03:33:31,706 INFO  [hdfs-HDFS-call-runner-2] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607909611572.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607909611572
14 Dec 2020 03:33:31,963 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 03:33:31,994 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607913211964.tmp
14 Dec 2020 03:33:32,980 INFO  [Thread-875] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 03:50:58,085 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 04:17:32,479 INFO  [DataStreamer for file /twitterraw/FlumeData.1607913211964.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 04:18:11,804 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 04:26:33,945 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 04:33:32,019 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 04:33:32,029 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607913211964.tmp
14 Dec 2020 04:33:32,041 INFO  [hdfs-HDFS-call-runner-5] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607913211964.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607913211964
14 Dec 2020 04:33:32,305 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 04:33:32,336 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607916812306.tmp
14 Dec 2020 04:33:33,170 INFO  [Thread-998] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 04:46:39,588 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 04:46:40,079 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 05:00:19,754 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 05:23:06,051 INFO  [DataStreamer for file /twitterraw/FlumeData.1607916812306.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 05:33:32,380 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 05:33:32,383 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607916812306.tmp
14 Dec 2020 05:33:32,420 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 05:33:32,433 INFO  [hdfs-HDFS-call-runner-7] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607916812306.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607916812306
14 Dec 2020 05:33:32,451 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607920412421.tmp
14 Dec 2020 05:33:33,546 INFO  [Thread-1121] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 06:04:25,627 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 06:18:46,186 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 06:33:32,477 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 06:33:32,488 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607920412421.tmp
14 Dec 2020 06:33:32,500 INFO  [hdfs-HDFS-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607920412421.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607920412421
14 Dec 2020 06:33:32,559 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 06:33:32,584 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607924012560.tmp
14 Dec 2020 06:33:34,287 INFO  [Thread-1243] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 06:35:14,502 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 06:35:39,981 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 06:35:40,647 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 06:36:09,465 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 06:36:33,880 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 07:33:32,611 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 07:33:32,621 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607924012560.tmp
14 Dec 2020 07:33:32,631 INFO  [hdfs-HDFS-call-runner-2] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607924012560.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607924012560
14 Dec 2020 07:33:32,771 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 07:33:32,820 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607927612772.tmp
14 Dec 2020 07:33:34,676 INFO  [Thread-1365] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 07:35:41,737 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:33:32,847 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 08:33:32,864 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607927612772.tmp
14 Dec 2020 08:33:32,874 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607927612772.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607927612772
14 Dec 2020 08:33:33,112 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 08:33:33,164 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607931213113.tmp
14 Dec 2020 08:33:37,349 INFO  [Thread-1487] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 08:46:41,490 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:47:10,454 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:47:33,541 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:54:17,744 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:54:51,483 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:55:08,918 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:56:33,901 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:56:55,094 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 08:57:09,792 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 09:33:33,198 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 09:33:33,201 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607931213113.tmp
14 Dec 2020 09:33:33,213 INFO  [hdfs-HDFS-call-runner-9] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607931213113.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607931213113
14 Dec 2020 09:33:33,594 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 09:33:33,612 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607934813595.tmp
14 Dec 2020 09:33:40,213 INFO  [Thread-1610] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 10:33:33,638 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 10:33:33,641 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607934813595.tmp
14 Dec 2020 10:33:33,661 INFO  [hdfs-HDFS-call-runner-1] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607934813595.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607934813595
14 Dec 2020 10:33:33,967 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 10:33:34,002 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607938413967.tmp
14 Dec 2020 10:33:41,290 INFO  [Thread-1732] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 10:36:39,435 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 10:42:09,535 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 10:43:08,945 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 10:43:34,053 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 10:43:52,637 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 11:33:34,053 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 11:33:34,058 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607938413967.tmp
14 Dec 2020 11:33:34,080 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607938413967.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607938413967
14 Dec 2020 11:33:34,655 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 11:33:34,681 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607942014655.tmp
14 Dec 2020 11:33:41,744 INFO  [Thread-1855] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 12:01:04,972 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 12:33:34,713 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 12:33:34,716 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607942014655.tmp
14 Dec 2020 12:33:34,737 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607942014655.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607942014655
14 Dec 2020 12:33:34,939 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 12:33:34,997 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607945614940.tmp
14 Dec 2020 12:33:40,249 INFO  [Thread-1977] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 13:33:35,030 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 13:33:35,031 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607945614940.tmp
14 Dec 2020 13:33:35,052 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 13:33:35,064 INFO  [hdfs-HDFS-call-runner-0] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607945614940.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607945614940
14 Dec 2020 13:33:35,089 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607949215053.tmp
14 Dec 2020 13:33:37,676 INFO  [Thread-2099] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 14:33:35,114 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 14:33:35,123 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607949215053.tmp
14 Dec 2020 14:33:35,132 INFO  [hdfs-HDFS-call-runner-3] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607949215053.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607949215053
14 Dec 2020 14:33:35,801 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 14:33:35,855 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607952815802.tmp
14 Dec 2020 14:33:38,383 INFO  [Thread-2222] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 14:38:55,094 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 15:00:07,622 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 15:30:33,516 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 15:32:55,712 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 15:33:35,895 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 15:33:35,899 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 15:33:35,910 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607952815802.tmp
14 Dec 2020 15:33:35,923 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607952815802.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607952815802
14 Dec 2020 15:33:35,959 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607956415899.tmp
14 Dec 2020 15:33:37,422 INFO  [Thread-2345] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 16:09:08,100 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 16:09:09,088 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 16:09:09,957 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 16:24:02,268 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 16:31:32,735 INFO  [DataStreamer for file /twitterraw/FlumeData.1607956415899.tmp] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 16:33:36,007 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:393)  - Writer callback called.
14 Dec 2020 16:33:36,020 INFO  [hdfs-HDFS-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607956415899.tmp
14 Dec 2020 16:33:36,027 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSDataStream.configure:57)  - Serializer = TEXT, UseRawLocalFileSystem = false
14 Dec 2020 16:33:36,037 INFO  [hdfs-HDFS-call-runner-6] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607956415899.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607956415899
14 Dec 2020 16:33:36,055 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.open:246)  - Creating hdfs://node-master:9000/twitterraw/FlumeData.1607960016028.tmp
14 Dec 2020 16:33:37,280 INFO  [Thread-2469] (org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend:239)  - SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
14 Dec 2020 17:00:11,023 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 17:03:19,541 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 17:04:48,982 ERROR [Twitter4J Async Dispatcher[0]] (com.casper.TwitterInterceptor.intercept:60)  - Error parsing EventBody to JSON. 
org.json.JSONException: JSONObject["place"] is not a JSONObject.
	at org.json.JSONObject.getJSONObject(JSONObject.java:782)
	at com.casper.TwitterInterceptor.intercept(TwitterInterceptor.java:43)
	at org.apache.flume.interceptor.InterceptorChain.intercept(InterceptorChain.java:51)
	at org.apache.flume.channel.ChannelProcessor.processEvent(ChannelProcessor.java:255)
	at com.casper.TwitterSource$1.onStatus(TwitterSource.java:90)
	at twitter4j.StatusStreamImpl.onStatus(StatusStreamImpl.java:75)
	at twitter4j.StatusStreamBase$1.run(StatusStreamBase.java:114)
	at twitter4j.internal.async.ExecuteThread.run(DispatcherImpl.java:116)
14 Dec 2020 17:04:57,299 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:125)  - Shutting down configuration: { sourceRunners:{Twitter=EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }} sinkRunners:{ToKafka=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@174205a1 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }, HDFS=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@bde8f1e counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }} channels:{hdfsChannel=org.apache.flume.channel.MemoryChannel{name: hdfsChannel}, kafkaChannel=org.apache.flume.channel.MemoryChannel{name: kafkaChannel}} }
14 Dec 2020 17:04:57,303 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:129)  - Stopping Source Twitter
14 Dec 2020 17:04:57,303 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: EventDrivenSourceRunner: { source:com.casper.TwitterSource{name:Twitter,state:START} }
14 Dec 2020 17:04:57,359 INFO  [Twitter Stream consumer-1[Disposing thread]] (twitter4j.internal.logging.SLF4JLogger.info:83)  - Inflater has been closed
14 Dec 2020 17:04:57,363 ERROR [Twitter Stream consumer-1[Disposing thread]] (com.casper.TwitterSource$1.onException:99)  - Error while listening to Twitter stream.
java.lang.NullPointerException: Inflater has been closed
	at java.util.zip.Inflater.ensureOpen(Inflater.java:389)
	at java.util.zip.Inflater.inflate(Inflater.java:257)
	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:152)
	at java.util.zip.GZIPInputStream.read(GZIPInputStream.java:117)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at twitter4j.StatusStreamBase.handleNextElement(StatusStreamBase.java:85)
	at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:57)
	at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:481)
14 Dec 2020 17:04:57,389 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink ToKafka
14 Dec 2020 17:04:57,391 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@174205a1 counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
14 Dec 2020 17:04:57,391 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:255)  - Failed to publish events
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1302)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
14 Dec 2020 17:04:57,392 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:268)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1302)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
	at org.apache.kafka.clients.producer.internals.ProduceRequestResult.await(ProduceRequestResult.java:76)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:61)
	at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:29)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:244)
	... 3 more
14 Dec 2020 17:05:02,393 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.producer.KafkaProducer.close:1090)  - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
14 Dec 2020 17:05:02,413 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: ToKafka stopped
14 Dec 2020 17:05:02,413 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: ToKafka. sink.start.time == 1607888005931
14 Dec 2020 17:05:02,413 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: ToKafka. sink.stop.time == 1607961902413
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.kafka.event.send.time == 73880426
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. channel.rollback.count == 1
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.complete == 0
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.empty == 0
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.batch.underflow == 41
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.channel.read.fail == 0
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.closed.count == 0
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.creation.count == 0
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.connection.failed.count == 0
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.attempt == 675941
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.drain.sucess == 675883
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: ToKafka. sink.event.write.fail == 1
14 Dec 2020 17:05:02,414 INFO  [agent-shutdown-hook] (org.apache.flume.sink.kafka.KafkaSink.stop:290)  - Kafka Sink ToKafka stopped. Metrics: SINK:ToKafka{sink.batch.underflow=41, sink.event.write.fail=1, sink.connection.failed.count=0, sink.channel.read.fail=0, sink.connection.closed.count=0, channel.kafka.event.send.time=73880426, sink.event.drain.attempt=675941, sink.batch.complete=0, sink.event.drain.sucess=675883, sink.connection.creation.count=0, channel.rollback.count=1, sink.batch.empty=0}
14 Dec 2020 17:05:02,415 INFO  [lifecycleSupervisor-1-3] (org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run:228)  - Component has already been stopped SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@174205a1 counterGroup:{ name:null counters:{runner.deliveryErrors=1, runner.backoffs.consecutive=0} } }
14 Dec 2020 17:05:02,415 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:139)  - Stopping Sink HDFS
14 Dec 2020 17:05:02,415 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@bde8f1e counterGroup:{ name:null counters:{runner.backoffs.consecutive=0} } }
14 Dec 2020 17:05:02,415 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.HDFSEventSink.stop:494)  - Closing hdfs://node-master:9000/twitterraw/FlumeData
14 Dec 2020 17:05:02,415 INFO  [agent-shutdown-hook] (org.apache.flume.sink.hdfs.BucketWriter.doClose:438)  - Closing hdfs://node-master:9000/twitterraw/FlumeData.1607960016028.tmp
14 Dec 2020 17:05:02,430 INFO  [hdfs-HDFS-call-runner-8] (org.apache.flume.sink.hdfs.BucketWriter$7.call:681)  - Renaming hdfs://node-master:9000/twitterraw/FlumeData.1607960016028.tmp to hdfs://node-master:9000/twitterraw/FlumeData.1607960016028
14 Dec 2020 17:05:02,435 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: SINK, name: HDFS stopped
14 Dec 2020 17:05:02,435 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: SINK, name: HDFS. sink.start.time == 1607888005526
14 Dec 2020 17:05:02,435 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: SINK, name: HDFS. sink.stop.time == 1607961902435
14 Dec 2020 17:05:02,435 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.complete == 0
14 Dec 2020 17:05:02,435 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.empty == 1
14 Dec 2020 17:05:02,435 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.batch.underflow == 41
14 Dec 2020 17:05:02,435 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.channel.read.fail == 0
14 Dec 2020 17:05:02,435 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.closed.count == 21
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.creation.count == 21
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.connection.failed.count == 0
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.attempt == 675941
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.drain.sucess == 675941
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: HDFS. sink.event.write.fail == 0
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel hdfsChannel
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: hdfsChannel}
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: hdfsChannel stopped
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.start.time == 1607888005518
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.stop.time == 1607961902436
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.capacity == 10000000
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.current.size == 0
14 Dec 2020 17:05:02,436 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.attempt == 675941
14 Dec 2020 17:05:02,437 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.put.success == 675941
14 Dec 2020 17:05:02,437 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.attempt == 675983
14 Dec 2020 17:05:02,437 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: hdfsChannel. channel.event.take.success == 675941
14 Dec 2020 17:05:02,437 INFO  [agent-shutdown-hook] (org.apache.flume.node.Application.stopAllComponents:149)  - Stopping Channel kafkaChannel
14 Dec 2020 17:05:02,437 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.unsupervise:169)  - Stopping component: org.apache.flume.channel.MemoryChannel{name: kafkaChannel}
14 Dec 2020 17:05:02,437 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:149)  - Component type: CHANNEL, name: kafkaChannel stopped
14 Dec 2020 17:05:02,437 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:155)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.start.time == 1607888005517
14 Dec 2020 17:05:02,440 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:161)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.stop.time == 1607961902437
14 Dec 2020 17:05:02,440 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.capacity == 1000
14 Dec 2020 17:05:02,440 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.current.size == 58
14 Dec 2020 17:05:02,440 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.attempt == 675941
14 Dec 2020 17:05:02,440 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.put.success == 675941
14 Dec 2020 17:05:02,440 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.attempt == 675982
14 Dec 2020 17:05:02,440 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: CHANNEL, name: kafkaChannel. channel.event.take.success == 675883
14 Dec 2020 17:05:02,441 INFO  [agent-shutdown-hook] (org.apache.flume.lifecycle.LifecycleSupervisor.stop:78)  - Stopping lifecycle supervisor 10
14 Dec 2020 17:05:02,443 INFO  [agent-shutdown-hook] (org.apache.flume.node.PollingPropertiesFileConfigurationProvider.stop:84)  - Configuration provider stopping
